{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6fb9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8534a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.sql.execution.arrow.enabled', 'true'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.cores', '5'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.host', 'kafka1'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.yarn.archive', 'hdfs:///user/spark/conf/spark-libs.jar'),\n",
       " ('spark.driver.port', '33401'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.kryoserializer.buffer.max', '2047'),\n",
       " ('spark.app.id', 'application_1667038354005_0002'),\n",
       " ('spark.executor.memoryOverhead', '1g'),\n",
       " ('spark.driver.memoryOverhead', '1g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://0.0.0.0:8089/proxy/application_1667038354005_0002'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'file:///root/spark/eventLog'),\n",
       " ('spark.default.parallelism', '163'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.network.timeout', '3600s'),\n",
       " ('spark.driver.appUIAddress', 'http://kafka1:4040'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/root/spark/python/lib/py4j-0.10.7-src.zip:/root/spark/python/:/python:/python/lib/py4j-0.10.7-src.zip:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  '0.0.0.0'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1667038354005_0002'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.eventLog.dir', 'file:///root/spark/eventLog'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export PYSPARK_PYTHON=python3.6.9\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "import pickle\n",
    "import tensorflow.keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d57c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169bd619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d455813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34172a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f62f53b6550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255dbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark._jsc.sc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e21f8cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JavaObject id=o165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8c60e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<py4j.java_gateway.JavaMember object at 0x7f624f3fc470>\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc21f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f609e123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f62f53b6550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b51caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c620059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc197658",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1454a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f624f41b630>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d0795f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yarn'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b8cf6",
   "metadata": {},
   "source": [
    "# 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa75ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|im at ap stgate i...|    0|\n",
      "|why do i love thi...|    0|\n",
      "|nowplaying charmi...|    0|\n",
      "|im at kuwait city...|    0|\n",
      "|lolim right hande...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "801932\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def regex_(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    text['total_text_list'] = text['total_text_list'].str.lower()\n",
    "    text=text[((text['total_text_list'].str.len()>= 10)) & ((text['total_text_list'].eq(' ')==False) & (text['total_text_list'].eq('')==False))]\n",
    "    return text\n",
    "def regex2_(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['text'] = text['text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    text['text'] = text['text'].str.lower()\n",
    "    text=text[((text['text'].str.len()>= 10)) & ((text['text'].eq(' ')==False) & (text['text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "def regex_3(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    # @로 시작하는 애들도 정규식 이용해서 없애기 @[^ ]+\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.lower()\n",
    "    text=text[((text['full_text'].str.len()>= 10)) & ((text['full_text'].str.split(\" \").str.len()>= 10)) & ((text['full_text'].eq(' ')==False) & (text['full_text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "def regex_4(text):\n",
    "    # 영어, 숫자, 특수만문자 제외 삭제.  \n",
    "    text = text.dropna(axis=0)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=pattern,repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'[^ a-zA-Z]',repl=r'',regex=True)\n",
    "    # @로 시작하는 애들도 정규식 이용해서 없애기 @[^ ]+\n",
    "    text['full_text'] = text['full_text'].str.replace(pat=r'(\\@\\w+.*?) ',repl=r'',regex=True)\n",
    "    text['full_text'] = text['full_text'].str.lower()\n",
    "    #text=text[((text['full_text'].str.len()>= 10)) & ((text['full_text'].str.split(\" \").str.len()>= 10)) & ((text['full_text'].eq(' ')==False) & (text['full_text'].eq('')==False))]\n",
    "    return text\n",
    "\n",
    "# general_minsun2.csv\n",
    "general = pd.read_csv('/root/spark/bigcomp_model/general.csv', sep=',',  lineterminator='\\n')\n",
    "general = general.drop_duplicates()\n",
    "general = spark.createDataFrame(general[['total_text_list']])\n",
    "general = general.select(col('total_text_list').alias('text')) \n",
    "general = general.withColumn('label', lit(0))\n",
    "general.show(5)\n",
    "print(general.count())\n",
    "general = general.withColumn(\"label\",col(\"label\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e7c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보안 관련 계정들 트윗\n",
    "security = pd.read_csv('/root/spark/bigcomp_model/dataset.csv', sep=',',  lineterminator='\\n')\n",
    "security = security[[\"text\",\"datetime\"]]\n",
    "security = security.sort_values(by='datetime' ,ascending=True)\n",
    "security = security.reset_index(drop=True)\n",
    "security = regex2_(security)\n",
    "security = security[~security['text'].str.contains(\"rt\", na=False, case=False)]\n",
    "security = security.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efaa579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38146657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bbcebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "security = spark.createDataFrame(security[['text']])\n",
    "security = security.withColumn('label', lit(1))\n",
    "security = security.withColumn(\"label\",col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "174a7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained1 = security.collect()[:200000]\n",
    "pretrained2 = general.collect()[:200000]\n",
    "\n",
    "\n",
    "pretrained1 = spark.createDataFrame(pretrained1)\n",
    "pretrained2 = spark.createDataFrame(pretrained2)\n",
    "pre = pretrained1.union(pretrained2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f884470b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word1 = security.collect()[200000:250000]\\nword2 = general.collect()[200000:250000]\\n\\nword3 = security.collect()[200000:300000]\\nword4 = general.collect()[200000:300000]\\n\\nword5 = security.collect()[200000:350000]\\nword6 = general.collect()[200000:350000]\\n\\nword7 = security.collect()[200000:400000]\\nword8 = general.collect()[200000:400000]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = security.collect()[200000:250000]\n",
    "t2 = security.collect()[250000:300000]\n",
    "t3 = security.collect()[300000:350000]\n",
    "t4 = security.collect()[350000:400000]\n",
    "\n",
    "t5 = general.collect()[200000:250000]\n",
    "t6 = general.collect()[250000:300000]\n",
    "t7 = general.collect()[300000:350000]\n",
    "t8 = general.collect()[350000:400000]\n",
    "\n",
    "\n",
    "t1 = spark.createDataFrame(t1)\n",
    "t2 = spark.createDataFrame(t2)\n",
    "t3 = spark.createDataFrame(t3)\n",
    "t4 = spark.createDataFrame(t4)\n",
    "t5 = spark.createDataFrame(t5)\n",
    "t6 = spark.createDataFrame(t6)\n",
    "t7 = spark.createDataFrame(t7)\n",
    "t8 = spark.createDataFrame(t8)\n",
    "\n",
    "df1 = t1.union(t5)\n",
    "df2 = t2.union(t6)\n",
    "df3 = t3.union(t7)\n",
    "df4 = t4.union(t8)\n",
    "\n",
    "\"\"\"word1 = security.collect()[200000:250000]\n",
    "word2 = general.collect()[200000:250000]\n",
    "\n",
    "word3 = security.collect()[200000:300000]\n",
    "word4 = general.collect()[200000:300000]\n",
    "\n",
    "word5 = security.collect()[200000:350000]\n",
    "word6 = general.collect()[200000:350000]\n",
    "\n",
    "word7 = security.collect()[200000:400000]\n",
    "word8 = general.collect()[200000:400000]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65f54d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = security.collect()[:250000]\n",
    "word2 = general.collect()[:250000]\n",
    "\n",
    "word3 = security.collect()[:300000]\n",
    "word4 = general.collect()[:300000]\n",
    "\n",
    "word5 = security.collect()[:350000]\n",
    "word6 = general.collect()[:350000]\n",
    "\n",
    "word7 = security.collect()[:400000]\n",
    "word8 = general.collect()[:400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f564aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = spark.createDataFrame(word1)\n",
    "word2= spark.createDataFrame(word2)\n",
    "\n",
    "word3 = spark.createDataFrame(word3)\n",
    "word4= spark.createDataFrame(word4)\n",
    "\n",
    "word5 = spark.createDataFrame(word5)\n",
    "word6= spark.createDataFrame(word6)\n",
    "\n",
    "word7 = spark.createDataFrame(word7)\n",
    "word8= spark.createDataFrame(word8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5be2e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = t1.union(t5)\n",
    "second = t1.union(t2).union(t5).union(t6)\n",
    "third = t1.union(t2).union(t3).union(t5).union(t6).union(t7)\n",
    "fourth = t1.union(t2).union(t3).union(t4).union(t5).union(t6).union(t7).union(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f24e89c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4da0124a5ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mstage_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStopWordsRemover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstage_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'filtered_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:200000] # 24939 99756\n",
    "        neg_df = result_pdf[200000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(word1, word2)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(first)\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "pre_df = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdfdc240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "1270\n",
      "29356\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:200000] # 24939 99756\n",
    "        neg_df = result_pdf[200000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "        X = np.array(X_p)\n",
    "        y = np.array(label_array)\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa, X, y\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(pretrained1, pretrained2)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(pre)\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb, X, y = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "pre_df = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda429a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "1247\n",
      "31544\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:100000] # 24939 99756\n",
    "        neg_df = result_pdf[100000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(word3, word4)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(second)\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "label_df1 = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9eb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99b27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ac9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77b21bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t1 = security.collect()[400000:450000]\n",
    "t2 = security.collect()[450000:500000]\n",
    "\n",
    "t3 = general.collect()[400000:450000]\n",
    "t4 = general.collect()[450000:500000]\n",
    "\n",
    "t1 = spark.createDataFrame(t1)\n",
    "t2 = spark.createDataFrame(t2)\n",
    "t3 = spark.createDataFrame(t3)\n",
    "t4 = spark.createDataFrame(t4)\n",
    "\n",
    "\n",
    "fine1 = t1.union(t3)\n",
    "fine2 = t2.union(t4)\n",
    "\n",
    "df1 = t1.union(t3)\n",
    "df2 = t1.union(t2).union(t3).union(t4)\n",
    "\n",
    "word1 = security.collect()[200000:450000]\n",
    "word2 = general.collect()[200000:450000]\n",
    "\n",
    "word3 = security.collect()[200000:500000]\n",
    "word4 = general.collect()[200000:500000]\n",
    "\n",
    "\n",
    "word1 = spark.createDataFrame(word1)\n",
    "word2= spark.createDataFrame(word2)\n",
    "\n",
    "word3 = spark.createDataFrame(word3)\n",
    "word4= spark.createDataFrame(word4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f6739df",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = security.collect()[200000:450000]\n",
    "word2 = general.collect()[200000:450000]\n",
    "\n",
    "word3 = security.collect()[200000:500000]\n",
    "word4 = general.collect()[200000:500000]\n",
    "\n",
    "\n",
    "word1 = spark.createDataFrame(word1)\n",
    "word2= spark.createDataFrame(word2)\n",
    "\n",
    "word3 = spark.createDataFrame(word3)\n",
    "word4= spark.createDataFrame(word4)\n",
    "dff1 = word1.union(word2)\n",
    "dff2 = word3.union(word4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93b6490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2 = word3.union(word4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6778c3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n",
      "1438\n",
      "43620\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:250000] # 24939 99756\n",
    "        neg_df = result_pdf[250000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(word1, word2)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(dff1)\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "label_df = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23528302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df7f2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def background_keyword(df): # input : spark dataframe을 pandas로 변환한 후, dataframe column\n",
    "    background = df.values.tolist()\n",
    "    token = Tokenizer()         # 토큰화 함수 지정\n",
    "    token.fit_on_texts(background)    # 토큰화 함수에 문장 적용\n",
    "    sorted_dic4 = sorted(token.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    word_counts_1 = {}\n",
    "    for i in sorted_dic4:\n",
    "        word_counts_1.update({i[0]:i[1]})\n",
    "    return word_counts_1 # dictionary 임\n",
    "def back_keyword(pos,neg): # pos : event-related keyword, neg : event-unrelated keyword\n",
    "    stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "    stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "    word1 = stage_1.transform(pos) #나는 first_train 부터 함!\n",
    "    word1 = stage_2.transform(word1)\n",
    "    word2 = stage_1.transform(neg) #나는 first_train 부터 함!\n",
    "    word2 = stage_2.transform(word2)\n",
    "    from pyspark.sql.functions import col, concat_ws\n",
    "    df1 = word1.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word1 = df1.select(\"filtered_words\").toPandas()\n",
    "    df2 = word2.withColumn(\"filtered_words\",\n",
    "       concat_ws(\",\",col(\"filtered_words\")))\n",
    "    word2 = df2.select(\"filtered_words\").toPandas()\n",
    "    word_counts_1 = background_keyword(word1[\"filtered_words\"])\n",
    "    word_counts_2 = background_keyword(word2[\"filtered_words\"])\n",
    "    return word_counts_1, word_counts_2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Author : Minseon Kim (2021)\n",
    "class BroadcastWrapper(object):\n",
    "    def __init__(self, data, token_list):\n",
    "        self.broadcast_var = sc.broadcast(data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "        self.token_list = token_list\n",
    "    \n",
    "#     def is_should_be_updated(self, data):\n",
    "#         cur_time = datetime.now()\n",
    "#         diff_sec = (cur_time - self.last_updated_time).total_seconds()\n",
    "#         return self.broadcast_var is None or diff_sec> 1\n",
    "    \n",
    "    def update_and_get_data(self, spark):\n",
    "        a = self.broadcast_var.value\n",
    "        self.broadcast_var.unpersist()\n",
    "        for i in self.token_list:\n",
    "            for j in i:\n",
    "                if j not in a.keys():\n",
    "                    a[j] = 1\n",
    "                else:\n",
    "                    a[j] += 1\n",
    "        new_data = a\n",
    "        self.broadcast_var = spark.broadcast(new_data)\n",
    "#         self.last_updated_time = datetime.now()\n",
    "#         return len(self.token_list)\n",
    "        return self.broadcast_var\n",
    "\n",
    "# 바꾼 버전\n",
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import types   \n",
    "\n",
    "def series_to_list(x):\n",
    "    seri = x.values.tolist()\n",
    "    for i in range(len(seri)):\n",
    "        seri[i] = seri[i].tolist()\n",
    "    return seri\n",
    "class TextToSequence(Transformer):\n",
    "    w1 = dict()\n",
    "    w2 = dict()\n",
    "    pos_t = Tokenizer(lower=False)\n",
    "    neg_t = Tokenizer(lower=False)\n",
    "    pos_vocab = []\n",
    "    neg_vocab = []\n",
    "    \n",
    "    def __init__(self, w1: Dict[str, int], w2:Dict[str, int]):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        result_pdf = df.select(\"filtered_words\").toPandas()\n",
    "        pos_df = result_pdf[:300000] # 24939 99756\n",
    "        neg_df = result_pdf[300000:]\n",
    "        print(len(result_pdf))\n",
    "        \n",
    "        broadcast_wrapper1 = BroadcastWrapper(self.w1, pos_df[\"filtered_words\"])\n",
    "        ww1 = broadcast_wrapper1.update_and_get_data(sc).value\n",
    "        \n",
    "        broadcast_wrapper2 = BroadcastWrapper(self.w2, neg_df[\"filtered_words\"])\n",
    "        ww2 = broadcast_wrapper2.update_and_get_data(sc).value\n",
    "        \n",
    "        www1 = {k: v for k, v in sorted(ww1.items(), key=lambda item: item[1], reverse=True)}\n",
    "        www2 = {k: v for k, v in sorted(ww2.items(), key=lambda item: item[1], reverse=True)}\n",
    "        print(www1['allow'])\n",
    "        print(www2['like'])\n",
    "        aa = [w for i, w in enumerate(www1) if i < 5000]\n",
    "        bb = [w for i, w in enumerate(www2) if i < 5000]\n",
    "        \n",
    "        self.pos_t.fit_on_texts(aa)\n",
    "        self.neg_t.fit_on_texts(bb)\n",
    "#         encoded_docs_pos = self.pos_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "#         encoded_docs_neg = self.neg_t.texts_to_sequences(df.select(\"filtered_words1\").toPandas())\n",
    "        result_list = series_to_list(result_pdf[\"filtered_words\"])\n",
    "        encoded_docs_pos = self.pos_t.texts_to_sequences(result_list) # 상위 event keyword 5k가 쓰인 tokenizer\n",
    "        encoded_docs_neg = self.neg_t.texts_to_sequences(result_list)\n",
    "        \n",
    "        X_p = pad_sequences(encoded_docs_pos, maxlen=100, padding='post').tolist()\n",
    "        X_n = pad_sequences(encoded_docs_neg, maxlen=100, padding='post').tolist()\n",
    "        \n",
    "        text_array = [str(row['text']) for row in df.select('text').collect()]\n",
    "        label_array = [int(row['label']) for row in df.select('label').collect()]\n",
    "\n",
    "        \n",
    "        zip_array = list(zip(text_array, label_array, X_p, X_n))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature1','feature2'])\n",
    "        \n",
    "\n",
    "        print(type(fdf))\n",
    "\n",
    "        return fdf, www1, aa\n",
    "    \n",
    "    \n",
    "word_counts_1, word_counts_2 = back_keyword(word3, word4)\n",
    "word_counts_1 = sc.broadcast(word_counts_1)\n",
    "word_counts_2 = sc.broadcast(word_counts_2)\n",
    "\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "\n",
    "r1 = stage_1.transform(dff2) #나는 first_train 부터 함!\n",
    "r2 = stage_2.transform(r1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a87df1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000\n",
      "1524\n",
      "52344\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# USE THE TRANSFORMER WITHOUT PIPELINE\n",
    "text_sequence = TextToSequence(w1 = word_counts_1.value, w2 = word_counts_2.value)\n",
    "df_example, www2, bb = text_sequence.transform(r2)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature1\"]).alias(\"feature1\"), \n",
    "    list_to_vector_udf(df_example[\"feature2\"]).alias(\"feature2\")\n",
    ")\n",
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index') # transformer의 마지막 단계!!\n",
    "label_df1 = label_str_index.fit(df_with_vectors).transform(df_with_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94d6a11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff0dc95b3c8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "354fb1a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+\n",
      "|label|            feature1|            feature2|label_index|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "|    1|[84.0,6.0,13.0,19...|[3097.0,659.0,340...|        1.0|\n",
      "|    1|[3425.0,2844.0,15...|[1110.0,0.0,0.0,0...|        1.0|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90290c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9656f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e02e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+\n",
      "|label|            feature1|            feature2|label_index|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "|    1|[96.0,8.0,11.0,22...|[3289.0,662.0,341...|        1.0|\n",
      "|    1|[4419.0,2972.0,16...|[1130.0,0.0,0.0,0...|        1.0|\n",
      "+-----+--------------------+--------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "129a8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretr, prets = pre_df.randomSplit(weights=[0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14620f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16df2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainqq, testqq = label_df.randomSplit(weights=[0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fb98123",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainqq1, testqq1 = label_df1.randomSplit(weights=[0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7a19b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "trainqq.toPandas().to_csv(\"/root/spark/bigcomp_model/security/fifth_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4afff03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqq.toPandas().to_csv(\"/root/spark/bigcomp_model/security/fifth_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b55261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/dataframe.py:2111: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "trainqq1.toPandas().to_csv(\"/root/spark/bigcomp_model/security/sixth_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c04c542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqq1.toPandas().to_csv(\"/root/spark/bigcomp_model/security/six_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1105a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d6467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406cafcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 735,110\n",
      "Trainable params: 735,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "nb_classes=2\n",
    "vocab_sizes = 5001\n",
    "neg_vocab_sizes = 5001\n",
    "embedding_vector_length = 100\n",
    "hidden_dims = 250\n",
    "max_length = 100\n",
    "dropout_ratio = 0.5\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sizes, embedding_vector_length, input_length=max_length))\n",
    "\n",
    "#어휘 크기(최대 정수 인덱스+1, 고밀도 임베딩의 치수, 입력 시퀀스의 길이\n",
    "\n",
    "# 모델은 크기의 정수 행렬 (배치,\n",
    "# input_length) 및 입력에서 가장 큰 정수 (즉, 단어 인덱스)\n",
    "#은 999 (어휘 크기)보다 크지 않아야합니다.\n",
    "# 이제 model.output_shape는 (None, 10, 64)이고, 여기서`None`은 배치입니다.\n",
    "# dimension.\n",
    "\"\"\"model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\"\"\"\n",
    "\n",
    "\"\"\"model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\"\"\"\n",
    "\n",
    "\"\"\"#model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 3,  padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(128, activation='relu')) #\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(Activation('relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"model.add(SimpleRNN(128,return_sequences=True))\n",
    "model.add(SimpleRNN(128,return_sequences=True))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "# #체크포인트 생성?!\n",
    "# # checkpoint_filepath = '/root/spark/baseline/'\n",
    "# fname = \"checkpoint-{epoch:02d}-{val_loss:.2f}\"\n",
    "# checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", mode=\"min\",\n",
    "#     save_best_only=True, verbose=1)\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dff832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100, 128)          29312     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100, 128)          32896     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 595,462\n",
      "Trainable params: 595,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "nb_classes=2\n",
    "vocab_sizes = 5001\n",
    "neg_vocab_sizes = 5001\n",
    "embedding_vector_length = 100\n",
    "hidden_dims = 250\n",
    "max_length = 100\n",
    "dropout_ratio = 0.5\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sizes, embedding_vector_length, input_length=max_length))\n",
    "\n",
    "#어휘 크기(최대 정수 인덱스+1, 고밀도 임베딩의 치수, 입력 시퀀스의 길이\n",
    "\n",
    "# 모델은 크기의 정수 행렬 (배치,\n",
    "# input_length) 및 입력에서 가장 큰 정수 (즉, 단어 인덱스)\n",
    "#은 999 (어휘 크기)보다 크지 않아야합니다.\n",
    "# 이제 model.output_shape는 (None, 10, 64)이고, 여기서`None`은 배치입니다.\n",
    "# dimension.\n",
    "\"\"\"model.add(Conv1D(128, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\"\"\"\n",
    "\n",
    "\"\"\"model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(10, activation='relu'))\"\"\"\n",
    "\n",
    "\"\"\"#model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 3,  padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(128, activation='relu')) #\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "\n",
    "model.add(SimpleRNN(128,return_sequences=True))\n",
    "model.add(SimpleRNN(128,return_sequences=True))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# #체크포인트 생성?!\n",
    "# # checkpoint_filepath = '/root/spark/baseline/'\n",
    "# fname = \"checkpoint-{epoch:02d}-{val_loss:.2f}\"\n",
    "# checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", mode=\"min\",\n",
    "#     save_best_only=True, verbose=1)\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e2fc959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 256)           128256    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               64250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 502       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 693,108\n",
      "Trainable params: 693,108\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, concatenate\n",
    "from tensorflow.python.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D, Flatten, GlobalMaxPooling1D, Dropout, MaxPooling1D, Activation, Bidirectional, Conv2D, GlobalMaxPooling2D\n",
    "nb_classes=2\n",
    "vocab_sizes = 5001\n",
    "neg_vocab_sizes = 5001\n",
    "embedding_vector_length = 100\n",
    "hidden_dims = 250\n",
    "max_length = 100\n",
    "dropout_ratio = 0.5\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_sizes, embedding_vector_length, input_length=max_length))\n",
    "\n",
    "#어휘 크기(최대 정수 인덱스+1, 고밀도 임베딩의 치수, 입력 시퀀스의 길이\n",
    "\n",
    "# 모델은 크기의 정수 행렬 (배치,\n",
    "# input_length) 및 입력에서 가장 큰 정수 (즉, 단어 인덱스)\n",
    "#은 999 (어휘 크기)보다 크지 않아야합니다.\n",
    "# 이제 model.output_shape는 (None, 10, 64)이고, 여기서`None`은 배치입니다.\n",
    "# dimension.\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "#model.add(Bidirectional(LSTM(128)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(250, activation='relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\"\"\"#model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(256, 3,  padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "#model.add(Dense(128, activation='relu')) #\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "\"\"\"model.add(Bidirectional(LSTM(128)))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "model.add(Activation('relu')) # 한 층 더 쌓을까..? relu 함수는 정류된 함수로 은닉층에서 많이 사용된다. -를 차단함!\n",
    "# 은닉층으로 많이 사용되는 이유는 음수는 0으로 반환하므로 특정 양수값으로 수렴하지 않음. 그래서 기울기 소실이 발생하지 않음!\n",
    "# 시그모이드 함수는 기울기 소실이 발생한다는 단점이 존재. 그래서 relu를 은닉층으로 쓰는 거임!\n",
    "# 또한, 학습 속도가 매우 빠르다.\"\"\"\n",
    "\"\"\"model.add(Dense(nb_classes)) # stringIndexer 클래스 수만큼 쌓는 이유가 뭘까,,,\n",
    "model.add(Activation('sigmoid'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\"\"\"\n",
    "\n",
    "# #체크포인트 생성?!\n",
    "# # checkpoint_filepath = '/root/spark/baseline/'\n",
    "# fname = \"checkpoint-{epoch:02d}-{val_loss:.2f}\"\n",
    "# checkpoint = ModelCheckpoint(fname, monitor=\"val_loss\", mode=\"min\",\n",
    "#     save_best_only=True, verbose=1)\n",
    "# callbacks = [checkpoint]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################fine tuning fourth window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2a7fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e01d081",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ElephasEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-760ae29a1f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElephasEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ElephasEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "help(ElephasEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfcf276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_c4622a31d51a"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "from elephas import spark_model, ml_model\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"feature1\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(trans_model.to_json())\n",
    "estimator.set_categorical_labels(True) # dense 1이면 False로 설정해야함\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(3)\n",
    "estimator.set_epochs(10)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"asynchronous\")\n",
    "estimator.set_loss(\"binary_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ba54659",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a5d2e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-----+\n",
      "|label_index|            feature1|            feature2|label|\n",
      "+-----------+--------------------+--------------------+-----+\n",
      "|        1.0|[1.0,121.0,12.0,2...|[841.0,3042.0,413...|    1|\n",
      "|        1.0|[2.0,924.0,1136.0...|[927.0,4860.0,416...|    1|\n",
      "|        1.0|[2.0,3780.0,455.0...|[927.0,978.0,2633...|    1|\n",
      "|        1.0|[3.0,2.0,355.0,15...|[927.0,1615.0,183...|    1|\n",
      "|        1.0|[3.0,17.0,174.0,1...|[1710.0,1839.0,31...|    1|\n",
      "|        1.0|[3.0,635.0,71.0,1...|[4043.0,4773.0,11...|    1|\n",
      "|        1.0|[4.0,8.0,2.0,18.0...|[121.0,927.0,1371...|    1|\n",
      "|        1.0|[4.0,8.0,2.0,18.0...|[121.0,927.0,1371...|    1|\n",
      "|        1.0|[4.0,8.0,2.0,18.0...|[121.0,927.0,1371...|    1|\n",
      "|        1.0|[4.0,8.0,2.0,18.0...|[121.0,927.0,1371...|    1|\n",
      "+-----------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainqq.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "125a7c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label_index: double, feature1: vector, feature2: vector, label: bigint]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainqq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretr, prets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f32cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.28.0.1 - - [29/Oct/2022 10:31:37] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:31:37] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:31:37] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:34:34] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:34:34] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:34:34] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:34:34] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:34:34] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:34:34] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:37:30] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:37:30] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:37:32] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:37:32] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:37:35] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:37:35] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:40:26] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:40:26] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:40:30] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:40:30] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:40:37] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:40:37] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:43:15] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:43:15] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:43:23] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:43:23] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:43:31] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:43:31] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:46:03] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:46:03] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:46:16] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:46:16] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:46:25] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:46:25] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:48:53] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:48:53] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:49:13] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:49:13] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:49:24] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:49:24] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:51:46] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:51:46] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:52:08] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:52:08] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:52:20] \"POST /update HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [29/Oct/2022 10:52:20] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "my_dl1 = dl_pipeline.fit(trainqq) # lstm finetuning\n",
    "time3 = time.time() - time1\n",
    "print(time3)\n",
    "aaaaaa = \"완료!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea86ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d589c2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257.723426580429"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda92b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "my_dl1 = dl_pipeline.fit(trainqq1) # second winow\n",
    "time3 = time.time() - time1\n",
    "print(time3)\n",
    "aaaaaa = \"완료!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f940fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "my_dl1 = dl_pipeline.fit(trainqq1) # second winow\n",
    "time3 = time.time() - time1\n",
    "print(time3)\n",
    "aaaaaa = \"완료!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "#pred_test = my_dl.transform(label_dft)\n",
    "pred_test = my_dl1.transform(testqq)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType()) \n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    pred_test[\"prediction\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")\n",
    "pred_test.printSchema()\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"],\n",
    "    pred_test[\"prediction2\"],\n",
    "    list_to_vector_udf(pred_test[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f936c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8159a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e357238e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd57d18ecc0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b309ffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_index</th>\n",
       "      <th>prediction2</th>\n",
       "      <th>rawPrediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.460475e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.644039e-20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.293934e-20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.934441e-19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.574041e-11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60323</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.997528e-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.365455e-20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.002725e-20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60326</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.088632e-11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60327</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.843647e-15</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60328 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_index   prediction2  rawPrediction\n",
       "0              1.0  6.460475e-07            1.0\n",
       "1              1.0  6.644039e-20            1.0\n",
       "2              1.0  3.293934e-20            1.0\n",
       "3              1.0  2.934441e-19            1.0\n",
       "4              1.0  3.574041e-11            1.0\n",
       "...            ...           ...            ...\n",
       "60323          0.0  2.997528e-11            0.0\n",
       "60324          0.0  7.365455e-20            0.0\n",
       "60325          0.0  1.002725e-20            0.0\n",
       "60326          0.0  7.088632e-11            0.0\n",
       "60327          0.0  6.843647e-15            0.0\n",
       "\n",
       "[60328 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63a08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1675ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc4a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a123 = np.array(trainqq.select(\"feature1\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1234 = np.array(trainqq.select(\"label_index\").collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15870628",
   "metadata": {},
   "source": [
    "# test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
