{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41fe929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "import pickle\n",
    "import keras\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54d7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier, RandomForestClassifier\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.ml.regression import IsotonicRegression\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272e2826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efa0351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc4876db4e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79951cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7fc4876d4eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_context = SQLContext(sc)\n",
    "sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "397e2ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = spark.read.csv(\"hdfs:///user/spark/datafile/1.csv\", header=True, inferSchema=True)\n",
    "a2 = spark.read.csv(\"hdfs:///user/spark/datafile/2.csv\", header=True, inferSchema=True)\n",
    "a3 = spark.read.csv(\"hdfs:///user/spark/datafile/3.csv\", header=True, inferSchema=True)\n",
    "a4 = spark.read.csv(\"hdfs:///user/spark/datafile/4.csv\", header=True, inferSchema=True)\n",
    "a5 = spark.read.csv(\"hdfs:///user/spark/datafile/5.csv\", header=True, inferSchema=True)\n",
    "a6 = spark.read.csv(\"hdfs:///user/spark/datafile/6.csv\", header=True, inferSchema=True)\n",
    "a7 = spark.read.csv(\"hdfs:///user/spark/datafile/7.csv\", header=True, inferSchema=True)\n",
    "a8 = spark.read.csv(\"hdfs:///user/spark/datafile/8.csv\", header=True, inferSchema=True)\n",
    "a9 = spark.read.csv(\"hdfs:///user/spark/datafile/9.csv\", header=True, inferSchema=True)\n",
    "a10 = spark.read.csv(\"hdfs:///user/spark/datafile/10.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bde5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loaded_data = csi_pos_neg.repartition(10000)\n",
    "loaded_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed824ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8307a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = loaded_data.select('DEVICE_ID').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e306971c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91aced27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TIMESTAMP', 'timestamp'),\n",
       " ('누적전력량', 'double'),\n",
       " ('무효전력평균', 'double'),\n",
       " ('상전압평균', 'double'),\n",
       " ('선간전압평균', 'double'),\n",
       " ('역률평균', 'double'),\n",
       " ('온도', 'double'),\n",
       " ('유효전력평균', 'double'),\n",
       " ('전류고조파평균', 'double'),\n",
       " ('전류평균', 'double'),\n",
       " ('전압고조파평균', 'double'),\n",
       " ('주파수', 'double'),\n",
       " ('DEVICE_ID', 'int'),\n",
       " ('LABEL_역률평균', 'int'),\n",
       " ('LABEL_전압고조파평균', 'int'),\n",
       " ('LABEL_전류고조파평균', 'int')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99e0c6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a1.dropna(how='any')\n",
    "a2 = a2.dropna(how='any')\n",
    "a3 = a3.dropna(how='any')\n",
    "a4 = a4.dropna(how='any')\n",
    "a5 = a5.dropna(how='any')\n",
    "a6 = a6.dropna(how='any')\n",
    "a7 = a7.dropna(how='any')\n",
    "a8 = a8.dropna(how='any')\n",
    "a9 = a9.dropna(how='any')\n",
    "a10 = a10.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81756fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403429"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91c24a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|DEVICE_ID|count|\n",
      "+---------+-----+\n",
      "|2257     |90697|\n",
      "|2259     |90654|\n",
      "|2258     |89280|\n",
      "|1289     |53272|\n",
      "|1288     |49574|\n",
      "|2297     |43200|\n",
      "|1385     |42983|\n",
      "|1287     |34482|\n",
      "|1290     |32019|\n",
      "|7255     |28800|\n",
      "|7247     |28800|\n",
      "|7253     |28800|\n",
      "|5764     |28800|\n",
      "|5754     |28800|\n",
      "|7277     |26715|\n",
      "|5640     |24480|\n",
      "|7301     |24480|\n",
      "|7303     |24480|\n",
      "|7299     |24480|\n",
      "|6521     |20160|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_data.groupBy(\"DEVICE_ID\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd213b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"/root/rounded_df\", \"rb\") as file:\n",
    "#     df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d146a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "452d3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8dc22bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------------+----------+------------+--------+------+------------+--------------+--------+--------------+---------+---------+--------------+--------------------+--------------------+----------+\n",
      "|          TIMESTAMP|누적전력량|무효전력평균|상전압평균|선간전압평균|역률평균|  온도|유효전력평균|전류고조파평균|전류평균|전압고조파평균|   주파수|DEVICE_ID|LABEL_역률평균|LABEL_전압고조파평균|LABEL_전류고조파평균|LABEL_NAME|\n",
      "+-------------------+----------+------------+----------+------------+--------+------+------------+--------------+--------+--------------+---------+---------+--------------+--------------------+--------------------+----------+\n",
      "|2021-01-19 00:00:48| 1523.2915|         0.0|132.916672|  230.083328|     0.0| 38.75|         0.0|           0.0|     0.0|      3.287761|59.835415|     7303|             2|                   0|                   0|       1.0|\n",
      "|2021-01-19 00:01:48| 1523.2915|         0.0|131.916672|  228.416672|     0.0|39.375|         0.0|           0.0|     0.0|      3.190104|59.835415|     7303|             2|                   0|                   0|       1.0|\n",
      "|2021-01-19 00:02:48| 1523.2915|         0.0|     132.0|  228.666672|     0.0| 38.75|         0.0|           0.0|     0.0|      3.483073| 59.84474|     7303|             2|                   0|                   0|       1.0|\n",
      "|2021-01-19 00:03:48| 1523.2915|         0.0|132.666672|      229.75|     0.0| 38.75|         0.0|           0.0|     0.0|      3.287761| 59.87274|     7303|             2|                   0|                   0|       1.0|\n",
      "|2021-01-19 00:04:48| 1523.2915|         0.0|132.666672|  229.666672|     0.0| 38.75|         0.0|           0.0|     0.0|      3.580729| 59.81677|     7303|             2|                   0|                   0|       1.0|\n",
      "+-------------------+----------+------------+----------+------------+--------+------+------------+--------------+--------+--------------+---------+---------+--------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e03d1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d06b92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = loaded_data.drop(col(\"LABEL_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eeb7a735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TIMESTAMP',\n",
       " '누적전력량',\n",
       " '무효전력평균',\n",
       " '상전압평균',\n",
       " '선간전압평균',\n",
       " '역률평균',\n",
       " '온도',\n",
       " '유효전력평균',\n",
       " '전류고조파평균',\n",
       " '전류평균',\n",
       " '전압고조파평균',\n",
       " '주파수',\n",
       " 'DEVICE_ID',\n",
       " 'LABEL_역률평균',\n",
       " 'LABEL_전압고조파평균',\n",
       " 'LABEL_전류고조파평균']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37826193",
   "metadata": {},
   "outputs": [],
   "source": [
    "idindexer1 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a1)\n",
    "idindexer2 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a2)\n",
    "idindexer3 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a3)\n",
    "idindexer4 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a4)\n",
    "idindexer5 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a5)\n",
    "idindexer6 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a6)\n",
    "idindexer7 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a7)\n",
    "idindexer8 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a8)\n",
    "idindexer9 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a9)\n",
    "idindexer10 = StringIndexer(inputCol='DEVICE_ID', outputCol=\"indexedDEVICE_ID\").fit(a10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2916254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df1 = idindexer1.transform(a1)\n",
    "id_df2 = idindexer2.transform(a2)\n",
    "id_df3 = idindexer3.transform(a3)\n",
    "id_df4 = idindexer4.transform(a4)\n",
    "id_df5 = idindexer5.transform(a5)\n",
    "id_df6 = idindexer6.transform(a6)\n",
    "id_df7 = idindexer7.transform(a7)\n",
    "id_df8 = idindexer8.transform(a8)\n",
    "id_df9 = idindexer9.transform(a9)\n",
    "id_df10 = idindexer10.transform(a10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c13cf7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|indexedDEVICE_ID|\n",
      "+----------------+\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "|            15.0|\n",
      "+----------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_df.select(\"indexedDEVICE_ID\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3d5b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['누적전력량', 'indexedDEVICE_ID', '전압고조파평균', '상전압평균', '선간전압평균', '온도', '무효전력평균', '전류고조파평균', '전류평균', '유효전력평균', '역률평균', '주파수']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4303e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=features, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1feb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "va_df1 = va.transform(id_df1)\n",
    "va_df2 = va.transform(id_df2)\n",
    "va_df3 = va.transform(id_df3)\n",
    "va_df4 = va.transform(id_df4)\n",
    "va_df5 = va.transform(id_df5)\n",
    "va_df6 = va.transform(id_df6)\n",
    "va_df7 = va.transform(id_df7)\n",
    "va_df8 = va.transform(id_df8)\n",
    "va_df9 = va.transform(id_df9)\n",
    "va_df10 = va.transform(id_df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e65d908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[TIMESTAMP: timestamp, 누적전력량: double, 무효전력평균: double, 상전압평균: double, 선간전압평균: double, 역률평균: double, 온도: double, 유효전력평균: double, 전류고조파평균: double, 전류평균: double, 전압고조파평균: double, 주파수: double, DEVICE_ID: int, LABEL_역률평균: int, LABEL_전압고조파평균: int, LABEL_전류고조파평균: int, indexedDEVICE_ID: double, features: vector]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "va_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01710bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_total = ['LABEL_역률평균', 'LABEL_전압고조파평균', 'LABEL_전류고조파평균']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeb1bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "va2 = VectorAssembler(inputCols=label_total, outputCol='LABEL_NAME2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b78782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "va2_df1 = va2.transform(va_df1)\n",
    "va2_df2 = va2.transform(va_df2)\n",
    "va2_df3 = va2.transform(va_df3)\n",
    "va2_df4 = va2.transform(va_df4)\n",
    "va2_df5 = va2.transform(va_df5)\n",
    "va2_df6 = va2.transform(va_df6)\n",
    "va2_df7 = va2.transform(va_df7)\n",
    "va2_df8 = va2.transform(va_df8)\n",
    "va2_df9 = va2.transform(va_df9)\n",
    "va2_df10 = va2.transform(va_df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36301fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|   LABEL_NAME|\n",
      "+-------------+\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va2_df.select(\"LABEL_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685ddaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|   LABEL_NAME|count|\n",
      "+-------------+-----+\n",
      "|[1.0,1.0,2.0]|  100|\n",
      "|[1.0,2.0,2.0]|  156|\n",
      "|[1.0,0.0,2.0]|  308|\n",
      "|[0.0,2.0,2.0]|  498|\n",
      "|[2.0,2.0,2.0]|  979|\n",
      "|[0.0,1.0,2.0]|  989|\n",
      "|[2.0,1.0,2.0]| 1498|\n",
      "|[1.0,1.0,1.0]| 2023|\n",
      "|[0.0,0.0,2.0]| 2694|\n",
      "|[2.0,0.0,2.0]| 3954|\n",
      "|[1.0,0.0,1.0]| 4601|\n",
      "|[1.0,2.0,1.0]| 6380|\n",
      "|[1.0,0.0,0.0]|10965|\n",
      "|[0.0,1.0,1.0]|11795|\n",
      "|[2.0,1.0,1.0]|12071|\n",
      "|[0.0,2.0,1.0]|13760|\n",
      "|[2.0,2.0,1.0]|14269|\n",
      "|[2.0,0.0,1.0]|14828|\n",
      "|[0.0,0.0,1.0]|17608|\n",
      "|[1.0,1.0,0.0]|19622|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va2_df.groupBy(\"LABEL_NAME\").count().orderBy(col(\"count\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "560f4689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TIMESTAMP: timestamp (nullable = true)\n",
      " |-- 누적전력량: double (nullable = true)\n",
      " |-- 무효전력평균: double (nullable = true)\n",
      " |-- 상전압평균: double (nullable = true)\n",
      " |-- 선간전압평균: double (nullable = true)\n",
      " |-- 역률평균: double (nullable = true)\n",
      " |-- 온도: double (nullable = true)\n",
      " |-- 유효전력평균: double (nullable = true)\n",
      " |-- 전류고조파평균: double (nullable = true)\n",
      " |-- 전류평균: double (nullable = true)\n",
      " |-- 전압고조파평균: double (nullable = true)\n",
      " |-- 주파수: double (nullable = true)\n",
      " |-- DEVICE_ID: integer (nullable = true)\n",
      " |-- LABEL_역률평균: integer (nullable = true)\n",
      " |-- LABEL_전압고조파평균: integer (nullable = true)\n",
      " |-- LABEL_전류고조파평균: integer (nullable = true)\n",
      " |-- indexedDEVICE_ID: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- LABEL_NAME: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd47f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer1 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df1)\n",
    "labelIndexer2 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df2)\n",
    "labelIndexer3 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df3)\n",
    "labelIndexer4 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df4)\n",
    "labelIndexer5 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df5)\n",
    "labelIndexer6 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df6)\n",
    "labelIndexer7 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df7)\n",
    "labelIndexer8 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df8)\n",
    "labelIndexer9 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df9)\n",
    "labelIndexer10 = VectorIndexer(inputCol='LABEL_NAME2', outputCol=\"indexedLabel\", maxCategories=28).fit(va2_df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39e69463",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_med1 = labelIndexer1.transform(va2_df1)\n",
    "df_med2 = labelIndexer2.transform(va2_df2)\n",
    "df_med3 = labelIndexer3.transform(va2_df3)\n",
    "df_med4 = labelIndexer4.transform(va2_df4)\n",
    "df_med5 = labelIndexer5.transform(va2_df5)\n",
    "df_med6 = labelIndexer6.transform(va2_df6)\n",
    "df_med7 = labelIndexer7.transform(va2_df7)\n",
    "df_med8 = labelIndexer8.transform(va2_df8)\n",
    "df_med9 = labelIndexer9.transform(va2_df9)\n",
    "df_med10 = labelIndexer10.transform(va2_df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53e127b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "| indexedLabel|\n",
      "+-------------+\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,0.0,0.0]|\n",
      "|[2.0,1.0,0.0]|\n",
      "|[2.0,1.0,0.0]|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_med.select(\"indexedLabel\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b55deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dense_format_udf = udf(lambda x: str(x), StringType())\n",
    "\n",
    "df1 = df_med1.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df2 = df_med2.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df3 = df_med3.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df4 = df_med4.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df5 = df_med5.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df6 = df_med6.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df7 = df_med7.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df8 = df_med8.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df9 = df_med9.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))\n",
    "df10 = df_med10.withColumn('indexedLabel', dense_format_udf(col('LABEL_NAME2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8db4e506",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TIMESTAMP: timestamp (nullable = true)\n",
      " |-- 누적전력량: double (nullable = true)\n",
      " |-- 무효전력평균: double (nullable = true)\n",
      " |-- 상전압평균: double (nullable = true)\n",
      " |-- 선간전압평균: double (nullable = true)\n",
      " |-- 역률평균: double (nullable = true)\n",
      " |-- 온도: double (nullable = true)\n",
      " |-- 유효전력평균: double (nullable = true)\n",
      " |-- 전류고조파평균: double (nullable = true)\n",
      " |-- 전류평균: double (nullable = true)\n",
      " |-- 전압고조파평균: double (nullable = true)\n",
      " |-- 주파수: double (nullable = true)\n",
      " |-- DEVICE_ID: integer (nullable = true)\n",
      " |-- LABEL_역률평균: integer (nullable = true)\n",
      " |-- LABEL_전압고조파평균: integer (nullable = true)\n",
      " |-- LABEL_전류고조파평균: integer (nullable = true)\n",
      " |-- indexedDEVICE_ID: double (nullable = false)\n",
      " |-- LABEL_NAME: vector (nullable = true)\n",
      " |-- indexedLabel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c7466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.util import MLUtils\n",
    "# from pyspark.ml.feature import StandardScaler\n",
    "# scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cd1b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = scaler.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c89d6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_aa = StringIndexer(inputCol='indexedLabel', outputCol=\"label\").fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78b2bdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = label_aa.fit(df1).transform(df1)\n",
    "temp2 = label_aa.fit(df2).transform(df2)\n",
    "temp3 = label_aa.fit(df3).transform(df3)\n",
    "temp4 = label_aa.fit(df4).transform(df4)\n",
    "temp5 = label_aa.fit(df5).transform(df5)\n",
    "temp6 = label_aa.fit(df6).transform(df6)\n",
    "temp7 = label_aa.fit(df7).transform(df7)\n",
    "temp8 = label_aa.fit(df8).transform(df8)\n",
    "temp9 = label_aa.fit(df9).transform(df9)\n",
    "temp10 = label_aa.fit(df10).transform(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8a3f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = label_aa.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "353635b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label|count |\n",
      "+-----+------+\n",
      "|0.0  |181956|\n",
      "|1.0  |173408|\n",
      "|2.0  |119804|\n",
      "|3.0  |81100 |\n",
      "|4.0  |54057 |\n",
      "|5.0  |36171 |\n",
      "|6.0  |33655 |\n",
      "|7.0  |19622 |\n",
      "|8.0  |17608 |\n",
      "|9.0  |14828 |\n",
      "|10.0 |14269 |\n",
      "|11.0 |13760 |\n",
      "|12.0 |12071 |\n",
      "|13.0 |11795 |\n",
      "|14.0 |10965 |\n",
      "|15.0 |6380  |\n",
      "|16.0 |4601  |\n",
      "|17.0 |3954  |\n",
      "|18.0 |2694  |\n",
      "|19.0 |2023  |\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8b54040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TIMESTAMP', 'timestamp'),\n",
       " ('누적전력량', 'double'),\n",
       " ('무효전력평균', 'double'),\n",
       " ('상전압평균', 'double'),\n",
       " ('선간전압평균', 'double'),\n",
       " ('역률평균', 'double'),\n",
       " ('온도', 'double'),\n",
       " ('유효전력평균', 'double'),\n",
       " ('전류고조파평균', 'double'),\n",
       " ('전류평균', 'double'),\n",
       " ('전압고조파평균', 'double'),\n",
       " ('주파수', 'double'),\n",
       " ('DEVICE_ID', 'int'),\n",
       " ('LABEL_역률평균', 'int'),\n",
       " ('LABEL_전압고조파평균', 'int'),\n",
       " ('LABEL_전류고조파평균', 'int'),\n",
       " ('indexedDEVICE_ID', 'double'),\n",
       " ('LABEL_NAME', 'vector'),\n",
       " ('indexedLabel', 'string'),\n",
       " ('label', 'double')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d84ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = temp1.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c2 = temp2.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c3 = temp3.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c4 = temp4.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c5 = temp5.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c6 = temp6.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c7 = temp7.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c8 = temp8.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c9 = temp9.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))\n",
    "c10 = temp10.drop(col(\"LABEL_NAME\")).drop(col(\"indexedLabel\")).drop(col(\"indexedDEVICE_ID\")).drop(col(\"LABEL_NAME2\")).drop(col(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed4ec5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TIMESTAMP', 'timestamp'),\n",
       " ('누적전력량', 'double'),\n",
       " ('무효전력평균', 'double'),\n",
       " ('상전압평균', 'double'),\n",
       " ('선간전압평균', 'double'),\n",
       " ('역률평균', 'double'),\n",
       " ('온도', 'double'),\n",
       " ('유효전력평균', 'double'),\n",
       " ('전류고조파평균', 'double'),\n",
       " ('전류평균', 'double'),\n",
       " ('전압고조파평균', 'double'),\n",
       " ('주파수', 'double'),\n",
       " ('DEVICE_ID', 'int'),\n",
       " ('LABEL_역률평균', 'int'),\n",
       " ('LABEL_전압고조파평균', 'int'),\n",
       " ('LABEL_전류고조파평균', 'int'),\n",
       " ('label', 'double')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "936ee6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c1.write.csv(\"hdfs:///user/spark/datafile/c1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ca371d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c1.csv\")\n",
    "c2.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c2.csv\")\n",
    "c3.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c3.csv\")\n",
    "c4.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c4.csv\")\n",
    "c5.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c5.csv\")\n",
    "c6.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c6.csv\")\n",
    "c7.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c7.csv\")\n",
    "c8.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c8.csv\")\n",
    "c9.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c9.csv\")\n",
    "c10.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"hdfs:///user/spark/datafile/c10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bb2fde0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`label`' given input columns: [유효전력평균, 전류평균, LABEL_NAME, LABEL_전류고조파평균, indexedLabel, 역률평균, 전류고조파평균, indexedDEVICE_ID, TIMESTAMP, 전압고조파평균, 무효전력평균, LABEL_전압고조파평균, 선간전압평균, 주파수, 온도, LABEL_역률평균, 누적전력량, 상전압평균, DEVICE_ID];;\\n'Aggregate ['label], ['label, count(1) AS count#385L]\\n+- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, LABEL_NAME#181, <lambda>(LABEL_NAME#181) AS indexedLabel#304]\\n   +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, LABEL_NAME#181, UDF(LABEL_NAME#181) AS indexedLabel#261]\\n      +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, UDF(named_struct(LABEL_역률평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_역률평균#23 as double), LABEL_전압고조파평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_전압고조파평균#24 as double), LABEL_전류고조파평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_전류고조파평균#25 as double))) AS LABEL_NAME#181]\\n         +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, UDF(cast(DEVICE_ID#22 as string)) AS indexedDEVICE_ID#160]\\n            +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25]\\n               +- Filter AtLeastNNulls(n, TIMESTAMP#10,누적전력량#11,무효전력평균#12,상전압평균#13,선간전압평균#14,역률평균#15,온도#16,유효전력평균#17,전류고조파평균#18,전류평균#19,전압고조파평균#20,주파수#21,DEVICE_ID#22,LABEL_역률평균#23,LABEL_전압고조파평균#24,LABEL_전류고조파평균#25,LABEL_NAME#26)\\n                  +- Relation[TIMESTAMP#10,누적전력량#11,무효전력평균#12,상전압평균#13,선간전압평균#14,역률평균#15,온도#16,유효전력평균#17,전류고조파평균#18,전류평균#19,전압고조파평균#20,주파수#21,DEVICE_ID#22,LABEL_역률평균#23,LABEL_전압고조파평균#24,LABEL_전류고조파평균#25,LABEL_NAME#26] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o284.count.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`label`' given input columns: [유효전력평균, 전류평균, LABEL_NAME, LABEL_전류고조파평균, indexedLabel, 역률평균, 전류고조파평균, indexedDEVICE_ID, TIMESTAMP, 전압고조파평균, 무효전력평균, LABEL_전압고조파평균, 선간전압평균, 주파수, 온도, LABEL_역률평균, 누적전력량, 상전압평균, DEVICE_ID];;\n'Aggregate ['label], ['label, count(1) AS count#385L]\n+- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, LABEL_NAME#181, <lambda>(LABEL_NAME#181) AS indexedLabel#304]\n   +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, LABEL_NAME#181, UDF(LABEL_NAME#181) AS indexedLabel#261]\n      +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, UDF(named_struct(LABEL_역률평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_역률평균#23 as double), LABEL_전압고조파평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_전압고조파평균#24 as double), LABEL_전류고조파평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_전류고조파평균#25 as double))) AS LABEL_NAME#181]\n         +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, UDF(cast(DEVICE_ID#22 as string)) AS indexedDEVICE_ID#160]\n            +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25]\n               +- Filter AtLeastNNulls(n, TIMESTAMP#10,누적전력량#11,무효전력평균#12,상전압평균#13,선간전압평균#14,역률평균#15,온도#16,유효전력평균#17,전류고조파평균#18,전류평균#19,전압고조파평균#20,주파수#21,DEVICE_ID#22,LABEL_역률평균#23,LABEL_전압고조파평균#24,LABEL_전류고조파평균#25,LABEL_NAME#26)\n                  +- Relation[TIMESTAMP#10,누적전력량#11,무효전력평균#12,상전압평균#13,선간전압평균#14,역률평균#15,온도#16,유효전력평균#17,전류고조파평균#18,전류평균#19,전압고조파평균#20,주파수#21,DEVICE_ID#22,LABEL_역률평균#23,LABEL_전압고조파평균#24,LABEL_전류고조파평균#25,LABEL_NAME#26] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.count(RelationalGroupedDataset.scala:237)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8edcd11e579f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`label`' given input columns: [유효전력평균, 전류평균, LABEL_NAME, LABEL_전류고조파평균, indexedLabel, 역률평균, 전류고조파평균, indexedDEVICE_ID, TIMESTAMP, 전압고조파평균, 무효전력평균, LABEL_전압고조파평균, 선간전압평균, 주파수, 온도, LABEL_역률평균, 누적전력량, 상전압평균, DEVICE_ID];;\\n'Aggregate ['label], ['label, count(1) AS count#385L]\\n+- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, LABEL_NAME#181, <lambda>(LABEL_NAME#181) AS indexedLabel#304]\\n   +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, LABEL_NAME#181, UDF(LABEL_NAME#181) AS indexedLabel#261]\\n      +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, indexedDEVICE_ID#160, UDF(named_struct(LABEL_역률평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_역률평균#23 as double), LABEL_전압고조파평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_전압고조파평균#24 as double), LABEL_전류고조파평균_double_VectorAssembler_23435fdb3c7f, cast(LABEL_전류고조파평균#25 as double))) AS LABEL_NAME#181]\\n         +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25, UDF(cast(DEVICE_ID#22 as string)) AS indexedDEVICE_ID#160]\\n            +- Project [TIMESTAMP#10, 누적전력량#11, 무효전력평균#12, 상전압평균#13, 선간전압평균#14, 역률평균#15, 온도#16, 유효전력평균#17, 전류고조파평균#18, 전류평균#19, 전압고조파평균#20, 주파수#21, DEVICE_ID#22, LABEL_역률평균#23, LABEL_전압고조파평균#24, LABEL_전류고조파평균#25]\\n               +- Filter AtLeastNNulls(n, TIMESTAMP#10,누적전력량#11,무효전력평균#12,상전압평균#13,선간전압평균#14,역률평균#15,온도#16,유효전력평균#17,전류고조파평균#18,전류평균#19,전압고조파평균#20,주파수#21,DEVICE_ID#22,LABEL_역률평균#23,LABEL_전압고조파평균#24,LABEL_전류고조파평균#25,LABEL_NAME#26)\\n                  +- Relation[TIMESTAMP#10,누적전력량#11,무효전력평균#12,상전압평균#13,선간전압평균#14,역률평균#15,온도#16,유효전력평균#17,전류고조파평균#18,전류평균#19,전압고조파평균#20,주파수#21,DEVICE_ID#22,LABEL_역률평균#23,LABEL_전압고조파평균#24,LABEL_전류고조파평균#25,LABEL_NAME#26] csv\\n\""
     ]
    }
   ],
   "source": [
    "df.groupBy(\"indexedLabel\").count().orderBy(col(\"count\").desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "865f7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = labelIndexer.transform(va_df)\n",
    "# a.groupBy(\"indexedLabel\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2135ad80",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=cnt+1).fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbfacca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca_model = PCA(k = 9,inputCol = \"indexedFeatures\", outputCol = \"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9855cd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed7e109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2403429"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b30126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "467c2326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c317835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcc34ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "gbt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"indexedFeatures\", maxBins = cnt+1, maxDepth=30)\n",
    "# paramGrid = ParamGridBuilder().addGrid(gbt.maxBins,77).addGrid(gbt.maxIter, [4, 6, 8]).addGrid(gbt.lossType, [\"entropy\", \"gini\"]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fea01dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = Pipeline(stages=[label_aa, featureIndexer, pca_model, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b710b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[label_aa, featureIndexer, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbf013a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e832eb55c797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "model = pipeline.fit(train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79dfd9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14b45cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train, test) = predictions.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ca2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fea3c01c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"pca_features\" does not exist.\\nAvailable fields: TIMESTAMP, 누적전력량, 무효전력평균, 상전압평균, 선간전압평균, 역률평균, 온도, 유효전력평균, 전류고조파평균, 전류평균, 전압고조파평균, 주파수, DEVICE_ID, LABEL_역률평균, LABEL_전압고조파평균, LABEL_전류고조파평균, indexedDEVICE_ID, features, LABEL_NAME, indexedLabel, CrossValidator_990ca32bcf57_rand'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o710.fit.\n: java.lang.IllegalArgumentException: Field \"pca_features\" does not exist.\nAvailable fields: TIMESTAMP, 누적전력량, 무효전력평균, 상전압평균, 선간전압평균, 역률평균, 온도, 유효전력평균, 전류고조파평균, 전류평균, 전압고조파평균, 주파수, DEVICE_ID, LABEL_역률평균, LABEL_전압고조파평균, LABEL_전류고조파평균, indexedDEVICE_ID, features, LABEL_NAME, indexedLabel, CrossValidator_990ca32bcf57_rand\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-31749288f05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     numFolds=5)\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"pca_features\" does not exist.\\nAvailable fields: TIMESTAMP, 누적전력량, 무효전력평균, 상전압평균, 선간전압평균, 역률평균, 온도, 유효전력평균, 전류고조파평균, 전류평균, 전압고조파평균, 주파수, DEVICE_ID, LABEL_역률평균, LABEL_전압고조파평균, LABEL_전류고조파평균, indexedDEVICE_ID, features, LABEL_NAME, indexedLabel, CrossValidator_990ca32bcf57_rand'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#gbt = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"pca_features\", numTrees = 20)\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"pca_features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .addGrid(lr.maxIter, [10, 20, 50]) #Number of iterations\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(1.0 - accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743c774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d7771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a7d7db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|[19235.63,6.0,7.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,7.1...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,6.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.2...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.4...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.6...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,5.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.5...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.1...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.0...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.3...|\n",
      "|       0.0|  0.0|[19235.63,6.0,4.1...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.7...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.9...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.8...|\n",
      "|       0.0|  0.0|[19235.63,6.0,3.9...|\n",
      "|       1.0|  1.0|[19235.63,6.0,4.4...|\n",
      "|       0.0|  1.0|[19235.63,6.0,5.2...|\n",
      "|       1.0|  1.0|[19235.63,6.0,3.9...|\n",
      "|       0.0|  0.0|[19292.9336,6.0,3...|\n",
      "|       0.0|  0.0|[19334.4922,6.0,3...|\n",
      "|       1.0|  1.0|[19630.2773,6.0,3...|\n",
      "|       0.0|  1.0|[19714.9668,6.0,2...|\n",
      "|       1.0|  1.0|[19921.8613,6.0,3...|\n",
      "|       1.0|  1.0|[19965.0254,6.0,3...|\n",
      "|       1.0|  1.0|[20264.7031,6.0,2...|\n",
      "|       7.0|  7.0|[21013.541,6.0,4....|\n",
      "|       7.0|  1.0|[21518.5254,6.0,3...|\n",
      "|      14.0| 14.0|[21918.7676,6.0,2...|\n",
      "|      14.0|  7.0|[22057.0176,6.0,2...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e304111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|36699|\n",
      "|  1.0|34484|\n",
      "|  2.0|24258|\n",
      "|  3.0|16160|\n",
      "|  4.0|10807|\n",
      "|  5.0| 7321|\n",
      "|  6.0| 6777|\n",
      "|  7.0| 3961|\n",
      "|  8.0| 3513|\n",
      "|  9.0| 2902|\n",
      "| 10.0| 2847|\n",
      "| 11.0| 2692|\n",
      "| 13.0| 2413|\n",
      "| 12.0| 2300|\n",
      "| 14.0| 2147|\n",
      "| 15.0| 1287|\n",
      "| 16.0|  929|\n",
      "| 17.0|  786|\n",
      "| 18.0|  514|\n",
      "| 19.0|  435|\n",
      "| 20.0|  306|\n",
      "| 21.0|  229|\n",
      "| 22.0|  201|\n",
      "| 23.0|   99|\n",
      "| 24.0|   71|\n",
      "| 25.0|   21|\n",
      "| 26.0|   18|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a8b892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0|36904|\n",
      "|       1.0|34211|\n",
      "|       2.0|24331|\n",
      "|       3.0|16254|\n",
      "|       4.0|10738|\n",
      "|       5.0| 7235|\n",
      "|       6.0| 6789|\n",
      "|       7.0| 3973|\n",
      "|       8.0| 3551|\n",
      "|       9.0| 2888|\n",
      "|      10.0| 2754|\n",
      "|      11.0| 2683|\n",
      "|      12.0| 2386|\n",
      "|      13.0| 2379|\n",
      "|      14.0| 2176|\n",
      "|      15.0| 1293|\n",
      "|      16.0|  903|\n",
      "|      17.0|  802|\n",
      "|      18.0|  543|\n",
      "|      19.0|  436|\n",
      "|      20.0|  307|\n",
      "|      21.0|  219|\n",
      "|      22.0|  194|\n",
      "|      23.0|   99|\n",
      "|      24.0|   70|\n",
      "|      26.0|   32|\n",
      "|      25.0|   27|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\"prediction\").count().orderBy(col(\"count\").desc()).show(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c27452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4611893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.236113\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69ad80f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7638868866514412\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "607943fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "aa = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e174297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7634323776083484"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06479efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "cc = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2fbca826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631222872574142"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9adf7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
