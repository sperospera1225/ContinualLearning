{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a72d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYSPARK_PYTHON=python3.6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b5fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ed70fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import tensorflow.keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json\n",
    "from tensorflow import keras\n",
    "\n",
    "# import tensorflow.keras\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from numpy import zeros\n",
    "# from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "719324c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.4\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926146e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.sql.execution.arrow.enabled', 'true'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.cores', '5'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.host', 'kafka1'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.yarn.archive', 'hdfs:///user/spark/conf/spark-libs.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.kryoserializer.buffer.max', '2047'),\n",
       " ('spark.executor.memoryOverhead', '1g'),\n",
       " ('spark.driver.memoryOverhead', '1g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'file:///root/spark/eventLog'),\n",
       " ('spark.default.parallelism', '163'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.network.timeout', '3600s'),\n",
       " ('spark.driver.appUIAddress', 'http://kafka1:4040'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/root/spark/python/lib/py4j-0.10.7-src.zip:/root/spark/python/:/python:/python/lib/py4j-0.10.7-src.zip:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://0.0.0.0:8089/proxy/application_1643192276452_0057'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.app.id', 'application_1643192276452_0057'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  '0.0.0.0'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1643192276452_0057'),\n",
       " ('spark.eventLog.dir', 'file:///root/spark/eventLog'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '45033')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f7c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "connection = MongoClient('mongodb://117.17.189.202:27027')\n",
    "tweet = connection.test\n",
    "kafka_tweet = tweet.kafka_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6cf9a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @nanox_finance: #Airdrop Alert üö® \\n\\n@nanox...\n",
       "1                    */inamin HAHAHAHAHAHAHAHAHAHAHAHA\n",
       "2    4.  hai yang mau sewa zoom, unlock chegg,cek t...\n",
       "3               @AEDi32 para sa concert HAHAHAHAHAHAHA\n",
       "4    RT @KariOrenday: Lo de hoy es culpa de dos, Sa...\n",
       "5    RT @BTS_jp_official: #BTS ARTIST-MADE COLLECTI...\n",
       "6    RT @esbingcali: Para sa akin, ‚ÄúWHAT?‚Äù ang titl...\n",
       "7    Figueiredo enfin il r√©cup√©rer sa ceinture mdrr...\n",
       "8    @kisshmtc_16229 „ÅîÂøúÂãü„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åó„Åü‚ùó\\nÁµêÊûú„ÅØ\\n\\nÊÆãÂøµ‚Ä¶„ÅØ„Åö„Çå...\n",
       "9    RT @kep1er_worrld: [ADMIN] 220123 - Kep1ians w...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "doc = kafka_tweet.find({})\n",
    "dfdf =  pd.DataFrame(list(doc))\n",
    "dfdf['text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c32aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏûÑÏãúÏΩîÎìú\n",
    "csi_pos_neg = spark.read.csv(\"hdfs:///user/spark/datafile/csiposneg.csv\", header=True)\n",
    "csi_pos_neg = csi_pos_neg.withColumn(\"label\",col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa26455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/root/keywordset.txt', 'r', encoding='utf-8')\n",
    "keyfile = f.readline()\n",
    "keyword_list = list()\n",
    "keywords = keyfile.split(',')\n",
    "for keyword in keywords:\n",
    "    keyword_list.append(keyword.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b5e2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "df1 = dfdf[:1101600][['text']]\n",
    "df1 = spark.createDataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85918de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  An error occurred while calling z:org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile.\n",
      ": java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.readNextBatch(ArrowConverters.scala:243)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$3.<init>(ArrowConverters.scala:229)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.getBatchesFromStream(ArrowConverters.scala:228)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:216)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$readArrowStreamFromFile$2.apply(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2543)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.readArrowStreamFromFile(ArrowConverters.scala:214)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils$.readArrowStreamFromFile(PythonSQLUtils.scala:46)\n",
      "\tat org.apache.spark.sql.api.python.PythonSQLUtils.readArrowStreamFromFile(PythonSQLUtils.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "df2 = dfdf[1101600:2203200][['text']]\n",
    "df2 = spark.createDataFrame(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07999207",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = csi_pos_neg.collect()[:24939]\n",
    "t2 = csi_pos_neg.collect()[24939:49878]\n",
    "t3 = csi_pos_neg.collect()[49878:89756]\n",
    "t4 = csi_pos_neg.collect()[89756:99756]\n",
    "\n",
    "t5 = csi_pos_neg.collect()[99756:124695]\n",
    "t6 = csi_pos_neg.collect()[124695:149634]\n",
    "t7 = csi_pos_neg.collect()[149634:189512]\n",
    "t8 = csi_pos_neg.collect()[189512:199512]\n",
    "\n",
    "t1 = spark.createDataFrame(t1)\n",
    "t2 = spark.createDataFrame(t2)\n",
    "t3 = spark.createDataFrame(t3)\n",
    "t4 = spark.createDataFrame(t4)\n",
    "\n",
    "t5 = spark.createDataFrame(t5)\n",
    "t6 = spark.createDataFrame(t6)\n",
    "t7 = spark.createDataFrame(t7)\n",
    "t8 = spark.createDataFrame(t8)\n",
    "\n",
    "first_df = t1.union(t5).union(t2).union(t6) #.union(t3).union(t7) # 179512 \n",
    "first_df_df = t3.union(t7)\n",
    "second_df = t4.union(t8) # 20000 -> unrevealed data for all periods\n",
    "first_df = first_df.drop(col('_c0'))\n",
    "first_df_df = first_df_df.drop(col('_c0'))\n",
    "second_df = second_df.drop(col('_c0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### 2Î≤àÏß∏ time window##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481cf674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare_and_drop for semi-supervised learning\n",
    "df = spark.read.csv(\"hdfs:///user/spark/datafile/first_df.csv\", header=True)\n",
    "df = df.toPandas()\n",
    "df = df.astype({'text':'string'})\n",
    "df = df.astype({'label':'double'})\n",
    "\n",
    "df_1 = df[df['label']==1.0]\n",
    "length = len(df_1)\n",
    "df_2 = df[df['label']==0.0][:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24e3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_train = df_1[:-10000]\n",
    "df_1_test = df_1[-10000:]\n",
    "df_2_train = df_2[:-10000]\n",
    "df_2_test = df_2[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a1995cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_df = pd.concat([df_1_test, df_2_test])\n",
    "plus_df.reset_index(inplace=True, drop=True)\n",
    "plus_df.drop('_c0', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c63c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Cannot specify a mask or a size when passing an object that is converted with the __arrow_array__ protocol.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(len(plus_df))\n",
    "plus_df = spark.createDataFrame(plus_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d260466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|jvndboracle peopl...|    1|\n",
      "|lazyconsultant wi...|    1|\n",
      "|avec peuttre un p...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_df = second_df.drop(col('_c0'))\n",
    "second_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f2536cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a83506b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_df = second_df.union(plus_df) # 4ÎßåÍ∞úÎ°ú ÎäòÏñ¥ÎÇ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ae785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>figueiredo enfin il rcuprer sa ceinture mdrr i...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>mercedesamgf in  but on other sport</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>rt shezzcryptogem the new standard for home re...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>im zizzy and this is my friend pony</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>i love that this has already its crusty gif tr...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202209</th>\n",
       "      <td>128349</td>\n",
       "      <td>rt zorochi      hr date hengsurpriselink   ht</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202210</th>\n",
       "      <td>128350</td>\n",
       "      <td>rt siphillipssport i am told that the players ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202211</th>\n",
       "      <td>128351</td>\n",
       "      <td>anwdverpkbczx twitter</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202212</th>\n",
       "      <td>128352</td>\n",
       "      <td>rt gppulipaka crossadversarial learning for mo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202213</th>\n",
       "      <td>128353</td>\n",
       "      <td>rt rockeytezz aaj morning se josh high krna ha...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202214 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           _c0                                               text  label\n",
       "0            5  figueiredo enfin il rcuprer sa ceinture mdrr i...    1.0\n",
       "1           19                mercedesamgf in  but on other sport    1.0\n",
       "2           27  rt shezzcryptogem the new standard for home re...    1.0\n",
       "3           33                im zizzy and this is my friend pony    1.0\n",
       "4           39  i love that this has already its crusty gif tr...    1.0\n",
       "...        ...                                                ...    ...\n",
       "202209  128349      rt zorochi      hr date hengsurpriselink   ht    0.0\n",
       "202210  128350  rt siphillipssport i am told that the players ...    0.0\n",
       "202211  128351                             anwdverpkbczx twitter     0.0\n",
       "202212  128352  rt gppulipaka crossadversarial learning for mo...    0.0\n",
       "202213  128353  rt rockeytezz aaj morning se josh high krna ha...    0.0\n",
       "\n",
       "[202214 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_df = pd.concat([df_1_train, df_2_train])\n",
    "first_df.reset_index(inplace=True, drop=True)\n",
    "first_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50e39e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/spark/python/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  Cannot specify a mask or a size when passing an object that is converted with the __arrow_array__ protocol.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "first_df.drop('_c0', axis=1, inplace=True)\n",
    "first_df.dtypes\n",
    "first_df = spark.createDataFrame(first_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e76ffe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class voc:\n",
    "    staticList = list() # ÌòÑÏû¨Îäî Í≥ÑÏÜçÌï¥ÏÑú incrementalÌïòÍ≤å ÏÇ¨Ïù¥Ï¶àÍ∞Ä Ïª§ÏßÄÍ≥† ÏûàÎã§\n",
    "\n",
    "class word_index:\n",
    "    staticDict = dict() # ÌòÑÏû¨Îäî Í≥ÑÏÜçÌï¥ÏÑú incrementalÌïòÍ≤å ÏÇ¨Ïù¥Ï¶àÍ∞Ä Ïª§ÏßÄÍ≥† ÏûàÎã§\n",
    "\n",
    "class vocabulary:\n",
    "    staticList = list()\n",
    "    \n",
    "class aaa:\n",
    "    staticList = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a74baefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index.staticDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ba6bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2_voc.pkl','wb') as f: \n",
    "    pickle.dump(voc.staticList,f)\n",
    "    \n",
    "with open('2_word_index.pkl','wb') as f: \n",
    "    pickle.dump(word_index.staticDict,f)\n",
    "    \n",
    "with open('2_vocabulary.pkl','wb') as f: \n",
    "    pickle.dump(vocabulary.staticList,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dbf6cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('1_voc.pkl','rb') as f: \n",
    "    voc.staticList = pickle.load(f)\n",
    "    \n",
    "with open('1_word_index.pkl','rb') as f: \n",
    "    word_index.staticDict = pickle.load(f)\n",
    "    \n",
    "with open('1_vocabulary.pkl','rb') as f: \n",
    "    vocabulary.staticList = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c164a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Í≥ÑÏÜçÌï¥ÏÑú ÏÑ†Ïñ∏Ìï¥Ï£ºÍ∏∞\n",
    "class a:\n",
    "    length = len(voc.staticList) # ÌòÑÏû¨Îäî Í≥ÑÏÜçÌï¥ÏÑú incrementalÌïòÍ≤å ÏÇ¨Ïù¥Ï¶àÍ∞Ä Ïª§ÏßÄÍ≥† ÏûàÎã§\n",
    "\n",
    "class b:\n",
    "    length = len(word_index.staticDict) # ÌòÑÏû¨Îäî Í≥ÑÏÜçÌï¥ÏÑú incrementalÌïòÍ≤å ÏÇ¨Ïù¥Ï¶àÍ∞Ä Ïª§ÏßÄÍ≥† ÏûàÎã§\n",
    "\n",
    "class c:\n",
    "    length = len(vocabulary.staticList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9033e337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index.staticDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "558fabcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc.staticList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e82aff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class TextToSequence(Transformer):\n",
    "    vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = [' '.join(sample) for sample in df.select(\"filtered_words\").rdd.flatMap(lambda x: x).collect()]\n",
    "        \n",
    "        labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "        words = df.select(\"filtered_words\").rdd.flatMap(lambda x: x).collect()\n",
    "        # 200Ïùò Í∏∏Ïù¥Î°ú ÏÉÅÏúÑ 2ÎßåÍ∞úÏùò ÌÜ†ÌÅ∞Îßå Î∞òÏòÅ\n",
    "        text_ds = tf.data.Dataset.from_tensor_slices(samples).batch(128)\n",
    "        self.vectorizer.adapt(text_ds)\n",
    "        \n",
    "        temp = voc.staticList\n",
    "        print(len(temp))\n",
    "        # ÏóÑÎç∞Ïù¥Ìä∏ ÌïòÎäî ÏãùÏúºÎ°ú Íµ¨ÌòÑ ÎèôÏûë Î∞îÍæ∏Í∏∞!\n",
    "        voc.staticList = list(set(voc.staticList + self.vectorizer.get_vocabulary()))\n",
    "        \n",
    "        aa = list(set(self.vectorizer.get_vocabulary())-set(temp))\n",
    "        print(len(aa),\"!\")\n",
    "        \n",
    "        post_length = len(voc.staticList)\n",
    "        vocabulary.staticList = np.concatenate(words).tolist()\n",
    "        word_index.staticDict.update(dict(zip(aa, range(a.length, a.length+len(aa)))))\n",
    "        \n",
    "        x_train = self.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, labels, x_train.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text','label','feature'])\n",
    "        \n",
    "        return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4fec3d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`filtered_words`' given input columns: [text, label, feature, label_index];;\\n'Project ['filtered_words]\\n+- Project [text#138, label#139L, feature#140, UDF(cast(label#139L as string)) AS label_index#144]\\n   +- LogicalRDD [text#138, label#139L, feature#140], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o563.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`filtered_words`' given input columns: [text, label, feature, label_index];;\n'Project ['filtered_words]\n+- Project [text#138, label#139L, feature#140, UDF(cast(label#139L as string)) AS label_index#144]\n   +- LogicalRDD [text#138, label#139L, feature#140], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e3886585358f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_words\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \"\"\"\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`filtered_words`' given input columns: [text, label, feature, label_index];;\\n'Project ['filtered_words]\\n+- Project [text#138, label#139L, feature#140, UDF(cast(label#139L as string)) AS label_index#144]\\n   +- LogicalRDD [text#138, label#139L, feature#140], false\\n\""
     ]
    }
   ],
   "source": [
    "[' '.join(sample) for sample in preprocessed.select(\"filtered_words\").rdd.flatMap(lambda x: x).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8929e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## old version\n",
    "from pyspark.ml import Transformer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class TextToSequence(Transformer):\n",
    "    vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextToSequence, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "        words = df.select(\"filtered_words\").rdd.flatMap(lambda x: x).collect()\n",
    "        # 200Ïùò Í∏∏Ïù¥Î°ú ÏÉÅÏúÑ 2ÎßåÍ∞úÏùò ÌÜ†ÌÅ∞Îßå Î∞òÏòÅ\n",
    "        #vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "        text_ds = tf.data.Dataset.from_tensor_slices(samples).batch(128)\n",
    "        self.vectorizer.adapt(text_ds)\n",
    "        \n",
    "        temp = voc.staticList\n",
    "        print(len(temp))\n",
    "        # ÏóÑÎç∞Ïù¥Ìä∏ ÌïòÎäî ÏãùÏúºÎ°ú Íµ¨ÌòÑ ÎèôÏûë Î∞îÍæ∏Í∏∞!\n",
    "        voc.staticList = list(set(voc.staticList + self.vectorizer.get_vocabulary()))\n",
    "        \n",
    "        aa = list(set(self.vectorizer.get_vocabulary())-set(temp))\n",
    "        aaa.staticList = list(set(aaa.staticList + aa))\n",
    "        print(len(aa),\"!\")\n",
    "        \n",
    "        post_length = len(voc.staticList)\n",
    "        vocabulary.staticList = np.concatenate(words).tolist()\n",
    "        #np.array(words).flatten()# samplesÏóê Ï°¥Ïû¨ÌïòÎäî Îç∞Ïù¥ÌÑ∞ Î¶¨Ïä§Ìä∏ (Ï§ëÎ≥µÌóàÏö©)\n",
    "        #word_index.staticDict = dict(zip(voc.staticList, range(len(voc.staticList))))\n",
    "        word_index.staticDict.update(dict(zip(aa, range(a.length, a.length+len(aa)))))\n",
    "        \n",
    "        x_train = self.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, labels, x_train.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text','label','feature'])\n",
    "        \n",
    "        return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5b9e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000 !\n",
      "20000\n",
      "0 !\n"
     ]
    }
   ],
   "source": [
    "# get_tokens Ìï®ÏàòÍ≥º Í∞ôÏùÄ Ïö©ÎèÑ\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')\n",
    "# get_lemma Ìï®ÏàòÏôÄ Í∞ôÏùÄ Ïö©ÎèÑ\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')\n",
    "stage_3 = TextToSequence()\n",
    "stage_4 = StringIndexer(inputCol='label', outputCol='label_index')\n",
    "#label_df = label_str_index.fit(df_with_vectors).transform(df_with_vectors)\n",
    "transformer_pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4])\n",
    "preprocessing = transformer_pipeline.fit(first_df)\n",
    "preprocessed = preprocessing.transform(first_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21dd8d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'mobile': 1,\n",
       " 'filename': 2,\n",
       " 'circle': 3,\n",
       " 'fingerprints': 4,\n",
       " 'cardiology': 5,\n",
       " 'radeon': 6,\n",
       " 'review': 7,\n",
       " 'arte': 8,\n",
       " 'grilled': 9,\n",
       " 'uba': 10,\n",
       " 'lazysunday': 11,\n",
       " 'arti': 12,\n",
       " 'wired': 13,\n",
       " 'jug': 14,\n",
       " 'evolution': 15,\n",
       " 'rex': 16,\n",
       " 'why': 17,\n",
       " 'kale': 18,\n",
       " 'brandon': 19,\n",
       " 'cooper': 20,\n",
       " 'cranberry': 21,\n",
       " 'hi': 22,\n",
       " 'voltaire': 23,\n",
       " 'hells': 24,\n",
       " 'ela': 25,\n",
       " 'gu': 26,\n",
       " 'networkcentric': 27,\n",
       " 'uxbridge': 28,\n",
       " 'crop': 29,\n",
       " 'spacex': 30,\n",
       " 'avengers': 31,\n",
       " 'mart': 32,\n",
       " 'errors': 33,\n",
       " 'comptia': 34,\n",
       " 'dread': 35,\n",
       " 'flir': 36,\n",
       " 'cissp': 37,\n",
       " 'siemens': 38,\n",
       " 'mistakes': 39,\n",
       " 'symsasa': 40,\n",
       " 'shepherd': 41,\n",
       " 'general': 42,\n",
       " 'warmed': 43,\n",
       " 'happyprinceday': 44,\n",
       " 'presbyterian': 45,\n",
       " 'williams': 46,\n",
       " 'masks': 47,\n",
       " 'allah': 48,\n",
       " 'herb': 49,\n",
       " 'troubleshooting': 50,\n",
       " 'resuming': 51,\n",
       " 'pull': 52,\n",
       " 'northern': 53,\n",
       " 'chased': 54,\n",
       " 'blackmail': 55,\n",
       " 'plea': 56,\n",
       " 'hybrid': 57,\n",
       " 'ginger': 58,\n",
       " 'assault': 59,\n",
       " 'ponds': 60,\n",
       " 'machine': 61,\n",
       " 'redirecting': 62,\n",
       " 'coherent': 63,\n",
       " 'elevated': 64,\n",
       " 'practiced': 65,\n",
       " 'ghost': 66,\n",
       " 'highperformance': 67,\n",
       " 'unintended': 68,\n",
       " 'api': 69,\n",
       " 'dubbed': 70,\n",
       " 'dhahran': 71,\n",
       " 'sacked': 72,\n",
       " 'looks': 73,\n",
       " 'jeddah': 74,\n",
       " 'opposes': 75,\n",
       " 'heard': 76,\n",
       " 'itonline': 77,\n",
       " 'program': 78,\n",
       " 'converters': 79,\n",
       " 'attendant': 80,\n",
       " 'honesty': 81,\n",
       " 'while': 82,\n",
       " 'clothes': 83,\n",
       " 'ee': 84,\n",
       " 'musicians': 85,\n",
       " 'permitted': 86,\n",
       " 'allegations': 87,\n",
       " 'pentesting': 88,\n",
       " 'satisfied': 89,\n",
       " 'crowd': 90,\n",
       " 'prince': 91,\n",
       " 'telecom': 92,\n",
       " 'tibco': 93,\n",
       " 'zanjradio': 94,\n",
       " 'starring': 95,\n",
       " 'exploding': 96,\n",
       " 'designed': 97,\n",
       " 'vtech': 98,\n",
       " 'xads': 99,\n",
       " 'bones': 100,\n",
       " 'bna': 101,\n",
       " 'affair': 102,\n",
       " 'tongue': 103,\n",
       " 'kiki': 104,\n",
       " 'nodes': 105,\n",
       " 'raw': 106,\n",
       " 'grooming': 107,\n",
       " 'spaghetti': 108,\n",
       " 'pitbull': 109,\n",
       " 'focuses': 110,\n",
       " 'nymaim': 111,\n",
       " 'detailing': 112,\n",
       " 'attributed': 113,\n",
       " 'memo': 114,\n",
       " 'spiritually': 115,\n",
       " 'tips': 116,\n",
       " 'lifetime': 117,\n",
       " 'xinhua': 118,\n",
       " 'skies': 119,\n",
       " 'separate': 120,\n",
       " 'lady': 121,\n",
       " 'listener': 122,\n",
       " 'hip': 123,\n",
       " 'russians': 124,\n",
       " 'clintons': 125,\n",
       " 'kinivo': 126,\n",
       " 'denialofservice': 127,\n",
       " 'statesman': 128,\n",
       " 'nbc': 129,\n",
       " 'accept': 130,\n",
       " 'lumpur': 131,\n",
       " 'provi': 132,\n",
       " 'axis': 133,\n",
       " 'rendering': 134,\n",
       " 'expects': 135,\n",
       " 'searles': 136,\n",
       " 'lasers': 137,\n",
       " 'facts': 138,\n",
       " 'croissant': 139,\n",
       " 'killing': 140,\n",
       " 'robbery': 141,\n",
       " 'mbps': 142,\n",
       " 'nr': 143,\n",
       " 'an': 144,\n",
       " 'gary': 145,\n",
       " 'kg': 146,\n",
       " 'cybercriminals': 147,\n",
       " 'greys': 148,\n",
       " 'explodes': 149,\n",
       " 'mornin': 150,\n",
       " 'onefs': 151,\n",
       " 'refund': 152,\n",
       " 'ob': 153,\n",
       " 'argued': 154,\n",
       " 'worshipping': 155,\n",
       " 'prominent': 156,\n",
       " 'zlabteam': 157,\n",
       " 'fairview': 158,\n",
       " 'uncertainty': 159,\n",
       " 'grandrapids': 160,\n",
       " 'merely': 161,\n",
       " 'russ': 162,\n",
       " 'roman': 163,\n",
       " 'reversed': 164,\n",
       " 'about': 165,\n",
       " 'conscious': 166,\n",
       " 'cornwall': 167,\n",
       " 'taekwondo': 168,\n",
       " 'jbscarva': 169,\n",
       " 'clamav': 170,\n",
       " 'waldorf': 171,\n",
       " 'blackouts': 172,\n",
       " 'costume': 173,\n",
       " 'younger': 174,\n",
       " 'processors': 175,\n",
       " 'varieties': 176,\n",
       " 'looked': 177,\n",
       " 'gorge': 178,\n",
       " 'peterborough': 179,\n",
       " 'mdjobs': 180,\n",
       " 'kapustkiy': 181,\n",
       " 'forcing': 182,\n",
       " 'wheaton': 183,\n",
       " 'thee': 184,\n",
       " 'change': 185,\n",
       " 'mic': 186,\n",
       " 'programmable': 187,\n",
       " 'kaduna': 188,\n",
       " 'gpg': 189,\n",
       " 'sonya': 190,\n",
       " 'their': 191,\n",
       " 'airlines': 192,\n",
       " 'macrumors': 193,\n",
       " 'alr': 194,\n",
       " 'tres': 195,\n",
       " 'naturalhair': 196,\n",
       " 'quinnnorton': 197,\n",
       " 'wpa': 198,\n",
       " 'wellness': 199,\n",
       " 'turbo': 200,\n",
       " 'remx': 201,\n",
       " 'destiny': 202,\n",
       " 'citizenship': 203,\n",
       " 'insurancejournal': 204,\n",
       " 'financialexpress': 205,\n",
       " 'accommodate': 206,\n",
       " 'consistent': 207,\n",
       " 'wsykgh': 208,\n",
       " 'newsisbuzz': 209,\n",
       " 'cyberexaminer': 210,\n",
       " 'kennedy': 211,\n",
       " 'tile': 212,\n",
       " 'magnetic': 213,\n",
       " 'intended': 214,\n",
       " 'panthers': 215,\n",
       " 'chevrolet': 216,\n",
       " 'elizabeth': 217,\n",
       " 'upon': 218,\n",
       " 'mooresville': 219,\n",
       " 'nigga': 220,\n",
       " 'nwub': 221,\n",
       " 'luton': 222,\n",
       " 'climbed': 223,\n",
       " 'blanc': 224,\n",
       " 'cy': 225,\n",
       " 'damage': 226,\n",
       " 'upstate': 227,\n",
       " 'zline': 228,\n",
       " 'nope': 229,\n",
       " 'tomb': 230,\n",
       " 'ufo': 231,\n",
       " 'analyzed': 232,\n",
       " 'posing': 233,\n",
       " 'rampant': 234,\n",
       " 'borisjohnson': 235,\n",
       " 'kicklahomamon': 236,\n",
       " 'ancestors': 237,\n",
       " 'ian': 238,\n",
       " 'disturb': 239,\n",
       " 'use': 240,\n",
       " 'regardless': 241,\n",
       " 'wabash': 242,\n",
       " 'sdn': 243,\n",
       " 'faith': 244,\n",
       " 'wendy': 245,\n",
       " 'journalists': 246,\n",
       " 'bernardino': 247,\n",
       " 'catching': 248,\n",
       " 'aerialgeneral': 249,\n",
       " 'rumors': 250,\n",
       " 'sesh': 251,\n",
       " 'yilp': 252,\n",
       " 'sissy': 253,\n",
       " 'cp': 254,\n",
       " 'pai': 255,\n",
       " 'intellectual': 256,\n",
       " 'politico': 257,\n",
       " 'trials': 258,\n",
       " 'okc': 259,\n",
       " 'autocad': 260,\n",
       " 'moscow': 261,\n",
       " 'domain': 262,\n",
       " 'poplar': 263,\n",
       " 'adversarial': 264,\n",
       " 'usgs': 265,\n",
       " 'socialnetwork': 266,\n",
       " 'lcbo': 267,\n",
       " 'doctalos': 268,\n",
       " 'microsaver': 269,\n",
       " 'owns': 270,\n",
       " 'agelastic': 271,\n",
       " 'geeky': 272,\n",
       " 'adb': 273,\n",
       " 'duncan': 274,\n",
       " 'dental': 275,\n",
       " 'demographic': 276,\n",
       " 'animal': 277,\n",
       " 'canoe': 278,\n",
       " 'zoom': 279,\n",
       " 'cakes': 280,\n",
       " 'iasielloe': 281,\n",
       " 'commissions': 282,\n",
       " 'guard': 283,\n",
       " 'javaopenjdk': 284,\n",
       " 'denverchalkartfestival': 285,\n",
       " 'btw': 286,\n",
       " 'afraid': 287,\n",
       " 'above': 288,\n",
       " 'teenage': 289,\n",
       " 'smashing': 290,\n",
       " 'worked': 291,\n",
       " 'going': 292,\n",
       " 'boozallen': 293,\n",
       " 'utility': 294,\n",
       " 'homie': 295,\n",
       " 'murphy': 296,\n",
       " 'lower': 297,\n",
       " 'hijacked': 298,\n",
       " 'interstate': 299,\n",
       " 'mngr': 300,\n",
       " 'headaches': 301,\n",
       " 'depaul': 302,\n",
       " 'xiggti': 303,\n",
       " 'ncweaver': 304,\n",
       " 'avoiding': 305,\n",
       " 'illustrator': 306,\n",
       " 'hanover': 307,\n",
       " 'citylife': 308,\n",
       " 'avatar': 309,\n",
       " 'amazing': 310,\n",
       " 'ballybong': 311,\n",
       " 'recruitment': 312,\n",
       " 'unauthorised': 313,\n",
       " 'aos': 314,\n",
       " 'cart': 315,\n",
       " 'shortly': 316,\n",
       " 'stratfor': 317,\n",
       " 'tracy': 318,\n",
       " 'publish': 319,\n",
       " 'mundo': 320,\n",
       " 'friend': 321,\n",
       " 'crawl': 322,\n",
       " 'erratarob': 323,\n",
       " 'beermenus': 324,\n",
       " 'vault': 325,\n",
       " 'century': 326,\n",
       " 'generic': 327,\n",
       " 'saugatuck': 328,\n",
       " 'optimization': 329,\n",
       " 'delivered': 330,\n",
       " 'hazmat': 331,\n",
       " 'evaluation': 332,\n",
       " 'elpaso': 333,\n",
       " 'mathematics': 334,\n",
       " 'intentional': 335,\n",
       " 'horses': 336,\n",
       " 'tibetan': 337,\n",
       " 'cards': 338,\n",
       " 'uae': 339,\n",
       " 'munin': 340,\n",
       " 'newshtml': 341,\n",
       " 'groceries': 342,\n",
       " 'draft': 343,\n",
       " 'savior': 344,\n",
       " 'vbs': 345,\n",
       " 'spikes': 346,\n",
       " 'coins': 347,\n",
       " 'november': 348,\n",
       " 'bjs': 349,\n",
       " 'behindthescenes': 350,\n",
       " 'assignment': 351,\n",
       " 'rescued': 352,\n",
       " 'terrible': 353,\n",
       " 'selfdriving': 354,\n",
       " 'riser': 355,\n",
       " 'installing': 356,\n",
       " 'online': 357,\n",
       " 'vest': 358,\n",
       " 'donandrewbailey': 359,\n",
       " 'lil': 360,\n",
       " 'moose': 361,\n",
       " 'job': 362,\n",
       " 'livonia': 363,\n",
       " 'nokia': 364,\n",
       " 'lifestyle': 365,\n",
       " 'further': 366,\n",
       " 'anywhere': 367,\n",
       " 'tango': 368,\n",
       " 'anatomy': 369,\n",
       " 'chahe': 370,\n",
       " 'rules': 371,\n",
       " 'scottsbluff': 372,\n",
       " 'dm': 373,\n",
       " 'warming': 374,\n",
       " 'zxperia': 375,\n",
       " 'travelers': 376,\n",
       " 'databreaches': 377,\n",
       " 'inker': 378,\n",
       " 'happiest': 379,\n",
       " 'digest': 380,\n",
       " 'frozen': 381,\n",
       " 'foxit': 382,\n",
       " 'del': 383,\n",
       " 'snopes': 384,\n",
       " 'paperback': 385,\n",
       " 'wind': 386,\n",
       " 'urbanaftermath': 387,\n",
       " 'zach': 388,\n",
       " 'weir': 389,\n",
       " 'sunflower': 390,\n",
       " 'logmein': 391,\n",
       " 'penny': 392,\n",
       " 'controlled': 393,\n",
       " 'reservoir': 394,\n",
       " 'ponder': 395,\n",
       " 'talk': 396,\n",
       " 'achievement': 397,\n",
       " 'zenchangeangel': 398,\n",
       " 'pe': 399,\n",
       " 'thepelicanpub': 400,\n",
       " 'unleashed': 401,\n",
       " 'letters': 402,\n",
       " 'salecol': 403,\n",
       " 'comments': 404,\n",
       " 'control': 405,\n",
       " 'peanut': 406,\n",
       " 'indicts': 407,\n",
       " 'iecc': 408,\n",
       " 'urging': 409,\n",
       " 'guilt': 410,\n",
       " 'timetable': 411,\n",
       " 'highland': 412,\n",
       " 'virginia': 413,\n",
       " 'shields': 414,\n",
       " 'rhubarb': 415,\n",
       " 'magnolia': 416,\n",
       " 'imitation': 417,\n",
       " 'vast': 418,\n",
       " 'zix': 419,\n",
       " 'later': 420,\n",
       " 'bluetooth': 421,\n",
       " 'rainfall': 422,\n",
       " 'optimize': 423,\n",
       " 'lend': 424,\n",
       " 'medias': 425,\n",
       " 'inf': 426,\n",
       " 'nontechnical': 427,\n",
       " 'nutrition': 428,\n",
       " 'carried': 429,\n",
       " 'tompkins': 430,\n",
       " 'higheredjobs': 431,\n",
       " 'bishops': 432,\n",
       " 'testgeocode': 433,\n",
       " 'costs': 434,\n",
       " 'individuals': 435,\n",
       " 'securi': 436,\n",
       " 'slots': 437,\n",
       " 'usbased': 438,\n",
       " 'washable': 439,\n",
       " 'dock': 440,\n",
       " 'sayreville': 441,\n",
       " 'suggest': 442,\n",
       " 'bossert': 443,\n",
       " 'libreoffice': 444,\n",
       " 'atlantaprotest': 445,\n",
       " 'writer': 446,\n",
       " 'ruled': 447,\n",
       " 'substring': 448,\n",
       " 'reacts': 449,\n",
       " 'delish': 450,\n",
       " 'iead': 451,\n",
       " 'atelier': 452,\n",
       " 'algo': 453,\n",
       " 'roadway': 454,\n",
       " 'helpful': 455,\n",
       " 'atomsoffice': 456,\n",
       " 'nonprofit': 457,\n",
       " 'swimwear': 458,\n",
       " 'santarosa': 459,\n",
       " 'cheeky': 460,\n",
       " 'asian': 461,\n",
       " 'nurselife': 462,\n",
       " 'primary': 463,\n",
       " 'accidental': 464,\n",
       " 'chteau': 465,\n",
       " 'knights': 466,\n",
       " 'vaughan': 467,\n",
       " 'cybersecuritys': 468,\n",
       " 'splitcosts': 469,\n",
       " 'caregiver': 470,\n",
       " 'ack': 471,\n",
       " 'manifesto': 472,\n",
       " 'und': 473,\n",
       " 'rings': 474,\n",
       " 'tumblers': 475,\n",
       " 'hay': 476,\n",
       " 'progressive': 477,\n",
       " 'autoblog': 478,\n",
       " 'innovative': 479,\n",
       " 'riverdale': 480,\n",
       " 'pinot': 481,\n",
       " 'caseyjohnellis': 482,\n",
       " 'openjpeg': 483,\n",
       " 'warren': 484,\n",
       " 'absurd': 485,\n",
       " 'departmentn': 486,\n",
       " 'organizers': 487,\n",
       " 'quoteoftheday': 488,\n",
       " 'simplifies': 489,\n",
       " 'iphonex': 490,\n",
       " 'pornstar': 491,\n",
       " 'hdr': 492,\n",
       " 'weapons': 493,\n",
       " 'skimmer': 494,\n",
       " 'golfing': 495,\n",
       " 'sheep': 496,\n",
       " 'ampamp': 497,\n",
       " 'ads': 498,\n",
       " 'windows': 499,\n",
       " 'tzu': 500,\n",
       " 'ifff': 501,\n",
       " 'cob': 502,\n",
       " 'poppin': 503,\n",
       " 'managers': 504,\n",
       " 'seattle': 505,\n",
       " 'motors': 506,\n",
       " 'guards': 507,\n",
       " 'hey': 508,\n",
       " 'smuggling': 509,\n",
       " 'wes': 510,\n",
       " 'lone': 511,\n",
       " 'tybee': 512,\n",
       " 'fruitfly': 513,\n",
       " 'dnssec': 514,\n",
       " 'grandmother': 515,\n",
       " 'wet': 516,\n",
       " 'xr': 517,\n",
       " 'itsoftware': 518,\n",
       " 'ether': 519,\n",
       " 'evening': 520,\n",
       " 'toolsby': 521,\n",
       " 'scrub': 522,\n",
       " 'muffins': 523,\n",
       " 'neurologist': 524,\n",
       " 'sharemusic': 525,\n",
       " 'montebello': 526,\n",
       " 'sf': 527,\n",
       " 'fifteen': 528,\n",
       " 'lumia': 529,\n",
       " 'structures': 530,\n",
       " 'sensational': 531,\n",
       " 'doctors': 532,\n",
       " 'becomes': 533,\n",
       " 'shaped': 534,\n",
       " 'begun': 535,\n",
       " 'honey': 536,\n",
       " 'hartshorne': 537,\n",
       " 'default': 538,\n",
       " 'amelia': 539,\n",
       " 'warns': 540,\n",
       " 'certification': 541,\n",
       " 'seo': 542,\n",
       " 'bento': 543,\n",
       " 'fas': 544,\n",
       " 'dakota': 545,\n",
       " 'sold': 546,\n",
       " 'maintain': 547,\n",
       " 'pedestrian': 548,\n",
       " 'airport': 549,\n",
       " 'respectful': 550,\n",
       " 'musicphill': 551,\n",
       " 'pandan': 552,\n",
       " 'humanrights': 553,\n",
       " 'piercing': 554,\n",
       " 'woulda': 555,\n",
       " 'streetlife': 556,\n",
       " 'amber': 557,\n",
       " 'bjj': 558,\n",
       " 'zero': 559,\n",
       " 'authentica': 560,\n",
       " 'spoof': 561,\n",
       " 'crane': 562,\n",
       " 'reliance': 563,\n",
       " 'harmful': 564,\n",
       " 'exquisite': 565,\n",
       " 'hartford': 566,\n",
       " 'nationwide': 567,\n",
       " 'frimer': 568,\n",
       " 'steam': 569,\n",
       " 'dot': 570,\n",
       " 'potato': 571,\n",
       " 'exactly': 572,\n",
       " 'humanity': 573,\n",
       " 'rescuedogsofinstagram': 574,\n",
       " 'squid': 575,\n",
       " 'ourmine': 576,\n",
       " 'yoroi': 577,\n",
       " 'cornell': 578,\n",
       " 'referendum': 579,\n",
       " 'drweb': 580,\n",
       " 'indigenous': 581,\n",
       " 'sierra': 582,\n",
       " 'param': 583,\n",
       " 'sundayservice': 584,\n",
       " 'fiji': 585,\n",
       " 'libtasn': 586,\n",
       " 'recruit': 587,\n",
       " 'mnat': 588,\n",
       " 'weapon': 589,\n",
       " 'brunswick': 590,\n",
       " 'optical': 591,\n",
       " 'positives': 592,\n",
       " 'dictionary': 593,\n",
       " 'dime': 594,\n",
       " 'adelaide': 595,\n",
       " 'gown': 596,\n",
       " 'gent': 597,\n",
       " 'edison': 598,\n",
       " 'bucks': 599,\n",
       " 'tiger': 600,\n",
       " 'yomiuri': 601,\n",
       " 'spend': 602,\n",
       " 'sciences': 603,\n",
       " 'posted': 604,\n",
       " 'deux': 605,\n",
       " 'chimera': 606,\n",
       " 'dining': 607,\n",
       " 'viper': 608,\n",
       " 'babes': 609,\n",
       " 'quali': 610,\n",
       " 'ransomwares': 611,\n",
       " 'jericho': 612,\n",
       " 'awful': 613,\n",
       " 'perpetuating': 614,\n",
       " 'hasherezade': 615,\n",
       " 'defensive': 616,\n",
       " 'darkreadingcomthreatintelli': 617,\n",
       " 'expression': 618,\n",
       " 'ecommerce': 619,\n",
       " 'staff': 620,\n",
       " 'insightful': 621,\n",
       " 'kentucky': 622,\n",
       " 'marcos': 623,\n",
       " 'salon': 624,\n",
       " 'dammit': 625,\n",
       " 'azure': 626,\n",
       " 'factory': 627,\n",
       " 'managing': 628,\n",
       " 'nh': 629,\n",
       " 'bye': 630,\n",
       " 'ashford': 631,\n",
       " 'recognize': 632,\n",
       " 'agents': 633,\n",
       " 'netsecuorgifb': 634,\n",
       " 'hughes': 635,\n",
       " 'fraser': 636,\n",
       " 'irish': 637,\n",
       " 'day': 638,\n",
       " 'usher': 639,\n",
       " 'adverts': 640,\n",
       " 'rm': 641,\n",
       " 'holistic': 642,\n",
       " 'xpost': 643,\n",
       " 'cooks': 644,\n",
       " 'bedding': 645,\n",
       " 'bombardier': 646,\n",
       " 'channel': 647,\n",
       " 'granetman': 648,\n",
       " 'investigates': 649,\n",
       " 'truecrypt': 650,\n",
       " 'waverly': 651,\n",
       " 'tasty': 652,\n",
       " 'dreamer': 653,\n",
       " 'orr': 654,\n",
       " 'cinema': 655,\n",
       " 'versace': 656,\n",
       " 'infoworld': 657,\n",
       " 'melting': 658,\n",
       " 'abuses': 659,\n",
       " 'damascus': 660,\n",
       " 'tracker': 661,\n",
       " 'recycle': 662,\n",
       " 'hence': 663,\n",
       " 'ils': 664,\n",
       " 'er': 665,\n",
       " 'lisa': 666,\n",
       " 'mxlab': 667,\n",
       " 'greer': 668,\n",
       " 'cr': 669,\n",
       " 'investigate': 670,\n",
       " 'fathersdaygifts': 671,\n",
       " 'theyve': 672,\n",
       " 'toys': 673,\n",
       " 'clickjacking': 674,\n",
       " 'servi': 675,\n",
       " 'work': 676,\n",
       " 'applesa': 677,\n",
       " 'wsdailysummary': 678,\n",
       " 'tin': 679,\n",
       " 'unprepared': 680,\n",
       " 'arista': 681,\n",
       " 'show': 682,\n",
       " 'mature': 683,\n",
       " 'banna': 684,\n",
       " 'descriptions': 685,\n",
       " 'dmc': 686,\n",
       " 'colfax': 687,\n",
       " 'wpm': 688,\n",
       " 'requests': 689,\n",
       " 'chr': 690,\n",
       " 'frederick': 691,\n",
       " 'wage': 692,\n",
       " 'cleared': 693,\n",
       " 'diary': 694,\n",
       " 'semibogan': 695,\n",
       " 'lockdownlife': 696,\n",
       " 'swimming': 697,\n",
       " 'trees': 698,\n",
       " 'messaging': 699,\n",
       " 'isolation': 700,\n",
       " 'reactions': 701,\n",
       " 'political': 702,\n",
       " 'education': 703,\n",
       " 'nclc': 704,\n",
       " 'lee': 705,\n",
       " 'bayview': 706,\n",
       " 'tyson': 707,\n",
       " 'burnt': 708,\n",
       " 'pmremote': 709,\n",
       " 'speednews': 710,\n",
       " 'carryout': 711,\n",
       " 'celebrities': 712,\n",
       " 'relieve': 713,\n",
       " 'circuit': 714,\n",
       " 'policy': 715,\n",
       " 'ridgeway': 716,\n",
       " 'macos': 717,\n",
       " 'somebody': 718,\n",
       " 'chromecast': 719,\n",
       " 'modifying': 720,\n",
       " 'trains': 721,\n",
       " 'slap': 722,\n",
       " 'manually': 723,\n",
       " 'via': 724,\n",
       " 'magnitude': 725,\n",
       " 'ind': 726,\n",
       " 'mssj': 727,\n",
       " 'izzy': 728,\n",
       " 'households': 729,\n",
       " 'sigh': 730,\n",
       " 'productmgmt': 731,\n",
       " 'learning': 732,\n",
       " 'buddies': 733,\n",
       " 'typ': 734,\n",
       " 'poughkeepsie': 735,\n",
       " 'apologizes': 736,\n",
       " 'rosa': 737,\n",
       " 'retreat': 738,\n",
       " 'gamers': 739,\n",
       " 'favor': 740,\n",
       " 'passcode': 741,\n",
       " 'informationage': 742,\n",
       " 'sawmillriverparkway': 743,\n",
       " 'newsize': 744,\n",
       " 'seasons': 745,\n",
       " 'doorstep': 746,\n",
       " 'liberdade': 747,\n",
       " 'martins': 748,\n",
       " 'contributions': 749,\n",
       " 'corn': 750,\n",
       " 'onedrive': 751,\n",
       " 'downloading': 752,\n",
       " 'symantec': 753,\n",
       " 'suspension': 754,\n",
       " 'dragonfly': 755,\n",
       " 'clyde': 756,\n",
       " 'kenosha': 757,\n",
       " 'targe': 758,\n",
       " 'gitlab': 759,\n",
       " 'xcodeghost': 760,\n",
       " 'nbn': 761,\n",
       " 'illustrations': 762,\n",
       " 'stanbul': 763,\n",
       " 'indicators': 764,\n",
       " 'wore': 765,\n",
       " 'holes': 766,\n",
       " 'mytropeziennewave': 767,\n",
       " 'philosophy': 768,\n",
       " 'vhcfy': 769,\n",
       " 'snaps': 770,\n",
       " 'solve': 771,\n",
       " 'rent': 772,\n",
       " 'tenablecompluginsnessus': 773,\n",
       " 'themselves': 774,\n",
       " 'nedostatak': 775,\n",
       " 'earth': 776,\n",
       " 'milo': 777,\n",
       " 'ezra': 778,\n",
       " 'harvard': 779,\n",
       " 'fresenius': 780,\n",
       " 'physicianjobs': 781,\n",
       " 'firewalls': 782,\n",
       " 'banners': 783,\n",
       " 'wpad': 784,\n",
       " 'vt': 785,\n",
       " 'sidechannel': 786,\n",
       " 'ubergizmo': 787,\n",
       " 'merritt': 788,\n",
       " 'cleveland': 789,\n",
       " 'teaching': 790,\n",
       " 'moves': 791,\n",
       " 'hadnt': 792,\n",
       " 'goslings': 793,\n",
       " 'lago': 794,\n",
       " 'everywhere': 795,\n",
       " 'collections': 796,\n",
       " 'chicitymycity': 797,\n",
       " 'soulful': 798,\n",
       " 'ebike': 799,\n",
       " 'matt': 800,\n",
       " 'devastating': 801,\n",
       " 'spectacular': 802,\n",
       " 'residents': 803,\n",
       " 'coq': 804,\n",
       " 'smoky': 805,\n",
       " 'warcraft': 806,\n",
       " 'arrives': 807,\n",
       " 'unity': 808,\n",
       " 'wir': 809,\n",
       " 'kreme': 810,\n",
       " 'tokyo': 811,\n",
       " 'advocate': 812,\n",
       " 'tether': 813,\n",
       " 'transactions': 814,\n",
       " 'youtuber': 815,\n",
       " 'trailer': 816,\n",
       " 'mandarin': 817,\n",
       " 'openview': 818,\n",
       " 'breaks': 819,\n",
       " 'rmnu': 820,\n",
       " 'aadhaar': 821,\n",
       " 'preload': 822,\n",
       " 'malvertising': 823,\n",
       " 'layer': 824,\n",
       " 'ph': 825,\n",
       " 'anker': 826,\n",
       " 'ikotun': 827,\n",
       " 'triblive': 828,\n",
       " 'cloudera': 829,\n",
       " 'tidal': 830,\n",
       " 'grim': 831,\n",
       " 'healthcarejobs': 832,\n",
       " 'mondelz': 833,\n",
       " 'myfootballclub': 834,\n",
       " 'cybercriminal': 835,\n",
       " 'galvin': 836,\n",
       " 'far': 837,\n",
       " 'melbourne': 838,\n",
       " 'punta': 839,\n",
       " 'since': 840,\n",
       " 'figures': 841,\n",
       " 'infested': 842,\n",
       " 'england': 843,\n",
       " 'outlets': 844,\n",
       " 'celebrated': 845,\n",
       " 'chez': 846,\n",
       " 'palo': 847,\n",
       " 'salaam': 848,\n",
       " 'iceland': 849,\n",
       " 'freedom': 850,\n",
       " 'deutsche': 851,\n",
       " 'bend': 852,\n",
       " 'wears': 853,\n",
       " 'saint': 854,\n",
       " 'postgres': 855,\n",
       " 'fu': 856,\n",
       " 'burbanksquawk': 857,\n",
       " 'police': 858,\n",
       " 'queer': 859,\n",
       " 'antenna': 860,\n",
       " 'rvcb': 861,\n",
       " 'lucky': 862,\n",
       " 'certifications': 863,\n",
       " 'weblog': 864,\n",
       " 'darkreading': 865,\n",
       " 'brings': 866,\n",
       " 'heavily': 867,\n",
       " 'alaska': 868,\n",
       " 'upside': 869,\n",
       " 'uttar': 870,\n",
       " 'wysiwyg': 871,\n",
       " 'forgotten': 872,\n",
       " 'gran': 873,\n",
       " 'outage': 874,\n",
       " 'tags': 875,\n",
       " 'kitploit': 876,\n",
       " 'backed': 877,\n",
       " 'finder': 878,\n",
       " 'zealand': 879,\n",
       " 'guidance': 880,\n",
       " 'securitybreach': 881,\n",
       " 'henderson': 882,\n",
       " 'urbangraffitisbcn': 883,\n",
       " 'babies': 884,\n",
       " 'lovelocsacademy': 885,\n",
       " 'fortigate': 886,\n",
       " 'seizure': 887,\n",
       " 'kim': 888,\n",
       " 'nintendos': 889,\n",
       " 'threaten': 890,\n",
       " 'nailed': 891,\n",
       " 'institute': 892,\n",
       " 'knives': 893,\n",
       " 'sich': 894,\n",
       " 'unveil': 895,\n",
       " 'vulnerable': 896,\n",
       " 'ssrf': 897,\n",
       " 'december': 898,\n",
       " 'chop': 899,\n",
       " 'gmz': 900,\n",
       " 'threads': 901,\n",
       " 'roseville': 902,\n",
       " 'upmc': 903,\n",
       " 'massachusetts': 904,\n",
       " 'voip': 905,\n",
       " 'quiz': 906,\n",
       " 'appreciation': 907,\n",
       " 'apparently': 908,\n",
       " 'hempstead': 909,\n",
       " 'iphoneiphone': 910,\n",
       " 'newsroom': 911,\n",
       " 'ieea': 912,\n",
       " 'erp': 913,\n",
       " 'avoid': 914,\n",
       " 'pentester': 915,\n",
       " 'divided': 916,\n",
       " 'piano': 917,\n",
       " 'mspne': 918,\n",
       " 'north': 919,\n",
       " 'cellular': 920,\n",
       " 'qsputl': 921,\n",
       " 'oxygen': 922,\n",
       " 'appointments': 923,\n",
       " 'welcoming': 924,\n",
       " 'seminoe': 925,\n",
       " 'productive': 926,\n",
       " 'marion': 927,\n",
       " 'revisited': 928,\n",
       " 'mabhsb': 929,\n",
       " 'qakbot': 930,\n",
       " 'breeze': 931,\n",
       " 'vietnamese': 932,\n",
       " 'translate': 933,\n",
       " 'scarlett': 934,\n",
       " 'outrage': 935,\n",
       " 'albanydailystar': 936,\n",
       " 'faversham': 937,\n",
       " 'hightech': 938,\n",
       " 'electronic': 939,\n",
       " 'bureau': 940,\n",
       " 'corrections': 941,\n",
       " 'title': 942,\n",
       " 'underneath': 943,\n",
       " 'escaped': 944,\n",
       " 'lawsuits': 945,\n",
       " 'greedy': 946,\n",
       " 'var': 947,\n",
       " 'atop': 948,\n",
       " 'opencart': 949,\n",
       " 'detroit': 950,\n",
       " 'switch': 951,\n",
       " 'however': 952,\n",
       " 'unrelated': 953,\n",
       " 'cognition': 954,\n",
       " 'spotted': 955,\n",
       " 'momentum': 956,\n",
       " 'flux': 957,\n",
       " 'diplomats': 958,\n",
       " 'examines': 959,\n",
       " 'valid': 960,\n",
       " 'dpi': 961,\n",
       " 'kadunabuyandsell': 962,\n",
       " 'o': 963,\n",
       " 'shot': 964,\n",
       " 'parties': 965,\n",
       " 'faq': 966,\n",
       " 'cen': 967,\n",
       " 'increasingly': 968,\n",
       " 'decided': 969,\n",
       " 'galaxies': 970,\n",
       " 'zncg': 971,\n",
       " 'bullguard': 972,\n",
       " 'veg': 973,\n",
       " 'divine': 974,\n",
       " 'studying': 975,\n",
       " 'versions': 976,\n",
       " 'squash': 977,\n",
       " 'beforeandafter': 978,\n",
       " 'eight': 979,\n",
       " 'remaining': 980,\n",
       " 'florist': 981,\n",
       " 'cyberark': 982,\n",
       " 'treehousebrewco': 983,\n",
       " 'teas': 984,\n",
       " 'ardenarcade': 985,\n",
       " 'orscheln': 986,\n",
       " 'twoway': 987,\n",
       " 'stark': 988,\n",
       " 'pentecost': 989,\n",
       " 'utilize': 990,\n",
       " 'return': 991,\n",
       " 'contract': 992,\n",
       " 'cma': 993,\n",
       " 'covington': 994,\n",
       " 'inlet': 995,\n",
       " 'unified': 996,\n",
       " 'ytnef': 997,\n",
       " 'jennieo': 998,\n",
       " 'iea': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index.staticDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23526c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25566"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc.staticList) # 20000, 31732  ##31701, 32512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb1926e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25566"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index.staticDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2726df9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125630"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vocabulary.staticList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59df5231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'chaescutie',\n",
       " 'wtslfbtwice',\n",
       " 'assorted',\n",
       " 'photocards',\n",
       " 'mopgcash',\n",
       " 'set',\n",
       " 'modshopee',\n",
       " 'coreplydm',\n",
       " 'claim']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.staticList[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30bb3127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'feature', 'label_index']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135aed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = preprocessed.select(\n",
    "    preprocessed[\"text\"],\n",
    "    preprocessed[\"label_index\"],\n",
    "    list_to_vector_udf(preprocessed[\"feature\"]).alias(\"feature\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7876a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label_index', 'feature']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_vectors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ae5921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from mittens import GloVe, Mittens\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "glove_path = \"/root/spark/glove.6B.100d.txt\"\n",
    "pre_glove = glove2dict(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5b7615c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "866ea6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÎëêÎ≤àÏß∏ time window Ïù¥ÏÉÅÎ∂ÄÌÑ∞ Í∞Å time windowÏóê ÎßûÏ∂îÏñ¥ÏÑú repo_glove_N\n",
    "# load the pre-trained Glove weight\n",
    "f = open(\"repo_glove_1.pkl\",\"rb\")\n",
    "pre_glove = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7df5552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400621"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b416e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "732787"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary.staticList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93c0b417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['threatmeter',\n",
       " 'hacked',\n",
       " 'emails',\n",
       " 'san',\n",
       " 'francisco',\n",
       " 'muni',\n",
       " 'rail',\n",
       " 'system',\n",
       " 'hacker',\n",
       " 'reveals',\n",
       " 'clues',\n",
       " 'identity',\n",
       " 'tactics',\n",
       " 'kr',\n",
       " 'fin',\n",
       " 'first',\n",
       " 'android',\n",
       " 'malware',\n",
       " 'targeting',\n",
       " 'pcs',\n",
       " 'uncovered',\n",
       " 'fbxtynm',\n",
       " 'adobe',\n",
       " 'fixes',\n",
       " 'six',\n",
       " 'code',\n",
       " 'execution',\n",
       " 'bugs',\n",
       " 'flash',\n",
       " 'mokgxr',\n",
       " 'scienceporn',\n",
       " 'vacuum',\n",
       " 'guess',\n",
       " 'riskware',\n",
       " 'hmoyfzb',\n",
       " 'fbjaz',\n",
       " 'rt',\n",
       " 'exploithunt',\n",
       " 'wzemae',\n",
       " 'data',\n",
       " 'protection',\n",
       " 'fusion',\n",
       " 'itsec',\n",
       " 'databreach',\n",
       " 'news',\n",
       " 'wzenei',\n",
       " 'webroot',\n",
       " 'spamvertised',\n",
       " 'lost',\n",
       " 'message',\n",
       " 'facebook',\n",
       " 'campaign',\n",
       " 'leads',\n",
       " 'pharmaceutical',\n",
       " 'scams',\n",
       " 'xunah',\n",
       " 'cybersecurity',\n",
       " 'hacker',\n",
       " 'privacy',\n",
       " 'big',\n",
       " 'data',\n",
       " 'rmge',\n",
       " 'postsnag',\n",
       " 'gingcredsfromlockedmachines',\n",
       " 'ddos',\n",
       " 'mvrzcc',\n",
       " 'top',\n",
       " 'actions',\n",
       " 'runtime',\n",
       " 'application',\n",
       " 'protection',\n",
       " 'fbodttge',\n",
       " 'watch',\n",
       " 'aqvqvy',\n",
       " 'threatmeter',\n",
       " 'facebook',\n",
       " 'algorithm',\n",
       " 'watching',\n",
       " 'loetua',\n",
       " 'product',\n",
       " 'model',\n",
       " 'v',\n",
       " 'product',\n",
       " 'name',\n",
       " 'forensic',\n",
       " 'ultradock',\n",
       " 'v',\n",
       " 'product',\n",
       " 'ty',\n",
       " 'rltxbn',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'rt',\n",
       " 'securitytube',\n",
       " 'video',\n",
       " 'ettercap',\n",
       " 'beef',\n",
       " 'mashup',\n",
       " 'never',\n",
       " 'know',\n",
       " 'moment',\n",
       " 'sincere',\n",
       " 'words',\n",
       " 'impact',\n",
       " 'life',\n",
       " 'thezigziglar',\n",
       " 'success',\n",
       " 'leadership',\n",
       " 'cvechrome',\n",
       " 'google',\n",
       " 'chrome',\n",
       " 'allows',\n",
       " 'remote',\n",
       " 'atta',\n",
       " 'cve',\n",
       " 'schneider',\n",
       " 'electric',\n",
       " 'umotion',\n",
       " 'builder',\n",
       " 'software',\n",
       " 'versions',\n",
       " 'prior',\n",
       " 'v',\n",
       " 'cross',\n",
       " 'site',\n",
       " 'scripting',\n",
       " 'xss',\n",
       " 'vulnerability',\n",
       " 'exists',\n",
       " 'allow',\n",
       " 'injection',\n",
       " 'malicious',\n",
       " 'scripts',\n",
       " 'tsmrng',\n",
       " 'coresecuritycom',\n",
       " 'mikko',\n",
       " 'hyppnen',\n",
       " 'warns',\n",
       " 'isis',\n",
       " 'credible',\n",
       " 'offensive',\n",
       " 'cyber',\n",
       " 'capability',\n",
       " 'ebfjwkb',\n",
       " 'securityaffairs',\n",
       " 'isis',\n",
       " 'hacking',\n",
       " 'security',\n",
       " 'bm',\n",
       " 'none',\n",
       " 'google',\n",
       " 'accounts',\n",
       " 'connected',\n",
       " 'theyre',\n",
       " 'kept',\n",
       " 'deprecate',\n",
       " 'opsec',\n",
       " 'reasons',\n",
       " 'even',\n",
       " 'dedicate',\n",
       " 'hardware',\n",
       " 'ai',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'help',\n",
       " 'defend',\n",
       " 'enterprise',\n",
       " 'fromcyberattacks',\n",
       " 'itsecuritynewsinfohowaiandmac',\n",
       " 'hinelearningcanhelpyoudefendtheenterprisefromcyberattacks',\n",
       " 'us',\n",
       " 'military',\n",
       " 'veterans',\n",
       " 'join',\n",
       " 'standing',\n",
       " 'rock',\n",
       " 'protests',\n",
       " 'n',\n",
       " 'dakota',\n",
       " 'pnusd',\n",
       " 'malware',\n",
       " 'based',\n",
       " 'attack',\n",
       " 'hit',\n",
       " 'japanese',\n",
       " 'monju',\n",
       " 'nuclear',\n",
       " 'power',\n",
       " 'plant',\n",
       " 'dgzdrv',\n",
       " 'windows',\n",
       " 'egghunter',\n",
       " 'wow',\n",
       " 'corelanbeindexphp',\n",
       " 'windowsegghunter',\n",
       " 'mandriva',\n",
       " 'linux',\n",
       " 'security',\n",
       " 'advisory',\n",
       " 'fbmfya',\n",
       " 'cvewindowsserver',\n",
       " 'windows',\n",
       " 'windowsserver',\n",
       " 'windowsserver',\n",
       " 'windows',\n",
       " 'opentype',\n",
       " 'digicert',\n",
       " 'works',\n",
       " 'partners',\n",
       " 'move',\n",
       " 'past',\n",
       " 'google',\n",
       " 'distrust',\n",
       " 'symantec',\n",
       " 'tls',\n",
       " 'certificates',\n",
       " 'rhelpnetsecu',\n",
       " 'rityagttqv',\n",
       " 'helpnetsecurity',\n",
       " 'security',\n",
       " 'analyst',\n",
       " 'series',\n",
       " 'eccouncil',\n",
       " 'press',\n",
       " 'comprised',\n",
       " 'o',\n",
       " 'pptbc',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'hacking',\n",
       " 'something',\n",
       " 'many',\n",
       " 'people',\n",
       " 'fall',\n",
       " 'victim',\n",
       " 'point',\n",
       " 'lllc',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'mid',\n",
       " 'level',\n",
       " 'surfacing',\n",
       " 'engineer',\n",
       " 'design',\n",
       " 'engineer',\n",
       " 'withheld',\n",
       " 'edgewater',\n",
       " 'njsource',\n",
       " 'coroflot',\n",
       " 'lynis',\n",
       " 'releases',\n",
       " 'open',\n",
       " 'source',\n",
       " 'auditing',\n",
       " 'linux',\n",
       " 'system',\n",
       " 'lynisopensou',\n",
       " 'rceauditinglinuxsystem',\n",
       " 'trumps',\n",
       " 'new',\n",
       " 'plan',\n",
       " 'save',\n",
       " 'jobs',\n",
       " 'au',\n",
       " 'mtfqyg',\n",
       " 'yup',\n",
       " 'thegrugqstatu',\n",
       " 'na',\n",
       " 'cve',\n",
       " 'blackberry',\n",
       " 'qnx',\n",
       " 'software',\n",
       " 'development',\n",
       " 'platform',\n",
       " 'zjqvmp',\n",
       " 'blackberry',\n",
       " 'qnx',\n",
       " 'software',\n",
       " 'development',\n",
       " 'pla',\n",
       " 'cve',\n",
       " 'crosssite',\n",
       " 'scripting',\n",
       " 'xss',\n",
       " 'vulnerability',\n",
       " 'jsviewerpaneljs',\n",
       " 'file',\n",
       " 'previewer',\n",
       " 'plugin',\n",
       " 'tjzhul',\n",
       " 'iasielloe',\n",
       " 'counter',\n",
       " 'cyber',\n",
       " 'terrorism',\n",
       " 'analysis',\n",
       " 'growth',\n",
       " 'trends',\n",
       " 'progress',\n",
       " 'challenges',\n",
       " 'wmeqle',\n",
       " 'hacking',\n",
       " 'new',\n",
       " 'diy',\n",
       " 'google',\n",
       " 'dorks',\n",
       " 'based',\n",
       " 'hacking',\n",
       " 'tool',\n",
       " 'funjf',\n",
       " 'securityaffairs',\n",
       " 'diy',\n",
       " 'cybercrime',\n",
       " 'hacking',\n",
       " 'google',\n",
       " 'kaspersky',\n",
       " 'mobile',\n",
       " 'security',\n",
       " 'blackberry',\n",
       " 'netsecuorgidbfbf',\n",
       " 'security',\n",
       " 'nakedsecurity',\n",
       " 'policetaunting',\n",
       " 'facebook',\n",
       " 'selfie',\n",
       " 'poster',\n",
       " 'jailed',\n",
       " 'naked',\n",
       " 'security',\n",
       " 'ifabab',\n",
       " 'security',\n",
       " 'w',\n",
       " 'shizgobptrbdr',\n",
       " 'fbvwtgt',\n",
       " 'doubling',\n",
       " 'security',\n",
       " 'sunday',\n",
       " 'business',\n",
       " 'post',\n",
       " 'qgpckd',\n",
       " 'eebfol',\n",
       " 'mm',\n",
       " 'bugtraq',\n",
       " 'kl',\n",
       " 'barracuda',\n",
       " 'waf',\n",
       " 'internal',\n",
       " 'development',\n",
       " 'credential',\n",
       " 'disclosure',\n",
       " 'tproyz',\n",
       " 'dlink',\n",
       " 'dgstc',\n",
       " 'cross',\n",
       " 'site',\n",
       " 'request',\n",
       " 'forgery',\n",
       " 'files',\n",
       " 'exploit',\n",
       " 'vision',\n",
       " 'clear',\n",
       " 'jesse',\n",
       " 'lyn',\n",
       " 'stoner',\n",
       " 'quote',\n",
       " 'xindi',\n",
       " 'coldfusion',\n",
       " 'gkjx',\n",
       " 'really',\n",
       " 'anonymity',\n",
       " 'internet',\n",
       " 'linkedincomtodaypostart',\n",
       " 'iclecanyoureallyhaveanonymityontheinternet',\n",
       " 'jvndbisc',\n",
       " 'bind',\n",
       " 'named',\n",
       " 'validator',\n",
       " 'prnewswire',\n",
       " 'onetrust',\n",
       " 'acquires',\n",
       " 'leading',\n",
       " 'website',\n",
       " 'auditing',\n",
       " 'cookie',\n",
       " 'compliance',\n",
       " 'solution',\n",
       " 'ied',\n",
       " 'security',\n",
       " 'ip',\n",
       " 'webcam',\n",
       " 'turns',\n",
       " 'android',\n",
       " 'phone',\n",
       " 'remote',\n",
       " 'camera',\n",
       " 'video',\n",
       " 'alleged',\n",
       " 'jpmorgan',\n",
       " 'hacker',\n",
       " 'arrested',\n",
       " 'new',\n",
       " 'york',\n",
       " 'joshua',\n",
       " 'aaron',\n",
       " 'deported',\n",
       " 'russia',\n",
       " 'cuffed',\n",
       " 'entry',\n",
       " 'us',\n",
       " 'vduocyaa',\n",
       " 'eventdyson',\n",
       " 'eye',\n",
       " 'mlc',\n",
       " 'dyson',\n",
       " 'eye',\n",
       " 'troj',\n",
       " 'cerberabv',\n",
       " 'nbwww',\n",
       " 'facebook',\n",
       " 'removes',\n",
       " 'eu',\n",
       " 'referendum',\n",
       " 'status',\n",
       " 'suggestions',\n",
       " 'facebook',\n",
       " 'removes',\n",
       " 'proleave',\n",
       " 'proremain',\n",
       " 'status',\n",
       " 'update',\n",
       " 'sug',\n",
       " 'ccejphu',\n",
       " 'cve',\n",
       " 'inet',\n",
       " 'module',\n",
       " 'freebsd',\n",
       " 'x',\n",
       " 'prerelease',\n",
       " 'betap',\n",
       " 'rcp',\n",
       " 'x',\n",
       " 'uwzwgc',\n",
       " 'facebook',\n",
       " 'privacy',\n",
       " 'hoax',\n",
       " 'chain',\n",
       " 'letter',\n",
       " 'rises',\n",
       " 'grave',\n",
       " 'mtgpsv',\n",
       " 'ubuntu',\n",
       " 'security',\n",
       " 'notice',\n",
       " 'usn',\n",
       " 'filesu',\n",
       " 'sntxt',\n",
       " 'packetstorm',\n",
       " 'euleros',\n",
       " 'sp',\n",
       " 'binutils',\n",
       " 'eulerossa',\n",
       " 'tenablecompluginsnessus',\n",
       " 'nessus',\n",
       " 'opdevsec',\n",
       " 'rt',\n",
       " 'cyberhitchhiker',\n",
       " 'cyberthieves',\n",
       " 'seek',\n",
       " 'cash',\n",
       " 'bitcoin',\n",
       " 'boom',\n",
       " 'cyb',\n",
       " 'erthievesseekcashbitcoinboom',\n",
       " 'infosec',\n",
       " 'malware',\n",
       " 'truekonrads',\n",
       " 'disagree',\n",
       " 'think',\n",
       " 'better',\n",
       " 'probably',\n",
       " 'even',\n",
       " 'better',\n",
       " 'cross',\n",
       " 'penetration',\n",
       " 'team',\n",
       " 'pos',\n",
       " 'malware',\n",
       " 'treasurehunter',\n",
       " 'source',\n",
       " 'code',\n",
       " 'leaked',\n",
       " 'qsrvnw',\n",
       " 'job',\n",
       " 'outlook',\n",
       " 'cisa',\n",
       " 'professionals',\n",
       " 'hlkphc',\n",
       " 'ransommatrix',\n",
       " 'fbjozm',\n",
       " 'pci',\n",
       " 'set',\n",
       " 'ban',\n",
       " 'ssl',\n",
       " 'protocol',\n",
       " 'fbnjpcez',\n",
       " 'cve',\n",
       " 'ibm',\n",
       " 'bigfix',\n",
       " 'platform',\n",
       " 'set',\n",
       " 'httponly',\n",
       " 'attribute',\n",
       " 'authorization',\n",
       " 'tokens',\n",
       " 'session',\n",
       " 'cookies',\n",
       " 'crosssite',\n",
       " 'scripting',\n",
       " 'vulnerability',\n",
       " 'also',\n",
       " 'existed',\n",
       " 'attackers',\n",
       " 'may',\n",
       " 'able',\n",
       " 'get',\n",
       " 'cookie',\n",
       " 'va',\n",
       " 'qnoasj',\n",
       " 'threatmeter',\n",
       " 'pulse',\n",
       " 'secure',\n",
       " 'pulse',\n",
       " 'one',\n",
       " 'onpremise',\n",
       " 'information',\n",
       " 'disclosure',\n",
       " 'xelyy',\n",
       " 'cve',\n",
       " 'realplayer',\n",
       " 'allows',\n",
       " 'remote',\n",
       " 'attackers',\n",
       " 'cause',\n",
       " 'denial',\n",
       " 'service',\n",
       " 'dividebyzero',\n",
       " 'error',\n",
       " 'rggtan',\n",
       " 're',\n",
       " 'executable',\n",
       " 'installers',\n",
       " 'vulnerablewevil',\n",
       " 'case',\n",
       " 'fsecureonlinescannerexe',\n",
       " 'allows',\n",
       " 'arbitra',\n",
       " 'fulldisclosure',\n",
       " 'dec',\n",
       " 'fulldisclosure',\n",
       " 'hacker',\n",
       " 'interviews',\n",
       " 'speaking',\n",
       " 'ghostshell',\n",
       " 'egsgtwq',\n",
       " 'securityaffairs',\n",
       " 'ghostshell',\n",
       " 'hacking',\n",
       " 'service',\n",
       " 'manager',\n",
       " 'throw',\n",
       " 'xml',\n",
       " 'spanner',\n",
       " 'orchestrator',\n",
       " 'works',\n",
       " 'yx',\n",
       " 'microsofttechnet',\n",
       " 'water',\n",
       " 'wars',\n",
       " 'sinking',\n",
       " 'feeling',\n",
       " 'philippinechina',\n",
       " 'relations',\n",
       " 'itsecuritynewsinfowaterwarsas',\n",
       " 'inkingfeelinginphilippinechinarelations',\n",
       " 'nokias',\n",
       " 'network',\n",
       " 'profits',\n",
       " 'drop',\n",
       " 'raise',\n",
       " 'concerns',\n",
       " 'alcatel',\n",
       " 'deal',\n",
       " 'jussi',\n",
       " 'rosendahl',\n",
       " 'leila',\n",
       " 'abboud',\n",
       " 'helsinkipa',\n",
       " 'eujiib',\n",
       " 'star',\n",
       " 'wars',\n",
       " 'day',\n",
       " 'australia',\n",
       " 'asia',\n",
       " 'people',\n",
       " 'going',\n",
       " 'interstellar',\n",
       " 'crazy',\n",
       " 'thanks',\n",
       " 'wonders',\n",
       " 'dyzwjy',\n",
       " 'new',\n",
       " 'ads',\n",
       " 'google',\n",
       " 'maps',\n",
       " 'might',\n",
       " 'suggest',\n",
       " 'pit',\n",
       " 'stop',\n",
       " 'mcdonalds',\n",
       " 'irdelg',\n",
       " 'hacker',\n",
       " 'exposed',\n",
       " 'facebook',\n",
       " 'bug',\n",
       " 'get',\n",
       " 'reward',\n",
       " 'unexpected',\n",
       " 'source',\n",
       " 'times',\n",
       " 'india',\n",
       " 'hacker',\n",
       " 'fbgklf',\n",
       " 'india',\n",
       " 'emerged',\n",
       " 'hub',\n",
       " 'industry',\n",
       " 'due',\n",
       " 'phenomena',\n",
       " 'lleme',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'azealia',\n",
       " 'banks',\n",
       " 'twitter',\n",
       " 'account',\n",
       " 'suspended',\n",
       " 'racist',\n",
       " 'rant',\n",
       " 'targeting',\n",
       " 'zayn',\n",
       " 'malik',\n",
       " 'niltemf',\n",
       " 'medsec',\n",
       " 'goes',\n",
       " 'way',\n",
       " 'medical',\n",
       " 'device',\n",
       " 'flaw',\n",
       " 'eschewing',\n",
       " 'bug',\n",
       " 'bounty',\n",
       " 'route',\n",
       " 'cybersecurity',\n",
       " 'fi',\n",
       " 'bunck',\n",
       " 'infosec',\n",
       " 'windows',\n",
       " 'update',\n",
       " 'release',\n",
       " 'date',\n",
       " 'may',\n",
       " 'arrive',\n",
       " 'aug',\n",
       " 'digital',\n",
       " 'trends',\n",
       " 'netsecuorgicf',\n",
       " 'security',\n",
       " 'big',\n",
       " 'data',\n",
       " 'pushing',\n",
       " 'd',\n",
       " 'printing',\n",
       " 'tipping',\n",
       " 'point',\n",
       " 'washalsec',\n",
       " 'rt',\n",
       " 'isaudit',\n",
       " 'introducing',\n",
       " 'windows',\n",
       " 'defender',\n",
       " 'application',\n",
       " 'control',\n",
       " 'windows',\n",
       " 'native',\n",
       " 'whitelisting',\n",
       " 'mmpc',\n",
       " 'introducingwindowsdefenderapplicationcontrol',\n",
       " 'understanding',\n",
       " 'account',\n",
       " 'creation',\n",
       " 'privilege',\n",
       " 'escalation',\n",
       " 'vulnerability',\n",
       " 'joomla',\n",
       " 'epayt',\n",
       " 'appseceu',\n",
       " 'tin',\n",
       " 'zaw',\n",
       " 'scott',\n",
       " 'matsumoto',\n",
       " 'threat',\n",
       " 'modeling',\n",
       " 'brief',\n",
       " 'history',\n",
       " 'unified',\n",
       " 'approach',\n",
       " 'securitytubenetvideo',\n",
       " 'securitytube',\n",
       " 'techcrunch',\n",
       " 'sony',\n",
       " 'rd',\n",
       " 'massive',\n",
       " 'leak',\n",
       " 'million',\n",
       " 'users',\n",
       " 'personal',\n",
       " 'info',\n",
       " 'hacked',\n",
       " 'japan',\n",
       " 'hacked',\n",
       " 'infosec',\n",
       " 'anonymous',\n",
       " 'cve',\n",
       " 'iihwhz',\n",
       " 'cybersecurity',\n",
       " 'hacker',\n",
       " 'informationskrieg',\n",
       " 'und',\n",
       " 'cyber',\n",
       " 'war',\n",
       " 'vvfyn',\n",
       " 'facebooks',\n",
       " 'data',\n",
       " 'blowup',\n",
       " 'daos',\n",
       " 'leveraging',\n",
       " 'blockchain',\n",
       " 'future',\n",
       " 'qqdzch',\n",
       " 'rotating',\n",
       " 'iframe',\n",
       " 'urls',\n",
       " 'one',\n",
       " 'minute',\n",
       " 'mbrlf',\n",
       " 'cve',\n",
       " 'wireshark',\n",
       " 'wireshark',\n",
       " 'score',\n",
       " 'wireshark',\n",
       " 'ixveriwave',\n",
       " 'file',\n",
       " 'parser',\n",
       " 'crash',\n",
       " 'addressed',\n",
       " 'wiretapvwrc',\n",
       " 'correcting',\n",
       " 'signature',\n",
       " 'timestamp',\n",
       " 'bou',\n",
       " 'vulnerabilityd',\n",
       " 'etailsqidcve',\n",
       " 'une',\n",
       " 'rvolution',\n",
       " 'cinmatographique',\n",
       " 'rt',\n",
       " 'lepost',\n",
       " 'james',\n",
       " 'cameron',\n",
       " 'vatil',\n",
       " 'entrer',\n",
       " 'dans',\n",
       " 'lhistoire',\n",
       " 'ce',\n",
       " 'juillet',\n",
       " 'marcus',\n",
       " 'aka',\n",
       " 'wntn',\n",
       " 'seventeen',\n",
       " 'years',\n",
       " 'old',\n",
       " 'figur',\n",
       " 'vuvcg',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'gchq',\n",
       " 'ran',\n",
       " 'dos',\n",
       " 'attack',\n",
       " 'chatrooms',\n",
       " 'used',\n",
       " 'anonymous',\n",
       " 'lulzsec',\n",
       " 'drzxakp',\n",
       " 'securityaffairs',\n",
       " 'gchq',\n",
       " 'anonymous',\n",
       " 'lulzsec',\n",
       " 'shingles',\n",
       " 'look',\n",
       " 'virus',\n",
       " 'linmanuel',\n",
       " 'mirandas',\n",
       " 'diagnosis',\n",
       " 'cbs',\n",
       " 'news',\n",
       " 'qpjdjq',\n",
       " 'jqueryuirails',\n",
       " 'jqsy',\n",
       " 'groundbreaking',\n",
       " 'exploration',\n",
       " 'identify',\n",
       " 'fight',\n",
       " 'security',\n",
       " 'qsputl',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'massive',\n",
       " 'epsilon',\n",
       " 'email',\n",
       " 'breach',\n",
       " 'hits',\n",
       " 'citi',\n",
       " 'chase',\n",
       " 'dvfdh',\n",
       " 'rt',\n",
       " 'securityaffairs',\n",
       " 'oracle',\n",
       " 'january',\n",
       " 'critical',\n",
       " 'patch',\n",
       " 'update',\n",
       " 'also',\n",
       " 'addresses',\n",
       " 'spectre',\n",
       " 'meltdown',\n",
       " 'bcott',\n",
       " 'securityaffairs',\n",
       " 'cybersecurity',\n",
       " 'hacker',\n",
       " 'antivirus',\n",
       " 'hackers',\n",
       " 'handbook',\n",
       " 'fdpthm',\n",
       " 'wuzhicms',\n",
       " 'multiple',\n",
       " 'cross',\n",
       " 'site',\n",
       " 'scripting',\n",
       " 'vulnerabilities',\n",
       " 'qttscg',\n",
       " 'cybersecurity',\n",
       " 'hacker',\n",
       " 'cybersecurity',\n",
       " 'everyone',\n",
       " 'securing',\n",
       " 'home',\n",
       " 'small',\n",
       " 'business',\n",
       " 'network',\n",
       " 'ksvwa',\n",
       " 'guy',\n",
       " 'knows',\n",
       " 'make',\n",
       " 'deal',\n",
       " 'cve',\n",
       " 'cisco',\n",
       " 'unified',\n",
       " 'communications',\n",
       " 'manager',\n",
       " 'realtime',\n",
       " 'monitoring',\n",
       " 'tool',\n",
       " 'information',\n",
       " 'disclosure',\n",
       " 'threats',\n",
       " 'mcafee',\n",
       " 'shopping',\n",
       " 'apps',\n",
       " 'ctinglobal',\n",
       " 'rt',\n",
       " 'securityaffairs',\n",
       " 'terabytes',\n",
       " 'us',\n",
       " 'military',\n",
       " 'social',\n",
       " 'media',\n",
       " 'surveillance',\n",
       " 'miserably',\n",
       " 'left',\n",
       " 'wide',\n",
       " 'open',\n",
       " 'aws',\n",
       " 'buckets',\n",
       " 'axveky',\n",
       " 'cyberexaminer',\n",
       " 'security',\n",
       " 'cve',\n",
       " 'pmbthy',\n",
       " 'infosec',\n",
       " 'jvndbpragyan',\n",
       " 'cms',\n",
       " 'sql',\n",
       " 'khqhu',\n",
       " 'pragyan',\n",
       " 'cms',\n",
       " 'sql',\n",
       " 'vulnerability',\n",
       " 'analysis',\n",
       " 'joomla',\n",
       " 'comacymailing',\n",
       " 'components',\n",
       " 'database',\n",
       " 'backup',\n",
       " 'disclosure',\n",
       " 'issuewlb',\n",
       " 'cxsecurity',\n",
       " 'cso',\n",
       " 'online',\n",
       " 'new',\n",
       " 'wordpress',\n",
       " 'plugin',\n",
       " 'exploit',\n",
       " 'endangers',\n",
       " 'thousands',\n",
       " 'websites',\n",
       " 'tbggb',\n",
       " 'exploit',\n",
       " 'alert',\n",
       " 'researchers',\n",
       " 'track',\n",
       " 'subway',\n",
       " 'riders',\n",
       " 'using',\n",
       " 'smartphone',\n",
       " 'malware',\n",
       " 'egmxk',\n",
       " 'fantastic',\n",
       " 'book',\n",
       " 'anyone',\n",
       " 'looking',\n",
       " 'learn',\n",
       " 'tools',\n",
       " 'techni',\n",
       " 'llflca',\n",
       " 'cybersecurity',\n",
       " 'bitcoin',\n",
       " 'voulnet',\n",
       " 'theres',\n",
       " 'firefox',\n",
       " 'extentsions',\n",
       " 'longer',\n",
       " 'supported',\n",
       " 'copycat',\n",
       " 'chrome',\n",
       " 'extension',\n",
       " 'already',\n",
       " 'power',\n",
       " 'open',\n",
       " 'sores',\n",
       " 'tagtaggerjs',\n",
       " 'cvwvy',\n",
       " 'honeypots',\n",
       " 'demonstrated',\n",
       " 'immense',\n",
       " 'value',\n",
       " 'internet',\n",
       " 'security',\n",
       " 'ifrajc',\n",
       " 'hacker',\n",
       " 'cybersecurity',\n",
       " 'mahara',\n",
       " 'x',\n",
       " 'url',\n",
       " 'denial',\n",
       " 'service',\n",
       " 'hfkrxb',\n",
       " 'microsoft',\n",
       " 'issues',\n",
       " 'outofband',\n",
       " 'fix',\n",
       " 'intel',\n",
       " 'broken',\n",
       " 'spectre',\n",
       " 'patch',\n",
       " 'newsmicrosoftoutofbandfixintels',\n",
       " 'infosecuritymagazinecomnewsmicrosoft',\n",
       " 'infosecmag',\n",
       " 'eff',\n",
       " 'supreme',\n",
       " 'court',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "# ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÏóêÏÑúÏùò token\n",
    "brown_data = vocabulary.staticList #[:200000]\n",
    "brown_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d914e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1777350"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords Ï†úÏô∏Ìïú token\n",
    "brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\n",
    "len(brown_nonstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9802bd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22364\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n"
     ]
    }
   ],
   "source": [
    "kkk = brown.words()[:200000]\n",
    "print(len(set(kkk)))\n",
    "print(kkk[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fef04966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3112"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Í∏∞Ï°¥Ïóê pre_gloveÏóêÏÑú ÏóÜÎçò ÏÉàÎ°úÏö¥ ÌÜ†ÌÅ∞Îì§\n",
    "oov = [token for token in brown_nonstop if token not in pre_glove.keys()]\n",
    "len(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb170fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3112"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(oov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9f2d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rareoov(xdict, val):\n",
    "    return [k for (k,v) in Counter(xdict).items() if v<=val]\n",
    "# frequencyÍ∞Ä 10Í∞ú Ïù¥ÌïòÏù∏ Îç∞Ïù¥ÌÑ∞ ÏßëÌï©\n",
    "oov_rare = get_rareoov(oov, 1) # Ï≤òÏùåÏóêÎßå 200ÏúºÎ°ú ÏÑ§Ï†ïÌïòÍ≥† Í∑∏ ÌõÑÎ°úÎäî 500ÏúºÎ°ú ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18bff28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3112"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oov_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a164545b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequencyÍ∞Ä 10Í∞ú ÎØ∏ÎßåÏù∏ Îç∞Ïù¥ÌÑ∞ Ï†úÏô∏Ìïú ÏÉàÎ°úÏö¥ ÌÜ†ÌÅ∞Îì§\n",
    "corp_vocab = list(set(oov) - set(oov_rare))\n",
    "len(corp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d336d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84446"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oov_rareÏóêÏÑú keyword_listÎÇ¥Ïùò tokenÏùÑ Ï†úÍ±∞ÌïòÍ≥†\n",
    "oov_rare = list(set(oov_rare) - set(keyword_list))\n",
    "len(oov_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79f774ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25566"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aaa.staticList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbe423e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aaa.staticList[0]) + 650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "275a593a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corp_vocabÏóê keyword_listÎÇ¥Ïùò tokenÏùÑ appendÌï¥ÏïºÌï®\n",
    "corp_vocab = list(set(corp_vocab + keyword_list)) ## Ïó¨Í∏∞Ïóêcorp_vocabÎåÄÏã† aa(Ï¶â voc.staticListÏóê Ï∂îÍ∞ÄÎêú Îã®Ïñ¥ÎèÑ Ï∂îÍ∞Ä Í≥†Î†§,,)\n",
    "len(corp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e74e6eec",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-9e61f5f97ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# time complexityÍ∞Ä ÎÑàÎ¨¥ Ïª§ÏÑú Ïã§ÌñâÌïòÏßÄ Î™ªÌï®\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Îî∞ÎùºÏÑú Í∑∏ÎÉ• brown_docÏóê brown_nonstopÎßå ÎÑòÍ≤®Ï§å..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbrown_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_nonstop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moov_rare\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-9e61f5f97ac7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# time complexityÍ∞Ä ÎÑàÎ¨¥ Ïª§ÏÑú Ïã§ÌñâÌïòÏßÄ Î™ªÌï®\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Îî∞ÎùºÏÑú Í∑∏ÎÉ• brown_docÏóê brown_nonstopÎßå ÎÑòÍ≤®Ï§å..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbrown_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown_nonstop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moov_rare\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞Ï§ëÏóêÏÑú frequencyÍ∞Ä 10ÎØ∏Îßå Îç∞Ïù¥ÌÑ∞Î•º Ï†úÏô∏ÌïòÍ≥† ÏÉàÎ°úÏö¥ ÌÜ†ÌÅ∞Îì§\n",
    "# time complexityÍ∞Ä ÎÑàÎ¨¥ Ïª§ÏÑú Ïã§ÌñâÌïòÏßÄ Î™ªÌï®\n",
    "# Îî∞ÎùºÏÑú Í∑∏ÎÉ• brown_docÏóê brown_nonstopÎßå ÎÑòÍ≤®Ï§å..\n",
    "brown_tokens = [token for token in brown_nonstop if token not in oov_rare]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8683da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_doc = [' '.join(brown_nonstop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ba1c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n",
      "Iteration 10000: loss: 60.98265838623047"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from mittens import GloVe, Mittens\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "# corp_vocab = list(set(oov))\n",
    "# brown_doc = [' '.join(brown_nonstop)]\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "X = cv.fit_transform(brown_doc)\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "coocc_ar = Xc.toarray()\n",
    "\n",
    "mittens_model = Mittens(n=100, max_iter=10000, learning_rate=0.001) # small learning rate should be offered 0.0001\n",
    "\n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc_ar,\n",
    "    vocab=corp_vocab,\n",
    "    initial_embedding_dict= pre_glove)\n",
    "\n",
    "newglove = dict(zip(corp_vocab, new_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ea89ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "pre_glove.update(newglove)\n",
    "\n",
    "f = open(\"repo_glove_2.pkl\",\"wb\")\n",
    "pickle.dump(pre_glove, f)\n",
    "print('success')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ebc2432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newglove.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33e2cde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400621"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_glove.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef6667",
   "metadata": {},
   "source": [
    "# ÏòàÏ∏° ÏàòÌñâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "530e5573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+\n",
      "|                text|label_index|             feature|\n",
      "+--------------------+-----------+--------------------+\n",
      "|threatmeter hacke...|        1.0|[176.0,319.0,1385...|\n",
      "|first android mal...|        1.0|[115.0,128.0,82.0...|\n",
      "|adobe fixes six c...|        1.0|[562.0,1449.0,220...|\n",
      "| scienceporn  in ...|        1.0|[1.0,4.0,5.0,1438...|\n",
      "|riskware hmoyfzb ...|        1.0|[1.0,1.0,1.0,0.0,...|\n",
      "+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_vectors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0226afe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 16486 words (3514 misses)\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained Glove weight\n",
    "f = open(\"repo_glove_1.pkl\",\"rb\")\n",
    "newglove = pickle.load(f)\n",
    "\n",
    "num_tokens = len(voc.staticList) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.staticDict.items():\n",
    "    embedding_vector = newglove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71ea7fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400621"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newglove.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebaa1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "nb_classes = 2\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b411d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pretrained_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         2000200   \n",
      "_________________________________________________________________\n",
      "f1 (Conv1D)                  (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "f2 (Conv1D)                  (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "f3 (Conv1D)                  (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,245,194\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", name='f1')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", name='f2')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", name='f3')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\", name='d1')(x)\n",
    "preds = layers.Dense(2, activation=\"softmax\", name='d2')(x)\n",
    "pretrained = keras.Model(int_sequences_input, preds, name=\"pretrained_model\")\n",
    "pretrained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10189090",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32642d",
   "metadata": {},
   "source": [
    "## fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a875564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 20063 words (5503 misses)\n"
     ]
    }
   ],
   "source": [
    "# load the pre-trained Glove weight\n",
    "f = open(\"repo_glove_2.pkl\",\"rb\")\n",
    "newglove = pickle.load(f)\n",
    "\n",
    "num_tokens = len(voc.staticList) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.staticDict.items():\n",
    "    embedding_vector = newglove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "752df66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "nb_classes = 2\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    name = \"e1\" # Í≥ÑÏÜç Î∞îÍøîÏ§òÏïºÌï¥\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a240660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pretrained (Functional)      (None, 128)               2785024   \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,801,794\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 2,556,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", name='f1')(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", name='f2')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", name='f3')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "pretrained_model = keras.Model(int_sequences_input, x, name=\"pretrained\")\n",
    "\n",
    "model = keras.Sequential([pretrained_model, layers.Dense(128, activation=\"relu\", name='d1'), layers.Dense(2, activation=\"softmax\", name='d2')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4620c5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6981e23898>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.load_weights(\"pretrained_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aaaa85ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f813f7dfa58>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f813f34fda0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x7f813f269a90>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x7f81e7cbf828>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x7f813f2baeb8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x7f813fa70978>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x7f813f7df128>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D at 0x7f813f2eab38>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f813f2ea470>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f813f2c40b8>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7bd96e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'e1/embeddings:0' shape=(25568, 100) dtype=float32, numpy=\n",
       " array([[ 0.32015198,  0.32985097, -0.63490605, ...,  0.605566  ,\n",
       "         -1.7903564 ,  1.261064  ],\n",
       "        [-1.0016309 ,  2.0705097 ,  0.06739932, ..., -1.5806402 ,\n",
       "         -0.74558794,  0.5540653 ],\n",
       "        [ 0.828892  ,  0.42481294, -1.5089525 , ...,  0.8579917 ,\n",
       "          1.1408814 , -0.8615934 ],\n",
       "        ...,\n",
       "        [ 0.7850558 ,  2.0836573 , -1.2791005 , ..., -0.15426576,\n",
       "         -1.610927  ,  1.0542847 ],\n",
       "        [ 0.6916777 ,  0.4460445 , -0.70862865, ..., -1.1327271 ,\n",
       "         -0.97815156,  1.7759557 ],\n",
       "        [-1.8696328 ,  1.4004666 , -0.22077304, ..., -0.2562677 ,\n",
       "         -0.12459616,  0.33991626]], dtype=float32)>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.layers[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7ae60fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'd2/kernel:0' shape=(128, 2) dtype=float32, numpy=\n",
       " array([[ 0.568587  , -0.5979264 ],\n",
       "        [-0.4200679 , -0.3741734 ],\n",
       "        [-0.80700105, -2.3267548 ],\n",
       "        [-0.6535369 ,  0.58927923],\n",
       "        [-2.9724457 , -0.10927045],\n",
       "        [-0.2552872 , -0.02338264],\n",
       "        [-0.01662377, -0.93165255],\n",
       "        [-0.3466121 ,  0.6077571 ],\n",
       "        [ 1.0392745 ,  1.8214877 ],\n",
       "        [ 0.151336  ,  0.68245864],\n",
       "        [ 0.18562372,  1.4929657 ],\n",
       "        [ 0.39345235, -0.62674046],\n",
       "        [-0.10179545, -1.6648691 ],\n",
       "        [-0.742026  , -0.5744636 ],\n",
       "        [ 0.26150528, -1.21723   ],\n",
       "        [ 0.861037  ,  1.450817  ],\n",
       "        [-0.13965195, -2.2885194 ],\n",
       "        [-0.65240747,  0.41471517],\n",
       "        [-1.4281089 , -1.248416  ],\n",
       "        [-0.8578196 ,  1.0589912 ],\n",
       "        [ 0.8788392 ,  0.6632096 ],\n",
       "        [ 0.78547144, -0.11804313],\n",
       "        [ 1.0712187 ,  0.03534868],\n",
       "        [ 1.1237779 , -1.1491095 ],\n",
       "        [ 2.7552938 ,  0.5297781 ],\n",
       "        [-0.53636134, -1.999765  ],\n",
       "        [ 1.4431041 , -0.94546664],\n",
       "        [ 0.14972864,  0.64627516],\n",
       "        [ 0.10877245,  0.31730193],\n",
       "        [-0.33822155, -0.8665231 ],\n",
       "        [ 0.4369921 , -0.8534431 ],\n",
       "        [-0.36424562,  0.8069341 ],\n",
       "        [-0.28137144, -2.0588331 ],\n",
       "        [-2.3171546 ,  1.553267  ],\n",
       "        [-0.19158918, -1.4062096 ],\n",
       "        [ 1.1044089 ,  0.4947025 ],\n",
       "        [ 0.6153719 ,  0.5951623 ],\n",
       "        [ 0.24635999, -2.4475384 ],\n",
       "        [ 0.21743464,  0.13107201],\n",
       "        [-0.04414855,  0.9673723 ],\n",
       "        [-0.8794731 ,  1.8776739 ],\n",
       "        [ 2.1395783 , -1.0890219 ],\n",
       "        [ 1.1884961 , -1.0969929 ],\n",
       "        [-0.24639033, -1.1399299 ],\n",
       "        [ 1.667783  ,  0.26625884],\n",
       "        [-0.7093267 ,  1.7414371 ],\n",
       "        [ 0.47574776,  0.21633698],\n",
       "        [-0.9133436 , -0.6281488 ],\n",
       "        [-0.16151698, -0.67015   ],\n",
       "        [-1.3506719 ,  0.0487797 ],\n",
       "        [ 0.3826862 ,  1.5616254 ],\n",
       "        [-0.5796396 ,  0.19036293],\n",
       "        [-1.842684  , -1.5236114 ],\n",
       "        [ 0.9551411 , -1.4641764 ],\n",
       "        [-0.82730013,  2.8599157 ],\n",
       "        [ 1.6343211 ,  0.07646222],\n",
       "        [ 0.20564361,  0.77889204],\n",
       "        [ 0.08413884,  1.2315502 ],\n",
       "        [ 1.5728886 ,  0.20533499],\n",
       "        [ 0.53152245, -0.14688124],\n",
       "        [-0.13208611, -0.42704427],\n",
       "        [-0.26389572,  1.045893  ],\n",
       "        [-1.4342489 , -0.31424588],\n",
       "        [ 0.88804805, -0.9730512 ],\n",
       "        [ 1.3556293 , -2.0223536 ],\n",
       "        [ 1.4335288 , -0.11524297],\n",
       "        [ 0.99554807,  0.7019587 ],\n",
       "        [ 0.25753847, -0.6320522 ],\n",
       "        [-0.01225283,  1.3217316 ],\n",
       "        [-0.6575367 ,  0.5584872 ],\n",
       "        [-1.7240651 ,  1.8941741 ],\n",
       "        [ 0.11386551, -0.3949718 ],\n",
       "        [ 1.95269   ,  0.5304827 ],\n",
       "        [-0.84228647, -0.8760843 ],\n",
       "        [ 0.04250144, -0.12205792],\n",
       "        [-1.0577837 ,  0.09135031],\n",
       "        [-0.20346338, -2.9504924 ],\n",
       "        [ 1.5930887 , -0.5977096 ],\n",
       "        [ 0.45698592, -2.4190974 ],\n",
       "        [ 1.1191283 ,  1.6393775 ],\n",
       "        [-1.3268781 ,  0.6161009 ],\n",
       "        [-1.1661578 ,  0.3082897 ],\n",
       "        [-0.5174286 , -0.22600546],\n",
       "        [ 0.51338094,  0.5232453 ],\n",
       "        [-0.39519477, -0.7038045 ],\n",
       "        [-0.42630282,  0.01907237],\n",
       "        [ 0.2835821 , -1.0019537 ],\n",
       "        [ 0.8189255 , -0.3434938 ],\n",
       "        [ 0.38426697,  1.4315574 ],\n",
       "        [ 1.0836619 , -0.47031677],\n",
       "        [-0.46084273,  0.970182  ],\n",
       "        [-0.59912115, -0.29614425],\n",
       "        [-0.78112453, -1.0333588 ],\n",
       "        [ 1.4510635 ,  0.31169915],\n",
       "        [-1.0555661 , -0.5254204 ],\n",
       "        [ 1.6162012 , -2.0718915 ],\n",
       "        [-0.73973185, -1.5909479 ],\n",
       "        [-0.47394907, -1.2450665 ],\n",
       "        [ 0.7402902 , -0.6013937 ],\n",
       "        [ 0.15546776,  0.71599114],\n",
       "        [ 2.107005  ,  1.9920281 ],\n",
       "        [ 0.36478803, -0.1666025 ],\n",
       "        [ 0.8895085 ,  0.55499595],\n",
       "        [ 0.84897274, -1.1187725 ],\n",
       "        [ 0.62657166,  0.17246288],\n",
       "        [-0.14857246, -0.33493647],\n",
       "        [ 0.530417  , -1.3035234 ],\n",
       "        [-0.5602573 ,  1.4490535 ],\n",
       "        [ 0.7190195 , -0.6721361 ],\n",
       "        [ 0.94158196, -0.63362706],\n",
       "        [ 0.13006623, -1.942734  ],\n",
       "        [ 2.1455119 ,  0.0988049 ],\n",
       "        [-0.06946949, -0.62476623],\n",
       "        [ 0.17152454,  0.14250922],\n",
       "        [ 0.39290828, -2.6370955 ],\n",
       "        [ 0.19194436,  0.694393  ],\n",
       "        [ 0.7815351 ,  1.5055013 ],\n",
       "        [ 0.74029285,  0.17239769],\n",
       "        [-1.3335078 , -0.94513404],\n",
       "        [ 0.08994979, -1.5691264 ],\n",
       "        [-1.2902809 ,  1.5219226 ],\n",
       "        [ 0.7621957 , -0.03359838],\n",
       "        [ 1.0223674 , -0.4169333 ],\n",
       "        [ 0.87579596, -0.26935613],\n",
       "        [-0.20820129, -0.53549546],\n",
       "        [ 1.0476463 ,  0.18453829],\n",
       "        [-1.613562  ,  0.42096025],\n",
       "        [ 0.7518164 , -1.0841439 ]], dtype=float32)>,\n",
       " <tf.Variable 'd2/bias:0' shape=(2,) dtype=float32, numpy=array([-1.9097711, -1.2359687], dtype=float32)>]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.layers[9].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e7df2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.layers[1].set_weights([embedding_matrix])\n",
    "pretrained_model.layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5628f7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f6981e0b6a0>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f698030f518>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x7f6981dfef60>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x7f6981e41710>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x7f6981defba8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling1D at 0x7f6981e34978>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv1D at 0x7f6981e67b00>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalMaxPooling1D at 0x7f6981e41860>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deee24aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained_model.layers[:-2]: \n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de541c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1e55fc03172b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check that all of the pretrained weights have been loaded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained' is not defined"
     ]
    }
   ],
   "source": [
    "# Check that all of the pretrained weights have been loaded.\n",
    "for a, b in zip(pretrained.weights, pretrained_model.weights):\n",
    "    np.testing.assert_allclose(a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59eea2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelÎ•º ÎèôÍ≤∞ÌïòÍ∏∞ Ï†Ñ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò: 2\n",
      "modelÎ•º ÎèôÍ≤∞ÌïòÍ∏∞ Ï†Ñ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò: 6\n",
      "modelÎ•º ÎèôÍ≤∞Ìïú ÌõÑ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò: 2\n",
      "modelÎ•º ÎèôÍ≤∞Ìïú ÌõÑ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò: 6\n"
     ]
    }
   ],
   "source": [
    "print('modelÎ•º ÎèôÍ≤∞ÌïòÍ∏∞ Ï†Ñ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò:',\n",
    "     len(pretrained_model.trainable_weights))\n",
    "print('modelÎ•º ÎèôÍ≤∞ÌïòÍ∏∞ Ï†Ñ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò:',\n",
    "     len(model.trainable_weights))\n",
    "#pretrained_model.trainable = False\n",
    "print('modelÎ•º ÎèôÍ≤∞Ìïú ÌõÑ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò:',\n",
    "     len(pretrained_model.trainable_weights))\n",
    "print('modelÎ•º ÎèôÍ≤∞Ìïú ÌõÑ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò:',\n",
    "     len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f461aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pretrained (Functional)      (None, 128)               2785024   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,801,794\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 2,556,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras import layers\n",
    "\n",
    "# int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "# embedded_sequences = embedding_layer(int_sequences_input)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "# x = layers.MaxPooling1D(5)(x)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "# x = layers.MaxPooling1D(5)(x)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "# x = layers.GlobalMaxPooling1D()(x)\n",
    "# pretrained_model = keras.Model(inputs=int_sequences_input, outputs=x, name=\"pretrained\")\n",
    "\n",
    "# # Sequential example:\n",
    "# model = keras.Sequential([pretrained_model, layers.Dense(128, activation=\"relu\"), layers.Dense(2, activation=\"softmax\")])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6d74105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'embed/embeddings:0' shape=(25568, 100) dtype=float32, numpy=\n",
      "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ],\n",
      "       [ 0.13277 , -0.11802 , -0.063487, ...,  0.10199 ,  1.0979  ,\n",
      "        -0.17951 ],\n",
      "       [-0.016233,  0.77261 , -0.052126, ...,  0.067194, -0.2468  ,\n",
      "        -0.12894 ],\n",
      "       ...,\n",
      "       [-0.75831 ,  0.42915 , -0.040031, ...,  0.11268 , -0.088691,\n",
      "        -0.20963 ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_6/kernel:0' shape=(5, 100, 128) dtype=float32, numpy=\n",
      "array([[[ 0.02555508, -0.03761939, -0.01906643, ...,  0.03926139,\n",
      "          0.04262729, -0.0114852 ],\n",
      "        [ 0.06523345,  0.06422513,  0.04403574, ..., -0.03475539,\n",
      "          0.07000978,  0.01799396],\n",
      "        [-0.04456381, -0.04624762, -0.0172409 , ...,  0.06080531,\n",
      "         -0.03362306, -0.07242708],\n",
      "        ...,\n",
      "        [ 0.00149523,  0.05364601,  0.01436412, ..., -0.02607363,\n",
      "         -0.01138011,  0.06933161],\n",
      "        [ 0.04208925,  0.06938622, -0.04046228, ...,  0.02062985,\n",
      "          0.01742989,  0.00247216],\n",
      "        [ 0.06380481,  0.02267549, -0.05905455, ...,  0.03455869,\n",
      "          0.0005587 , -0.0264839 ]],\n",
      "\n",
      "       [[ 0.04663457,  0.01409484,  0.03408898, ..., -0.05791549,\n",
      "         -0.07053029,  0.06435213],\n",
      "        [ 0.05364174, -0.02687805, -0.07099977, ...,  0.05803732,\n",
      "          0.02914582,  0.02729252],\n",
      "        [ 0.03982192, -0.02530805, -0.0211392 , ...,  0.00195527,\n",
      "          0.0708989 , -0.04897414],\n",
      "        ...,\n",
      "        [ 0.04443865,  0.02402097, -0.03504926, ...,  0.04210825,\n",
      "          0.07224467, -0.04366663],\n",
      "        [-0.00235499,  0.0279685 ,  0.04972479, ..., -0.04764944,\n",
      "         -0.00981852,  0.05166905],\n",
      "        [ 0.00587106,  0.01995499, -0.01548442, ..., -0.06362616,\n",
      "          0.02725562, -0.02885569]],\n",
      "\n",
      "       [[-0.05672379, -0.07197572, -0.00959521, ..., -0.05378919,\n",
      "          0.06137868, -0.00395954],\n",
      "        [-0.0430138 ,  0.00768634, -0.07104779, ...,  0.05425491,\n",
      "          0.04535935, -0.026992  ],\n",
      "        [-0.00181005,  0.05791045,  0.01743256, ...,  0.04320756,\n",
      "         -0.06479581,  0.07114009],\n",
      "        ...,\n",
      "        [ 0.02808279,  0.05127209, -0.05190396, ..., -0.02845738,\n",
      "         -0.01545024, -0.05025942],\n",
      "        [ 0.01155084,  0.00876214, -0.03739003, ...,  0.034801  ,\n",
      "         -0.00978693, -0.03009752],\n",
      "        [ 0.04164757, -0.04667188, -0.03693958, ...,  0.03087653,\n",
      "         -0.07208312, -0.02248073]],\n",
      "\n",
      "       [[ 0.00375776,  0.00164413, -0.01603766, ..., -0.03087105,\n",
      "         -0.01075033,  0.06862316],\n",
      "        [ 0.04405025, -0.01272743,  0.02052693, ..., -0.048645  ,\n",
      "          0.04106887, -0.01936203],\n",
      "        [-0.06105886,  0.01201645, -0.05260883, ...,  0.0033813 ,\n",
      "          0.04804138,  0.04342795],\n",
      "        ...,\n",
      "        [-0.0087636 ,  0.03939548, -0.00178818, ...,  0.01665532,\n",
      "          0.03622022, -0.00979152],\n",
      "        [ 0.03690959,  0.0557609 , -0.03029509, ..., -0.05817199,\n",
      "         -0.00685173, -0.02196562],\n",
      "        [-0.01792343, -0.07205   ,  0.07063801, ..., -0.02813378,\n",
      "         -0.06670015, -0.04743821]],\n",
      "\n",
      "       [[-0.02153126,  0.03944509,  0.01317661, ...,  0.03484379,\n",
      "          0.04049966, -0.04871939],\n",
      "        [ 0.04708989,  0.02278079,  0.01243365, ...,  0.04274757,\n",
      "         -0.06908918, -0.06812783],\n",
      "        [ 0.01961221, -0.00657301, -0.05708873, ..., -0.00795326,\n",
      "          0.00309772, -0.06702022],\n",
      "        ...,\n",
      "        [-0.0110185 ,  0.055043  , -0.04438617, ...,  0.03326757,\n",
      "         -0.02900575, -0.00980503],\n",
      "        [-0.01057247, -0.04306386, -0.06588455, ...,  0.06958068,\n",
      "         -0.06823944, -0.04210589],\n",
      "        [-0.06723949, -0.01386299, -0.02827194, ...,  0.03215853,\n",
      "          0.05322354, -0.04729956]]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_6/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'conv1d_7/kernel:0' shape=(5, 128, 128) dtype=float32, numpy=\n",
      "array([[[-0.05715167, -0.01074855,  0.04975273, ...,  0.01901331,\n",
      "          0.0107545 , -0.04655644],\n",
      "        [-0.00207092, -0.05548575, -0.0003511 , ..., -0.0017162 ,\n",
      "         -0.01783624, -0.05179743],\n",
      "        [ 0.02246071, -0.04438809,  0.04773799, ...,  0.01017506,\n",
      "          0.02346501,  0.01158442],\n",
      "        ...,\n",
      "        [ 0.04081019,  0.04248795,  0.06761648, ...,  0.04459646,\n",
      "          0.05922113, -0.02221815],\n",
      "        [ 0.01403442,  0.00980943,  0.02137651, ..., -0.05138215,\n",
      "          0.00371643,  0.05376302],\n",
      "        [ 0.04045881,  0.0523988 ,  0.04739524, ...,  0.00508598,\n",
      "         -0.05360427, -0.02032577]],\n",
      "\n",
      "       [[ 0.05263519,  0.01237524,  0.05913273, ..., -0.06634828,\n",
      "          0.04156925, -0.0142202 ],\n",
      "        [-0.05054285, -0.05437937,  0.06772429, ...,  0.00857067,\n",
      "         -0.04636325,  0.01650532],\n",
      "        [ 0.05263198, -0.05717116, -0.03851094, ...,  0.02535235,\n",
      "         -0.02966562,  0.06842625],\n",
      "        ...,\n",
      "        [-0.03766998, -0.01433099, -0.02921447, ...,  0.04264563,\n",
      "         -0.06755076, -0.04862948],\n",
      "        [ 0.01492146, -0.03028124,  0.03387616, ...,  0.04377531,\n",
      "          0.01221319, -0.01874297],\n",
      "        [ 0.03069419,  0.02869064, -0.00626399, ..., -0.03652698,\n",
      "         -0.04001808, -0.02907011]],\n",
      "\n",
      "       [[-0.0034994 ,  0.02486255, -0.00941204, ..., -0.02362364,\n",
      "          0.0425282 ,  0.02429822],\n",
      "        [ 0.0109371 ,  0.03818762,  0.04992172, ...,  0.01112125,\n",
      "         -0.03623319, -0.02106313],\n",
      "        [ 0.0011516 , -0.02087806,  0.0113342 , ..., -0.0345828 ,\n",
      "          0.06153648, -0.02039577],\n",
      "        ...,\n",
      "        [-0.03019859, -0.0577517 ,  0.01622037, ...,  0.00107366,\n",
      "          0.02000048,  0.03042673],\n",
      "        [ 0.04586585,  0.05386219,  0.05040023, ...,  0.05357598,\n",
      "         -0.06071486,  0.0288121 ],\n",
      "        [ 0.00335252, -0.02974857,  0.03792071, ..., -0.04470503,\n",
      "          0.06831835, -0.00912052]],\n",
      "\n",
      "       [[-0.0062241 ,  0.01744726,  0.0660273 , ...,  0.01943488,\n",
      "          0.03899519,  0.05156188],\n",
      "        [-0.06249758, -0.0273182 , -0.06176218, ...,  0.05548482,\n",
      "         -0.0094622 , -0.06427659],\n",
      "        [ 0.03193948, -0.04264525,  0.0365015 , ..., -0.04556474,\n",
      "          0.02331903, -0.04677013],\n",
      "        ...,\n",
      "        [-0.06386124,  0.06080724, -0.02886029, ...,  0.04163844,\n",
      "          0.04129105, -0.00918262],\n",
      "        [ 0.05109858,  0.06180739, -0.05262209, ..., -0.01301565,\n",
      "         -0.06740577, -0.03826977],\n",
      "        [ 0.02153534,  0.02163974, -0.04151584, ..., -0.04053903,\n",
      "          0.05003342, -0.04266946]],\n",
      "\n",
      "       [[ 0.03390393, -0.04375541, -0.00944759, ..., -0.00886511,\n",
      "          0.06563106, -0.04378133],\n",
      "        [-0.04648297,  0.02156023, -0.02410626, ..., -0.066062  ,\n",
      "         -0.02840904,  0.03048152],\n",
      "        [-0.03624694, -0.0282439 , -0.03008746, ...,  0.06722765,\n",
      "          0.04587647,  0.04888906],\n",
      "        ...,\n",
      "        [-0.04545156, -0.00313266, -0.03808061, ..., -0.05795672,\n",
      "          0.01103382, -0.06471505],\n",
      "        [ 0.05997829, -0.06197677, -0.03190462, ..., -0.02293598,\n",
      "         -0.01739577,  0.03565926],\n",
      "        [-0.01752949,  0.03849643,  0.02477263, ...,  0.05280579,\n",
      "         -0.04561058,  0.0150125 ]]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_7/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'conv1d_8/kernel:0' shape=(5, 128, 128) dtype=float32, numpy=\n",
      "array([[[ 5.8923587e-03, -3.0243497e-02,  1.8099442e-02, ...,\n",
      "         -4.5381874e-02, -5.0360806e-02, -5.5710427e-02],\n",
      "        [-1.6998213e-02, -7.5781718e-03,  1.9420996e-02, ...,\n",
      "          6.0950473e-02, -1.8747006e-02,  6.5301955e-03],\n",
      "        [ 5.2387327e-02, -6.8062037e-02, -3.1608641e-03, ...,\n",
      "         -6.4104795e-03,  5.9086323e-02, -4.7628544e-02],\n",
      "        ...,\n",
      "        [-3.4765296e-02, -5.2559208e-02,  1.2509279e-02, ...,\n",
      "         -1.0396868e-02,  2.3740433e-02,  5.2622624e-02],\n",
      "        [-8.3367862e-03,  1.5283182e-02, -4.9890824e-02, ...,\n",
      "          4.2973645e-02,  4.4721641e-02, -3.7775233e-02],\n",
      "        [-8.0176964e-03, -4.1494243e-02,  3.3883341e-02, ...,\n",
      "         -5.0827753e-02,  1.2191072e-02, -6.6192485e-02]],\n",
      "\n",
      "       [[-3.3955391e-02,  4.5913309e-03, -3.5471052e-02, ...,\n",
      "         -2.8365783e-02, -4.4880092e-02, -6.7110412e-02],\n",
      "        [-4.0604584e-02,  3.6422364e-02,  5.1887617e-02, ...,\n",
      "         -1.5068445e-02, -1.5812661e-02, -3.0813087e-02],\n",
      "        [-4.7486566e-02,  6.1119005e-02,  2.6232727e-02, ...,\n",
      "         -4.6042904e-02,  2.4621308e-02,  5.6284636e-02],\n",
      "        ...,\n",
      "        [-3.6878735e-02, -3.6340486e-02, -2.7542401e-02, ...,\n",
      "          1.0648228e-02, -1.9304484e-02, -4.8088033e-02],\n",
      "        [-6.3634999e-03,  3.8608752e-02,  3.4938112e-02, ...,\n",
      "          5.8668971e-02,  6.1161175e-02, -5.6896485e-02],\n",
      "        [-2.3880675e-03, -1.9613974e-02, -3.4447756e-02, ...,\n",
      "          1.2831986e-03,  1.6724549e-02,  5.3128533e-02]],\n",
      "\n",
      "       [[-1.7050058e-02, -3.9298646e-02, -4.6098847e-02, ...,\n",
      "          5.7123333e-02,  2.1700077e-02,  6.6359147e-02],\n",
      "        [ 5.5680715e-02, -2.6896760e-02, -1.2604445e-02, ...,\n",
      "          5.0458677e-02, -4.2152256e-02, -3.6525413e-02],\n",
      "        [-3.3246122e-02,  3.6155626e-02,  4.4338055e-02, ...,\n",
      "         -4.6739213e-02, -9.5677041e-03,  1.4114782e-02],\n",
      "        ...,\n",
      "        [ 3.1890348e-03,  2.7832955e-02, -4.0237978e-02, ...,\n",
      "         -7.6428130e-03, -6.0801730e-02, -3.8613047e-02],\n",
      "        [ 3.7019081e-02,  3.8692102e-02,  1.5392728e-02, ...,\n",
      "         -2.9109087e-02, -4.6416108e-02,  2.2647224e-02],\n",
      "        [-1.3071474e-02,  4.2761818e-02, -3.0900873e-02, ...,\n",
      "          5.0046094e-02,  4.1668035e-02,  3.4925818e-02]],\n",
      "\n",
      "       [[-5.8607716e-02, -2.8132051e-02, -6.4446405e-02, ...,\n",
      "         -3.5622519e-02,  4.5441240e-02,  5.8432952e-02],\n",
      "        [ 4.3048918e-02,  2.2929929e-02,  6.2271953e-05, ...,\n",
      "         -5.7943091e-02,  4.1223943e-02,  2.4013348e-02],\n",
      "        [ 5.2375965e-02,  5.0727889e-02, -1.4457706e-02, ...,\n",
      "         -3.6747672e-02,  6.7807451e-02,  2.4449587e-02],\n",
      "        ...,\n",
      "        [ 4.1348852e-02,  5.1691197e-02, -6.2281787e-03, ...,\n",
      "          8.3330199e-03, -2.6105125e-02, -2.2055432e-02],\n",
      "        [-1.7959237e-02,  5.5536941e-02,  1.0355860e-03, ...,\n",
      "         -3.3052117e-03, -3.6300756e-02, -4.9940266e-02],\n",
      "        [ 1.1456057e-02,  4.3809846e-02,  5.2081585e-02, ...,\n",
      "         -6.4782552e-02, -9.5576309e-03,  8.6171255e-03]],\n",
      "\n",
      "       [[ 3.9365493e-02, -6.7537025e-02,  8.0136359e-03, ...,\n",
      "         -5.9086382e-02, -2.4618406e-02, -6.1376836e-02],\n",
      "        [-5.4536235e-02, -1.9622087e-02,  2.1171503e-02, ...,\n",
      "         -4.5950972e-02, -1.0306500e-02, -5.5007033e-02],\n",
      "        [-6.3157000e-02,  1.1292174e-02,  1.7298825e-02, ...,\n",
      "         -2.9429972e-02, -4.4828512e-02, -3.4222312e-02],\n",
      "        ...,\n",
      "        [ 4.9021438e-02,  1.4878929e-02, -3.5038758e-02, ...,\n",
      "         -2.1928519e-02, -5.1224951e-02, -9.4722956e-04],\n",
      "        [-4.3845482e-02,  3.4520410e-02, -7.0472807e-04, ...,\n",
      "         -4.3020494e-02, -1.6163990e-02,  4.5843974e-02],\n",
      "        [ 4.5975029e-02,  5.2060187e-04,  6.6240162e-02, ...,\n",
      "          5.9297457e-02, -6.6058554e-02, -6.3130163e-02]]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_8/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'dense_2/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[ 0.12108998, -0.09496045, -0.03890068, ..., -0.11819407,\n",
      "         0.03702019,  0.00362958],\n",
      "       [-0.04715521, -0.14565301, -0.05936579, ...,  0.00674284,\n",
      "         0.089073  , -0.1258237 ],\n",
      "       [ 0.06609829, -0.04054169,  0.11124198, ...,  0.09982289,\n",
      "        -0.08998314, -0.10228631],\n",
      "       ...,\n",
      "       [-0.12643513,  0.06502092, -0.03829529, ..., -0.02489536,\n",
      "         0.06232296,  0.06047127],\n",
      "       [ 0.08427055,  0.14797164, -0.05735426, ..., -0.07321733,\n",
      "        -0.0175073 , -0.07498205],\n",
      "       [ 0.12142275, -0.1445441 ,  0.05845174, ..., -0.08283398,\n",
      "         0.03809655,  0.04541937]], dtype=float32)>\n",
      "<tf.Variable 'dense_2/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'dense_3/kernel:0' shape=(128, 2) dtype=float32, numpy=\n",
      "array([[-0.12313891, -0.03180672],\n",
      "       [ 0.16258521,  0.15698917],\n",
      "       [-0.13426566, -0.0063514 ],\n",
      "       [-0.10245745,  0.02224144],\n",
      "       [-0.11645361, -0.21329744],\n",
      "       [ 0.17604049, -0.10959072],\n",
      "       [-0.04686074,  0.18218787],\n",
      "       [-0.20584962,  0.16022743],\n",
      "       [-0.06258944,  0.07602499],\n",
      "       [-0.1639459 ,  0.17640267],\n",
      "       [ 0.00963587,  0.19967653],\n",
      "       [-0.15438743, -0.1645821 ],\n",
      "       [ 0.20711718, -0.14228356],\n",
      "       [-0.14276144, -0.03041916],\n",
      "       [-0.20417409, -0.13028643],\n",
      "       [ 0.19687809, -0.0308516 ],\n",
      "       [-0.14996623, -0.1021869 ],\n",
      "       [-0.11452757,  0.11326714],\n",
      "       [-0.05799259, -0.19727618],\n",
      "       [-0.09535588, -0.13688025],\n",
      "       [ 0.08715613,  0.0806139 ],\n",
      "       [ 0.1275637 , -0.15043664],\n",
      "       [ 0.12371244,  0.12170823],\n",
      "       [ 0.15439604, -0.17618525],\n",
      "       [ 0.02552554,  0.10907136],\n",
      "       [-0.20630988,  0.11138625],\n",
      "       [ 0.01776446, -0.05547762],\n",
      "       [ 0.11187942, -0.01630908],\n",
      "       [ 0.11484058,  0.03596415],\n",
      "       [ 0.04028197, -0.16664456],\n",
      "       [ 0.10134099,  0.01517746],\n",
      "       [ 0.00029524,  0.20634057],\n",
      "       [ 0.15503256,  0.08178787],\n",
      "       [ 0.1877514 ,  0.05500044],\n",
      "       [ 0.01588036, -0.17358775],\n",
      "       [ 0.0367348 , -0.17421454],\n",
      "       [-0.1536976 , -0.1317834 ],\n",
      "       [ 0.11684267,  0.14308156],\n",
      "       [ 0.19765909, -0.07713899],\n",
      "       [-0.1381703 , -0.06240018],\n",
      "       [ 0.07199584,  0.0525621 ],\n",
      "       [ 0.0870453 ,  0.00603962],\n",
      "       [-0.01265183, -0.1347593 ],\n",
      "       [-0.16651922, -0.19994466],\n",
      "       [ 0.09216206,  0.14572854],\n",
      "       [-0.19819364,  0.00068067],\n",
      "       [-0.07222734,  0.18466513],\n",
      "       [-0.18123892, -0.13409872],\n",
      "       [ 0.08600773, -0.01023397],\n",
      "       [ 0.09658767,  0.11580859],\n",
      "       [-0.01877131,  0.07049204],\n",
      "       [-0.16580752, -0.1630774 ],\n",
      "       [ 0.04023804,  0.05868633],\n",
      "       [ 0.0074912 ,  0.13250203],\n",
      "       [ 0.01651037,  0.05712716],\n",
      "       [ 0.18206675,  0.20512427],\n",
      "       [ 0.21464424, -0.00436665],\n",
      "       [-0.04724264,  0.1155846 ],\n",
      "       [ 0.17717473,  0.18303435],\n",
      "       [ 0.1113313 ,  0.1755472 ],\n",
      "       [-0.03162745, -0.20372063],\n",
      "       [-0.02350669,  0.15473308],\n",
      "       [-0.00469539,  0.04264344],\n",
      "       [ 0.02806818,  0.02481917],\n",
      "       [-0.14797658,  0.13554679],\n",
      "       [-0.16648746,  0.14758427],\n",
      "       [-0.1905985 ,  0.16036122],\n",
      "       [ 0.16745414, -0.14511062],\n",
      "       [-0.0918991 ,  0.14262746],\n",
      "       [-0.18996987, -0.11026745],\n",
      "       [ 0.07460137,  0.14352213],\n",
      "       [ 0.1550384 ,  0.1065623 ],\n",
      "       [ 0.18486519,  0.07357727],\n",
      "       [-0.06965899,  0.19578646],\n",
      "       [-0.07995361,  0.18326233],\n",
      "       [ 0.07184412,  0.1084172 ],\n",
      "       [-0.1255069 ,  0.06175773],\n",
      "       [ 0.15499856,  0.02612734],\n",
      "       [ 0.06773217, -0.14220807],\n",
      "       [-0.06661271, -0.12056211],\n",
      "       [ 0.0866297 , -0.00050503],\n",
      "       [ 0.10884823, -0.14712626],\n",
      "       [ 0.16853173, -0.01551992],\n",
      "       [-0.08162324,  0.14308046],\n",
      "       [-0.04735133, -0.07854064],\n",
      "       [ 0.03991015,  0.03105547],\n",
      "       [-0.02108759,  0.18799628],\n",
      "       [ 0.14263253,  0.13986517],\n",
      "       [-0.15353733, -0.07882993],\n",
      "       [-0.03910635,  0.12090535],\n",
      "       [-0.16082938, -0.11601619],\n",
      "       [-0.20071752, -0.11758128],\n",
      "       [ 0.08705862, -0.14719003],\n",
      "       [-0.03236328, -0.09066699],\n",
      "       [ 0.04165442, -0.00206132],\n",
      "       [ 0.16027133,  0.16630276],\n",
      "       [ 0.10226659,  0.15937169],\n",
      "       [ 0.00770023, -0.17909206],\n",
      "       [ 0.10264118,  0.03615724],\n",
      "       [ 0.20157133,  0.09416904],\n",
      "       [-0.07347344, -0.18307061],\n",
      "       [-0.10008773, -0.04145642],\n",
      "       [-0.12987867,  0.07767715],\n",
      "       [ 0.00951022, -0.0740782 ],\n",
      "       [ 0.17401667,  0.20026211],\n",
      "       [-0.07509662,  0.19120823],\n",
      "       [-0.1925035 , -0.12185794],\n",
      "       [ 0.15791006, -0.18120885],\n",
      "       [-0.02820104, -0.06352718],\n",
      "       [ 0.16079132, -0.07879387],\n",
      "       [ 0.18226148,  0.10120745],\n",
      "       [-0.1579804 , -0.15993944],\n",
      "       [-0.08555949,  0.04267289],\n",
      "       [ 0.20510365, -0.21474314],\n",
      "       [ 0.06204803, -0.2132452 ],\n",
      "       [-0.10537148,  0.11037399],\n",
      "       [ 0.14820193, -0.21175975],\n",
      "       [ 0.16031744,  0.01385628],\n",
      "       [ 0.17500396, -0.1440453 ],\n",
      "       [-0.10459063,  0.13230841],\n",
      "       [ 0.0472642 ,  0.16408695],\n",
      "       [ 0.14536606, -0.19737825],\n",
      "       [ 0.13360818, -0.18668443],\n",
      "       [-0.09300455,  0.086735  ],\n",
      "       [-0.16796768, -0.03027052],\n",
      "       [ 0.08349843, -0.1809819 ],\n",
      "       [ 0.04270123,  0.15356888],\n",
      "       [-0.15549348,  0.18866064]], dtype=float32)>\n",
      "<tf.Variable 'dense_3/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# for w in model.weights:\n",
    "#     print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73a07503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model.load_weights(\"/root/spark/model/aa_weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccba5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check that all of the pretrained weights have been loaded.\n",
    "# for a, b in zip(pretrained.weights, model.weights):\n",
    "#     np.testing.assert_allclose(a.numpy(), b.numpy())\n",
    "\n",
    "# print(\"\\n\", \"-\" * 50)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611e62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81a462b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, None, 100)         2556800   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 2,785,024\n",
      "Trainable params: 228,224\n",
      "Non-trainable params: 2,556,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", trainable=False)(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", trainable=False)(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\", trainable=False)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "# x = layers.Dense(128, activation=\"relu\", trainable=False)(x)\n",
    "# # x = layers.Dropout(0.5, name='dropout_k')(x)\n",
    "# preds = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "197c2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model (Functional)           (None, 128)               2785024   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,801,794\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 2,556,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "pretrained_model = keras.Model(inputs=int_sequences_input, outputs=x)\n",
    "\n",
    "model = keras.Sequential([pretrained_model, layers.Dense(128, activation=\"relu\"), layers.Dense(2, activation=\"softmax\")])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1236b80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 6 layers into a model with 4 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9779b5b515d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/root/spark/model/aa_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2227\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2228\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2229\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    686\u001b[0m                      \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m                      \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m                      ' layers.')\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m   \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 6 layers into a model with 4 layers."
     ]
    }
   ],
   "source": [
    "pretrained_model.load_weights(\"/root/spark/model/aa_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2831231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'embed/embeddings:0' shape=(25568, 100) dtype=float32, numpy=\n",
      "array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ],\n",
      "       [ 0.13277 , -0.11802 , -0.063487, ...,  0.10199 ,  1.0979  ,\n",
      "        -0.17951 ],\n",
      "       [-0.016233,  0.77261 , -0.052126, ...,  0.067194, -0.2468  ,\n",
      "        -0.12894 ],\n",
      "       ...,\n",
      "       [-0.75831 ,  0.42915 , -0.040031, ...,  0.11268 , -0.088691,\n",
      "        -0.20963 ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ],\n",
      "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
      "         0.      ]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_9/kernel:0' shape=(5, 100, 128) dtype=float32, numpy=\n",
      "array([[[-0.06583161, -0.02366187,  0.04817189, ..., -0.00501878,\n",
      "          0.04024842,  0.00257344],\n",
      "        [ 0.03357482, -0.06890798, -0.00362643, ..., -0.03384611,\n",
      "          0.04883148, -0.04939078],\n",
      "        [-0.02353059, -0.00887622, -0.05387938, ...,  0.00444328,\n",
      "          0.00635561, -0.02050303],\n",
      "        ...,\n",
      "        [-0.00875106, -0.00658011, -0.02572839, ...,  0.07028144,\n",
      "         -0.0271306 ,  0.02098783],\n",
      "        [-0.06065533, -0.02333001, -0.0217555 , ...,  0.05053586,\n",
      "         -0.05974485,  0.07111192],\n",
      "        [-0.04578236,  0.01422475, -0.03596873, ...,  0.01791084,\n",
      "          0.03895652, -0.06050066]],\n",
      "\n",
      "       [[-0.04665013,  0.07090106,  0.00279276, ..., -0.0018747 ,\n",
      "          0.03948583, -0.0217962 ],\n",
      "        [ 0.03474376,  0.02754667,  0.06340721, ..., -0.06395525,\n",
      "         -0.01669132,  0.01759432],\n",
      "        [-0.02251427, -0.07200684, -0.00381892, ...,  0.04216986,\n",
      "          0.04372454, -0.03840919],\n",
      "        ...,\n",
      "        [-0.07037914, -0.00149156, -0.05762697, ..., -0.05044438,\n",
      "          0.07029665, -0.0652928 ],\n",
      "        [ 0.0027409 ,  0.0224432 , -0.05746558, ...,  0.04019573,\n",
      "         -0.01605585,  0.00663062],\n",
      "        [-0.06656454,  0.06350552, -0.05766341, ...,  0.03706148,\n",
      "         -0.01997655, -0.04250579]],\n",
      "\n",
      "       [[ 0.06730165, -0.03115724, -0.04920463, ..., -0.04607897,\n",
      "         -0.01518002, -0.04116653],\n",
      "        [ 0.0253171 ,  0.03928527,  0.00430068, ...,  0.04316464,\n",
      "         -0.02477232, -0.02843173],\n",
      "        [-0.04165634, -0.03529247,  0.03169487, ..., -0.02663469,\n",
      "         -0.03077183, -0.06086363],\n",
      "        ...,\n",
      "        [-0.00149129,  0.04796633,  0.00500675, ...,  0.03819688,\n",
      "         -0.032324  , -0.00224601],\n",
      "        [-0.06514346, -0.02375987, -0.00311638, ...,  0.06829708,\n",
      "         -0.02790347,  0.05980866],\n",
      "        [ 0.01778592, -0.01854108,  0.06150157, ..., -0.04771705,\n",
      "         -0.07112081, -0.03355219]],\n",
      "\n",
      "       [[ 0.05023194, -0.03459503, -0.06457953, ...,  0.06452643,\n",
      "         -0.04666724,  0.03672609],\n",
      "        [ 0.06023028,  0.03288282, -0.02024933, ..., -0.01261974,\n",
      "         -0.04337461,  0.02909713],\n",
      "        [ 0.04128893, -0.05571987,  0.07002046, ...,  0.06992429,\n",
      "          0.02796853,  0.01668341],\n",
      "        ...,\n",
      "        [ 0.05465967,  0.02444365,  0.04390527, ...,  0.038193  ,\n",
      "          0.056744  ,  0.01872101],\n",
      "        [ 0.04608972,  0.04184357,  0.05122994, ..., -0.05835042,\n",
      "         -0.00913318,  0.05223937],\n",
      "        [ 0.04560794, -0.01592486, -0.04015533, ..., -0.03421424,\n",
      "         -0.05756973, -0.01866783]],\n",
      "\n",
      "       [[-0.06158585, -0.02808229, -0.04821352, ...,  0.01803053,\n",
      "          0.00932882, -0.04339825],\n",
      "        [ 0.0621687 , -0.0531864 , -0.02476076, ..., -0.05408009,\n",
      "         -0.00993083,  0.00870581],\n",
      "        [ 0.04918563,  0.05649749,  0.01644062, ...,  0.03674359,\n",
      "          0.04891225,  0.03703206],\n",
      "        ...,\n",
      "        [ 0.02989612,  0.04854511,  0.05300992, ...,  0.02577815,\n",
      "          0.0086647 , -0.01379614],\n",
      "        [ 0.01357786, -0.02013957, -0.06627727, ..., -0.06410667,\n",
      "         -0.06494138,  0.00424309],\n",
      "        [-0.06285697, -0.05241032,  0.05910953, ...,  0.03432146,\n",
      "          0.06665332,  0.02656227]]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_9/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'conv1d_10/kernel:0' shape=(5, 128, 128) dtype=float32, numpy=\n",
      "array([[[ 5.3018175e-02,  1.8055141e-02, -8.0285221e-04, ...,\n",
      "         -2.4741516e-03, -4.1781031e-02, -3.2513645e-02],\n",
      "        [ 4.1028880e-02, -1.1215158e-02,  2.2803813e-02, ...,\n",
      "          6.2048554e-02,  2.7663715e-02,  4.0833436e-02],\n",
      "        [ 4.9388506e-02,  2.4561226e-02, -5.2466393e-03, ...,\n",
      "          1.6011268e-02,  6.0829341e-02, -3.4681603e-02],\n",
      "        ...,\n",
      "        [ 6.1946139e-02, -5.9947573e-02,  8.7203681e-03, ...,\n",
      "          3.2634683e-02, -6.2635206e-03,  6.4735502e-02],\n",
      "        [-4.9954221e-02, -2.4764173e-02,  1.8171348e-02, ...,\n",
      "          6.0550004e-02,  5.0185896e-02, -5.9793450e-02],\n",
      "        [-3.6488995e-03,  5.5654846e-02, -1.8062845e-02, ...,\n",
      "          6.4868674e-02, -5.2301072e-02, -3.5230022e-02]],\n",
      "\n",
      "       [[ 6.1332062e-02, -1.6591124e-02,  3.8214818e-02, ...,\n",
      "         -3.6670707e-02,  6.7267880e-02, -1.0622162e-02],\n",
      "        [ 5.8172897e-02,  3.8136043e-02,  2.4005741e-02, ...,\n",
      "          2.4572194e-02, -1.3238836e-02,  1.7421447e-02],\n",
      "        [-3.4166127e-02, -2.2315986e-02, -4.6059117e-03, ...,\n",
      "         -4.2865947e-03,  2.0461552e-02, -4.7536433e-02],\n",
      "        ...,\n",
      "        [ 1.4415473e-02,  4.1339368e-02,  4.2751960e-02, ...,\n",
      "         -4.2267203e-02, -5.7084024e-02, -6.8296880e-02],\n",
      "        [ 5.8858678e-02, -4.0483847e-03, -4.3330837e-02, ...,\n",
      "         -2.8674852e-02, -2.6021533e-02,  1.1707440e-02],\n",
      "        [-2.2937827e-02,  1.4474958e-02, -4.7324002e-02, ...,\n",
      "          1.8729784e-02,  1.8874429e-02,  5.0737552e-02]],\n",
      "\n",
      "       [[-3.3308987e-02,  1.9962318e-02, -3.1492613e-02, ...,\n",
      "          5.1104009e-02, -5.0831214e-03, -2.7801841e-04],\n",
      "        [-6.4651035e-02, -1.3419896e-02, -5.3298023e-02, ...,\n",
      "          5.3698599e-02,  4.0798344e-02, -3.1536803e-02],\n",
      "        [-6.0141608e-03, -4.1328855e-02,  1.7428324e-03, ...,\n",
      "          6.0496941e-02,  3.3242337e-02,  5.6984290e-02],\n",
      "        ...,\n",
      "        [-2.3567781e-02, -2.5397062e-02,  3.7164472e-02, ...,\n",
      "          1.2442693e-03, -6.3581914e-02,  9.7079501e-03],\n",
      "        [ 6.1481655e-02, -3.0717187e-02,  2.6642226e-02, ...,\n",
      "          5.6085408e-02, -5.2672736e-02,  5.6547031e-02],\n",
      "        [-6.8115778e-03,  4.8143826e-02, -2.1319538e-03, ...,\n",
      "          4.1136444e-03, -4.2515770e-03,  4.7024027e-02]],\n",
      "\n",
      "       [[ 5.9044465e-02, -3.9644815e-02, -5.2393738e-02, ...,\n",
      "         -6.9349185e-03,  3.5398394e-02,  6.2647372e-02],\n",
      "        [-6.9767870e-03,  5.9370294e-02, -3.7167527e-02, ...,\n",
      "          1.6622730e-02, -3.2156423e-02,  6.8215668e-02],\n",
      "        [-4.2360902e-02, -1.6469579e-02,  1.8328428e-03, ...,\n",
      "         -2.2764638e-02, -2.9989261e-02,  8.6868405e-03],\n",
      "        ...,\n",
      "        [ 4.9079008e-02,  2.6060559e-02, -8.0647580e-03, ...,\n",
      "          6.1472028e-02,  5.4986551e-03, -4.5197468e-02],\n",
      "        [-6.1502360e-02,  4.3902405e-02, -6.1288297e-02, ...,\n",
      "          6.0370684e-02,  8.4504485e-05,  6.5716878e-03],\n",
      "        [-4.2861030e-03,  4.9628861e-02, -8.1430785e-03, ...,\n",
      "          6.5171525e-02,  4.9955577e-02,  5.0871104e-02]],\n",
      "\n",
      "       [[-2.2173502e-02, -2.5913291e-02,  5.6369156e-02, ...,\n",
      "          3.0866124e-02,  1.3598442e-02,  1.8382289e-02],\n",
      "        [ 5.9728876e-02, -2.9749721e-03,  6.7667037e-02, ...,\n",
      "         -4.9815163e-02, -6.7809075e-02, -4.2854518e-02],\n",
      "        [-3.0095559e-02, -3.6241271e-02, -4.1427709e-02, ...,\n",
      "         -4.1099820e-02,  5.3427905e-02, -6.7386068e-02],\n",
      "        ...,\n",
      "        [ 5.5012614e-02, -5.9002515e-02,  3.5287336e-02, ...,\n",
      "         -1.9222915e-02, -3.9052963e-02,  3.6397718e-02],\n",
      "        [-5.2172489e-02, -4.9784932e-02,  1.0650091e-02, ...,\n",
      "         -7.1127750e-03,  1.5991844e-02, -4.7238220e-02],\n",
      "        [ 4.0828735e-02, -4.5659356e-02,  2.3148872e-02, ...,\n",
      "         -3.4287572e-04,  4.3899968e-02, -1.0242268e-02]]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_10/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'conv1d_11/kernel:0' shape=(5, 128, 128) dtype=float32, numpy=\n",
      "array([[[ 4.5209415e-02, -4.8990034e-02,  6.2613934e-02, ...,\n",
      "          2.1088712e-02, -3.5763960e-02, -4.1791279e-02],\n",
      "        [ 2.0381406e-02,  3.8302153e-02,  5.1222533e-02, ...,\n",
      "          3.4721047e-03,  6.5437615e-02, -2.1368790e-02],\n",
      "        [ 3.3542819e-02,  4.8448436e-02,  5.0255276e-02, ...,\n",
      "          5.4918140e-02,  6.6093937e-02, -5.4016024e-02],\n",
      "        ...,\n",
      "        [ 3.1076208e-02, -4.0801641e-02, -5.2977953e-02, ...,\n",
      "          2.7203053e-02, -4.1196913e-02,  3.1914040e-03],\n",
      "        [ 2.9768668e-02, -4.5366690e-02, -5.4694016e-02, ...,\n",
      "         -6.3969448e-02, -2.9001988e-02, -1.6199283e-02],\n",
      "        [ 2.8192900e-02,  1.4283434e-02,  1.9063629e-02, ...,\n",
      "         -1.4679182e-02,  6.4557418e-02,  4.2377599e-02]],\n",
      "\n",
      "       [[-2.3654550e-03, -7.6867640e-05, -4.7406532e-02, ...,\n",
      "          6.3484058e-02, -5.2709430e-03, -5.2968927e-02],\n",
      "        [ 3.5621747e-02, -5.5901967e-02, -1.9583140e-02, ...,\n",
      "         -3.2451045e-02,  1.0040835e-02,  2.9840685e-02],\n",
      "        [ 2.8927036e-02,  3.1936392e-03, -3.0615393e-02, ...,\n",
      "         -1.8860307e-02, -2.1335818e-02,  6.6515341e-02],\n",
      "        ...,\n",
      "        [-2.4597272e-03, -5.9184283e-03,  1.5791766e-02, ...,\n",
      "          5.0586328e-02,  5.3137109e-02,  1.5658125e-02],\n",
      "        [-8.4970519e-03, -5.0476067e-02,  1.3431467e-02, ...,\n",
      "         -1.8369555e-03,  2.2210084e-02, -3.5231475e-02],\n",
      "        [-6.2886946e-02, -4.1378118e-02,  6.6870362e-02, ...,\n",
      "          2.8217472e-02,  5.7727113e-02,  2.4999067e-02]],\n",
      "\n",
      "       [[-6.2161140e-02,  5.6700781e-02,  3.9624430e-02, ...,\n",
      "         -4.0169533e-02,  2.7642198e-02,  5.2742667e-02],\n",
      "        [-1.7928209e-02,  4.9628899e-02, -4.5137770e-02, ...,\n",
      "         -1.7051771e-02,  4.9469091e-02, -6.7749478e-02],\n",
      "        [ 4.3185867e-02, -5.5195536e-02, -2.2152998e-02, ...,\n",
      "         -6.3831665e-02,  5.3891741e-02, -1.3260238e-02],\n",
      "        ...,\n",
      "        [-2.9109184e-02,  3.3904411e-02, -6.4620785e-03, ...,\n",
      "          6.8430364e-02, -5.1194325e-02, -3.8980909e-02],\n",
      "        [ 5.2101843e-02,  3.3480056e-02,  1.9779429e-03, ...,\n",
      "          4.0381312e-02,  5.8155671e-02,  3.6519371e-02],\n",
      "        [ 1.4615454e-02,  5.9979394e-02,  2.9178068e-02, ...,\n",
      "          4.5519806e-02, -2.3684558e-02, -1.9594453e-02]],\n",
      "\n",
      "       [[ 5.5688389e-02,  7.7872127e-03,  6.7152753e-02, ...,\n",
      "          2.9901475e-02,  4.6954304e-02, -5.0044782e-02],\n",
      "        [ 9.2891306e-03, -3.5431467e-02,  7.3434934e-03, ...,\n",
      "         -3.9903592e-02,  5.1209375e-02, -5.6071974e-02],\n",
      "        [ 6.1853006e-03, -6.3635066e-02, -1.7843261e-02, ...,\n",
      "          1.9764185e-02,  6.7423955e-03, -6.7628749e-02],\n",
      "        ...,\n",
      "        [ 3.1642869e-02, -3.8210578e-02,  7.4745342e-03, ...,\n",
      "         -4.0447209e-02, -2.4873979e-02,  1.9017451e-02],\n",
      "        [ 3.4755401e-02, -1.0904223e-03, -2.6310880e-02, ...,\n",
      "          5.0155535e-02,  6.4598799e-02,  5.6007721e-02],\n",
      "        [-1.2079548e-02,  5.8197767e-02,  3.4340575e-02, ...,\n",
      "         -1.4025047e-03, -3.6347732e-02,  3.0661069e-02]],\n",
      "\n",
      "       [[-1.4702305e-03, -2.0408858e-02,  2.0338945e-02, ...,\n",
      "         -9.0736263e-03,  5.5109598e-02, -7.2603412e-03],\n",
      "        [ 3.5800785e-02,  5.1446505e-02, -5.4151166e-02, ...,\n",
      "          6.0987934e-02, -1.6239259e-02,  2.5322713e-02],\n",
      "        [-5.9827335e-02, -5.8684126e-03,  1.7312601e-02, ...,\n",
      "         -2.1610036e-03, -5.6570716e-02, -5.0745875e-02],\n",
      "        ...,\n",
      "        [ 5.4488651e-02, -6.4285912e-02, -1.3022419e-02, ...,\n",
      "          3.2443009e-02,  5.6099184e-02,  3.7543289e-02],\n",
      "        [ 2.3121499e-02, -4.5863219e-02, -3.0652516e-02, ...,\n",
      "         -5.2976582e-02,  6.1853155e-02,  2.7904630e-02],\n",
      "        [ 2.8486155e-02,  4.1614100e-03,  9.0673715e-03, ...,\n",
      "          3.1317249e-03,  1.1891343e-02, -2.5003798e-02]]], dtype=float32)>\n",
      "<tf.Variable 'conv1d_11/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'dense_4/kernel:0' shape=(128, 128) dtype=float32, numpy=\n",
      "array([[ 0.12490045,  0.0166128 ,  0.13111894, ..., -0.01292682,\n",
      "        -0.05579256,  0.13492341],\n",
      "       [ 0.1104552 , -0.1215672 ,  0.04410872, ...,  0.02536653,\n",
      "        -0.00734948,  0.04166166],\n",
      "       [-0.05024324, -0.07328124, -0.00574084, ...,  0.09426546,\n",
      "         0.11774124,  0.01686165],\n",
      "       ...,\n",
      "       [-0.11852552,  0.14789946, -0.00941363, ...,  0.01392974,\n",
      "        -0.07650673, -0.12335717],\n",
      "       [-0.04051228, -0.14320582,  0.07449003, ..., -0.0893465 ,\n",
      "         0.11677973,  0.14213972],\n",
      "       [-0.12086117, -0.03583553,  0.05311304, ..., -0.01493612,\n",
      "        -0.08032966,  0.09026597]], dtype=float32)>\n",
      "<tf.Variable 'dense_4/bias:0' shape=(128,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>\n",
      "<tf.Variable 'dense_5/kernel:0' shape=(128, 2) dtype=float32, numpy=\n",
      "array([[-6.38476163e-02,  4.39119339e-03],\n",
      "       [ 1.88136682e-01,  3.60789150e-02],\n",
      "       [-1.81900799e-01,  9.59488750e-03],\n",
      "       [-2.00771004e-01, -1.13327727e-01],\n",
      "       [-1.33310184e-01,  7.30472952e-02],\n",
      "       [ 1.27005562e-01,  1.85267821e-01],\n",
      "       [ 7.23960251e-02,  1.97470769e-01],\n",
      "       [-3.68633121e-02, -2.01807141e-01],\n",
      "       [-5.79131544e-02,  2.24546641e-02],\n",
      "       [ 2.08296224e-01,  7.95052201e-02],\n",
      "       [ 6.39252216e-02, -2.07247227e-01],\n",
      "       [ 7.44541734e-02, -1.08843215e-01],\n",
      "       [-1.96827114e-01,  2.14198872e-01],\n",
      "       [ 1.98079497e-02, -4.36217636e-02],\n",
      "       [-7.29384422e-02, -3.17605138e-02],\n",
      "       [ 1.42171845e-01, -9.02399644e-02],\n",
      "       [ 1.20744571e-01,  1.27546683e-01],\n",
      "       [ 1.93042025e-01,  3.66971046e-02],\n",
      "       [ 1.40019104e-01,  1.53343663e-01],\n",
      "       [ 1.60149634e-02, -2.01024026e-01],\n",
      "       [-1.28032565e-01, -1.90354541e-01],\n",
      "       [ 7.00632781e-02,  7.79383332e-02],\n",
      "       [ 1.00726143e-01, -1.38471633e-01],\n",
      "       [-2.46791244e-02, -1.95386231e-01],\n",
      "       [-3.73336673e-02, -2.42709965e-02],\n",
      "       [ 8.71615559e-02, -1.80727020e-01],\n",
      "       [-8.90371054e-02, -1.66867569e-01],\n",
      "       [-1.61207289e-01, -1.32012367e-01],\n",
      "       [-4.33579236e-02, -1.09913617e-01],\n",
      "       [-3.43847573e-02,  5.52490354e-03],\n",
      "       [-1.44563854e-01, -1.23371780e-02],\n",
      "       [ 6.38590902e-02, -6.08562827e-02],\n",
      "       [-1.97141767e-01,  1.92729533e-02],\n",
      "       [-2.06328228e-01,  1.52674213e-01],\n",
      "       [ 1.00322679e-01,  9.15250033e-02],\n",
      "       [-7.78855830e-02, -1.92292631e-02],\n",
      "       [-1.19605824e-01,  1.37098297e-01],\n",
      "       [ 4.89154905e-02, -3.75207812e-02],\n",
      "       [-3.94791514e-02, -1.16179429e-01],\n",
      "       [ 1.65325925e-01,  1.02398098e-02],\n",
      "       [-1.23829417e-01, -1.21986270e-02],\n",
      "       [-9.68927443e-02, -1.46412596e-01],\n",
      "       [ 1.27569631e-01, -5.38500845e-02],\n",
      "       [ 1.14959612e-01, -2.44773179e-02],\n",
      "       [ 7.36761838e-02,  4.61628586e-02],\n",
      "       [ 2.03208730e-01, -6.07791990e-02],\n",
      "       [ 3.92463952e-02, -1.01533994e-01],\n",
      "       [-3.63200903e-04,  1.27056167e-01],\n",
      "       [-1.56201810e-01, -1.83123678e-01],\n",
      "       [ 1.83494613e-01,  1.24201223e-01],\n",
      "       [-2.07058981e-01, -1.08992472e-01],\n",
      "       [-1.66115597e-01,  6.87639564e-02],\n",
      "       [ 1.26256511e-01, -8.41221362e-02],\n",
      "       [ 1.12674460e-01,  1.82503805e-01],\n",
      "       [-1.71018019e-01,  1.62845120e-01],\n",
      "       [-1.54096708e-01,  1.23947725e-01],\n",
      "       [-1.66199505e-01,  1.01373985e-01],\n",
      "       [-8.54661614e-02, -2.57586986e-02],\n",
      "       [ 2.38077641e-02, -9.54115018e-02],\n",
      "       [-6.58851266e-02,  1.01505414e-01],\n",
      "       [-2.27119476e-02, -1.88741341e-01],\n",
      "       [ 1.78916290e-01, -1.17732435e-02],\n",
      "       [ 1.65343285e-04,  7.69605786e-02],\n",
      "       [-7.01130629e-02, -1.43452913e-01],\n",
      "       [ 1.34426549e-01, -1.09505035e-01],\n",
      "       [ 1.91828266e-01,  1.20744243e-01],\n",
      "       [-6.12085760e-02,  1.74210533e-01],\n",
      "       [ 2.04304650e-01,  1.11447617e-01],\n",
      "       [-2.05980644e-01,  9.96043235e-02],\n",
      "       [ 1.39063433e-01, -1.65243596e-02],\n",
      "       [ 1.59686461e-01, -1.28488079e-01],\n",
      "       [ 6.81243837e-03, -1.45630822e-01],\n",
      "       [ 1.94982454e-01,  5.18120676e-02],\n",
      "       [ 1.68668583e-01,  3.86877805e-02],\n",
      "       [ 9.66630727e-02,  1.20269492e-01],\n",
      "       [ 1.64131328e-01,  1.64542064e-01],\n",
      "       [-2.14592904e-01, -1.13086373e-01],\n",
      "       [ 8.64192843e-03,  2.11911991e-01],\n",
      "       [ 1.93864539e-01,  3.75758857e-02],\n",
      "       [ 3.33800614e-02,  1.78588167e-01],\n",
      "       [-6.60462528e-02,  1.53944120e-01],\n",
      "       [ 9.96675640e-02,  1.62081227e-01],\n",
      "       [-1.45078003e-02,  8.70294720e-02],\n",
      "       [-1.39313191e-02,  8.82190019e-02],\n",
      "       [-7.14178085e-02,  1.29241273e-01],\n",
      "       [-1.10355906e-01, -7.16836452e-02],\n",
      "       [-1.91986173e-01,  2.01509550e-01],\n",
      "       [ 9.24188346e-02,  2.08582297e-01],\n",
      "       [ 1.20472625e-01,  1.82509437e-01],\n",
      "       [-1.97995305e-01,  1.85252205e-01],\n",
      "       [-6.08800501e-02,  2.04352334e-01],\n",
      "       [ 1.97473332e-01, -1.89379811e-01],\n",
      "       [ 7.63742179e-02,  1.12148002e-01],\n",
      "       [-7.88642913e-02, -1.40878111e-01],\n",
      "       [ 2.99073011e-02,  9.54174548e-02],\n",
      "       [ 1.58953473e-01, -4.71505523e-03],\n",
      "       [ 5.32319695e-02, -1.89802840e-01],\n",
      "       [ 9.42114145e-02,  8.81524831e-02],\n",
      "       [-1.40792429e-02, -2.11877227e-02],\n",
      "       [ 1.74407646e-01,  1.91994116e-01],\n",
      "       [ 8.63129050e-02,  7.83149451e-02],\n",
      "       [-2.04867974e-01,  1.27142683e-01],\n",
      "       [-1.59927398e-01, -2.10317671e-02],\n",
      "       [ 7.19811171e-02, -7.08248317e-02],\n",
      "       [-1.42401427e-01, -2.03694403e-03],\n",
      "       [-1.53832197e-01,  8.68323594e-02],\n",
      "       [ 2.05643818e-01,  1.76852897e-01],\n",
      "       [ 1.27606317e-01,  4.63426858e-02],\n",
      "       [ 1.81330606e-01, -1.33194685e-01],\n",
      "       [-1.65878192e-01,  1.18869469e-01],\n",
      "       [-2.03369364e-01, -4.66951728e-03],\n",
      "       [-6.54500723e-03,  8.35267454e-02],\n",
      "       [-1.16398193e-01, -1.52205586e-01],\n",
      "       [-1.56866297e-01, -1.00486375e-01],\n",
      "       [-6.15808368e-03,  8.73935968e-02],\n",
      "       [-1.36611313e-02,  1.32703528e-01],\n",
      "       [ 1.24841377e-01, -1.96964294e-01],\n",
      "       [-1.47497088e-01, -3.05737853e-02],\n",
      "       [ 1.96391687e-01, -1.82643652e-01],\n",
      "       [-1.13366552e-01,  1.74998119e-01],\n",
      "       [-9.36907977e-02,  1.82502270e-02],\n",
      "       [ 8.23422223e-02, -1.55621946e-01],\n",
      "       [-1.99418619e-01,  1.98223397e-01],\n",
      "       [ 2.03308359e-01, -3.63319516e-02],\n",
      "       [-7.09317625e-03, -8.60697478e-02],\n",
      "       [ 1.55220672e-01,  1.94906160e-01],\n",
      "       [-7.40693361e-02,  1.93827763e-01],\n",
      "       [ 1.15190014e-01, -1.94174513e-01]], dtype=float32)>\n",
      "<tf.Variable 'dense_5/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "for w in model.weights:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe52af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a11ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8d982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2509a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/root/spark/model/keras_model_1_weights.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c85b385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embed (Embedding)            (None, None, 100)         2556800   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "=================================================================\n",
      "Total params: 2,785,024\n",
      "Trainable params: 228,224\n",
      "Non-trainable params: 2,556,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1de7183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "new_model = models.Sequential()\n",
    "new_model.add(model)\n",
    "new_model.add(layers.Dense(128, activation=\"relu\"))\n",
    "# new_model.add(layers.Dropout(0.5))\n",
    "new_model.add(layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "750422f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_1 (Functional)         (None, 128)               2785024   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,801,794\n",
      "Trainable params: 244,994\n",
      "Non-trainable params: 2,556,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e056f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelÎ•º ÎèôÍ≤∞ÌïòÍ∏∞ Ï†Ñ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò: 10\n",
      "modelÎ•º ÎèôÍ≤∞Ìïú ÌõÑ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò: 4\n"
     ]
    }
   ],
   "source": [
    "print('modelÎ•º ÎèôÍ≤∞ÌïòÍ∏∞ Ï†Ñ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò:',\n",
    "     len(new_model.trainable_weights))\n",
    "#model.trainable = False\n",
    "print('modelÎ•º ÎèôÍ≤∞Ìïú ÌõÑ ÌõàÎ†®ÎêòÎäî Í∞ÄÏ§ëÏπòÏùò Ïàò:',\n",
    "     len(new_model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0d8b203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6074ae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, regularizers\n",
    "\n",
    "optimizer_conf = optimizers.SGD(lr=0.0001)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6357c192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_7e53284c5961"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "from elephas import spark_model, ml_model\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"feature\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_categorical_labels(True) # one-hot encoding Ïó¨Î∂Ä\n",
    "estimator.set_nb_classes(2)\n",
    "estimator.set_num_workers(3)\n",
    "estimator.set_epochs(100)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2dbabcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_trainable = False\n",
    "# for layer in model.layers:\n",
    "#     if layer.name == \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "089c7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline = Pipeline(stages=[estimator])\n",
    "# e1ÏóêÏÑú Í∑∏ÎåÄÎ°ú Í∞ÄÏ†∏Ïò§ÎäîÏßÄÎäî ÌôïÏù∏Ìï¥Î≥º ÌïÑÏöîÏÑ±Ïù¥ ÏûàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f3b88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      ">>> Synchronous training complete.\n",
      "747.9298045635223\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "new_dl = new_pipeline.fit(df_with_vectors)\n",
    "t2 = time.time() - t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb536245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_2_ckpt/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model_2_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9070324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"pretrained_2_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f559bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('/root/spark/model/keras_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "514e2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_weights('/root/spark/model/keras_model_2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a008f6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElephasTransformer_affcc2c7c67e\n"
     ]
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasTransformer\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "import h5py\n",
    "from elephas.ml_model import load_ml_transformer\n",
    "\n",
    "cs_model = new_dl.stages[0]\n",
    "print(cs_model)\n",
    "\n",
    "cs_model.save(\"/root/spark/model/estimator2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0864805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSequence2(Transformer):\n",
    "    def __init__(self):\n",
    "        super(TextToSequence2, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        # 200Ïùò Í∏∏Ïù¥Î°ú ÏÉÅÏúÑ 2ÎßåÍ∞úÏùò ÌÜ†ÌÅ∞Îßå Î∞òÏòÅ\n",
    "        #vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "        \n",
    "        x_test = TextToSequence.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, labels, x_test.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text','label','feature'])\n",
    "        \n",
    "        return fdf\n",
    "    \n",
    "# get_tokens Ìï®ÏàòÍ≥º Í∞ôÏùÄ Ïö©ÎèÑ\n",
    "stage_5 = TextToSequence2()\n",
    "stage_6 = StringIndexer(inputCol='label', outputCol='label_index')\n",
    "test_pipeline = Pipeline(stages=[stage_5, stage_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e5361d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing2 = test_pipeline.fit(second_df)\n",
    "preprocessed2 = preprocessing2.transform(second_df)\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_test = preprocessed2.select(\n",
    "    preprocessed2[\"label_index\"], \n",
    "    list_to_vector_udf(preprocessed2[\"feature\"]).alias(\"feature\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23413f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = new_dl.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0261e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test2 = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa09e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test3 = pred_test2.select(\n",
    "    pred_test2[\"label_index\"], \n",
    "    list_to_vector_udf(pred_test2[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b8c2dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|prediction                               |\n",
      "+-----------------------------------------+\n",
      "|[0.46497347950935364, 0.5350266098976135]|\n",
      "|[0.5289415121078491, 0.47105854749679565]|\n",
      "|[0.4707801342010498, 0.5292198657989502] |\n",
      "|[0.4707200825214386, 0.5292799472808838] |\n",
      "|[0.5110920667648315, 0.48890793323516846]|\n",
      "|[0.44770410656929016, 0.5522958636283875]|\n",
      "|[0.4583183825016022, 0.5416815876960754] |\n",
      "|[0.4523548185825348, 0.5476451516151428] |\n",
      "|[0.4553672969341278, 0.5446327328681946] |\n",
      "|[0.48225489258766174, 0.5177451372146606]|\n",
      "|[0.4694705903530121, 0.5305294394493103] |\n",
      "|[0.4483574330806732, 0.5516425967216492] |\n",
      "|[0.43663471937179565, 0.5633652806282043]|\n",
      "|[0.49765413999557495, 0.502345860004425] |\n",
      "|[0.4503838121891022, 0.5496161580085754] |\n",
      "|[0.46125563979148865, 0.5387443900108337]|\n",
      "|[0.48579877614974976, 0.5142012238502502]|\n",
      "|[0.5275406241416931, 0.4724594056606293] |\n",
      "|[0.4848715662956238, 0.5151284337043762] |\n",
      "|[0.46705684065818787, 0.5329431295394897]|\n",
      "+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test.select('prediction').show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eec9cdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|label_index|rawPrediction|\n",
      "+-----------+-------------+\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          0.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          0.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test3.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4eb2aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6071\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"rawPrediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pred_test3)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "92bd5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_(text):\n",
    "    # ÏòÅÏñ¥, Ïà´Ïûê, ÌäπÏàòÎßåÎ¨∏Ïûê Ï†úÏô∏ ÏÇ≠Ï†ú.\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URLÏ†úÍ±∞\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URLÏ†úÍ±∞\n",
    "    text = re.sub(pattern, '', text)\n",
    "    only_english = re.sub('[^ a-zA-Z]', '', text)\n",
    "    pic_str = 'pic.twitter.com/'\n",
    "    only_english = only_english.lower()\n",
    "    \n",
    "    return only_english\n",
    "#     if bool(only_english and only_english.strip()) and len(only_english) >= 10:\n",
    "#         return only_english\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e5d317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_nlkt = udf(regex_, StringType())\n",
    "new_df = df2.withColumn(\"text\", udf_nlkt(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5aeb63f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @nanox_finance...|\n",
      "|*/inamin HAHAHAHA...|\n",
      "|4.  hai yang mau ...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2d32d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|rt nanoxfinance a...|\n",
      "|inamin hahahahaha...|\n",
      "|  hai yang mau se...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a92e5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSequence3(Transformer):\n",
    "    def __init__(self):\n",
    "        super(TextToSequence3, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        x_test = TextToSequence.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, x_test.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text', 'feature'])\n",
    "        \n",
    "        return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63a874c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_t = TextToSequence3()\n",
    "test_pipeline = Pipeline(stages=[stage_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19732379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|             feature|\n",
      "+--------------------+--------------------+\n",
      "|rt nanoxfinance a...|[64, 1, 1, 141, 1...|\n",
      "|inamin hahahahaha...|[1, 1, 0, 0, 0, 0...|\n",
      "|  hai yang mau se...|[12344, 12604, 1,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing2 = test_pipeline.fit(new_df)\n",
    "preprocessed2 = preprocessing2.transform(new_df)\n",
    "preprocessed2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "366f344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "df_test = preprocessed2.select(\n",
    "    preprocessed2[\"text\"], \n",
    "    list_to_vector_udf(preprocessed2[\"feature\"]).alias(\"feature\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a03378d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = new_dl.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "004d353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|prediction                               |\n",
      "+-----------------------------------------+\n",
      "|[0.42767348885536194, 0.5723265409469604]|\n",
      "|[0.05208161845803261, 0.9479184150695801]|\n",
      "|[0.02548149600625038, 0.9745185375213623]|\n",
      "+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test.select('prediction').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eeb9399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test2 = pred_test.select(\n",
    "    pred_test[\"text\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7b20a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l>=0.9 else (0.0 if l< 0.1 else None) , DoubleType())\n",
    "pred_test3 = pred_test2.select(\n",
    "    pred_test2[\"text\"], \n",
    "    list_to_vector_udf(pred_test2[\"prediction2\"]).alias(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "82d89772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                       |label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|rt nanoxfinance airdrop alert  nanoxfinance have started it last community airdropno referral needed  guaranteed for all pa|null |\n",
      "|inamin hahahahahahahahahahahaha                                                                                            |0.0  |\n",
      "|  hai yang mau sewa zoom unlock cheggcek turnitin bisa banget loh sama aku sudah ada  testi ya bisa cek                    |0.0  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test3.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53cdec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e843be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred_test3.filter((col('label') == 0.0) | (col('label') == 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b18fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  1.0|111107|\n",
      "|  0.0|421952|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7e23e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(\"/root/spark/df2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_(text):\n",
    "    # ÏòÅÏñ¥, Ïà´Ïûê, ÌäπÏàòÎßåÎ¨∏Ïûê Ï†úÏô∏ ÏÇ≠Ï†ú.\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URLÏ†úÍ±∞\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URLÏ†úÍ±∞\n",
    "    text = re.sub(pattern, '', text)\n",
    "    only_english = re.sub('[^ a-zA-Z]', '', text)\n",
    "    pic_str = 'pic.twitter.com/'\n",
    "    only_english = only_english.lower()\n",
    "    \n",
    "    return only_english\n",
    "#     if bool(only_english and only_english.strip()) and len(only_english) >= 10:\n",
    "#         return only_english\n",
    "#     return False\n",
    "\n",
    "udf_nlkt = udf(regex_, StringType())\n",
    "new_df = df1.withColumn(\"text\", udf_nlkt(\"text\"))\n",
    "\n",
    "df1.show(3)\n",
    "\n",
    "new_df.show(3)\n",
    "\n",
    "class TextToSequence3(Transformer):\n",
    "    def __init__(self):\n",
    "        super(TextToSequence3, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        x_test = TextToSequence.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, x_test.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text', 'feature'])\n",
    "        \n",
    "        return fdf\n",
    "\n",
    "stage_t = TextToSequence3()\n",
    "test_pipeline = Pipeline(stages=[stage_t])\n",
    "\n",
    "preprocessing2 = test_pipeline.fit(new_df)\n",
    "preprocessed2 = preprocessing2.transform(new_df)\n",
    "preprocessed2.show(3)\n",
    "\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "df_test = preprocessed2.select(\n",
    "    preprocessed2[\"text\"], \n",
    "    list_to_vector_udf(preprocessed2[\"feature\"]).alias(\"feature\")\n",
    ")\n",
    "\n",
    "pred_test = my_dl.transform(df_test)\n",
    "\n",
    "pred_test.select('prediction').show(3, truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test2 = pred_test.select(\n",
    "    pred_test[\"text\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l>=0.9 else (0.0 if l< 0.1 else None) , DoubleType())\n",
    "pred_test3 = pred_test2.select(\n",
    "    pred_test2[\"text\"], \n",
    "    list_to_vector_udf(pred_test2[\"prediction2\"]).alias(\"label\")\n",
    ")\n",
    "\n",
    "pred_test3.show(3, truncate=False)\n",
    "\n",
    "pred_test3.printSchema()\n",
    "\n",
    "df = pred_test3.filter((col('label') == 0.0) | (col('label') == 1.0))\n",
    "\n",
    "df.groupBy('label').count().orderBy('count').show()\n",
    "\n",
    "df.toPandas().to_csv(\"/root/spark/df1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37acd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0171edd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37048221",
   "metadata": {},
   "source": [
    "# Î™®Îç∏ ÌïôÏäµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdfdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4391d",
   "metadata": {},
   "source": [
    "## 1. SparkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "524a6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_with_vectors.select(\"feature\").rdd.flatMap(lambda x: x).collect()\n",
    "y_train = df_with_vectors.select(\"label_index\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9316935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "rdd = to_simple_rdd(sc, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec6c3612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n",
      " * Serving Flask app 'elephas.parameter.server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://172.28.0.1:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Initialize workers\n",
      ">>> Distribute load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.28.0.1 - - [17/Jan/2022 04:44:56] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:44:56] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:44:56] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:44:57] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:44:58] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:44:58] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:00] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:01] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:02] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:05] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:06] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:09] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:09] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:09] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:09] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:09] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 24.0 failed 4 times, most recent failure: Lost task 7.3 in stage 24.0 (TID 6203, kafka1, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 108, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1095, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:800 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:790 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:783 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:751 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 108, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1095, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:800 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:790 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:783 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:751 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6ebf963469d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hogwild'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m                 yaml, parameters, self.client, train_config, freq, optimizer, loss, metrics, custom)\n\u001b[1;32m    178\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>> Distribute load'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>> Async training complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mnew_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 24.0 failed 4 times, most recent failure: Lost task 7.3 in stage 24.0 (TID 6203, kafka1, executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 108, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1095, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:800 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:790 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:783 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:751 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/worker.py\", line 108, in train\n    self.model.fit(x_train, y_train, **self.train_config)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\", line 1095, in fit\n    tmp_logs = self.train_function(iterator)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n    result = self._call(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 726, in _initialize\n    *args, **kwds))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\n    graph_function, _ = self._maybe_define_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 3206, in _create_graph_function\n    capture_by_value=self._capture_by_value),\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\", line 977, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:800 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:790 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:783 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:751 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:203 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:152 __call__\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:256 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:1608 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4979 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:174 sigmoid_cross_entropy_with_logits\n        (logits.get_shape(), labels.get_shape()))\n\n    ValueError: logits and labels must have the same shape ((None, 2) vs (None, 1))\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.28.0.1 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.1 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.3 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n",
      "172.28.0.2 - - [17/Jan/2022 04:45:10] \"GET /parameters HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
    "spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f3cf3",
   "metadata": {},
   "source": [
    "## 2. ElephasEstimator (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64354aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, regularizers\n",
    "\n",
    "optimizer_conf = optimizers.SGD(lr=0.001) #0.0001\n",
    "opt_conf = optimizers.serialize(optimizer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "984bf151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_78731217fd3f"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "from elephas import spark_model, ml_model\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"text\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(pretrained.to_json())\n",
    "estimator.set_categorical_labels(True) # one-hot encoding Ïó¨Î∂Ä\n",
    "estimator.set_nb_classes(2)\n",
    "estimator.set_num_workers(3)\n",
    "estimator.set_epochs(100)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00201c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bc56b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- feature: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_vectors.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ee81d7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 21.0 failed 4 times, most recent failure: Lost task 12.3 in stage 21.0 (TID 4473, kafka1, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/root/spark/python/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/root/spark/python/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"/root/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/ml/adapter.py\", line 36, in <lambda>\n    row.label, MLLibVectors.fromML(row.features)))\n  File \"/root/spark/python/pyspark/mllib/linalg/__init__.py\", line 904, in fromML\n    raise TypeError(\"Unsupported vector type %s\" % type(vec))\nTypeError: Unsupported vector type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/root/spark/python/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/root/spark/python/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"/root/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/ml/adapter.py\", line 36, in <lambda>\n    row.label, MLLibVectors.fromML(row.features)))\n  File \"/root/spark/python/pyspark/mllib/linalg/__init__.py\", line 904, in fromML\n    raise TypeError(\"Unsupported vector type %s\" % type(vec))\nTypeError: Unsupported vector type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-af3ef9650368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/ml_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                         validation_split=self.get_validation_split())\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hogwild'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, rdd, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m             worker = SparkWorker(yaml, parameters, train_config,\n\u001b[1;32m    184\u001b[0m                                  optimizer, loss, metrics, custom)\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mtraining_outcomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mnew_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mnumber_of_sub_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_outcomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 12 in stage 21.0 failed 4 times, most recent failure: Lost task 12.3 in stage 21.0 (TID 4473, kafka1, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/root/spark/python/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/root/spark/python/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"/root/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/ml/adapter.py\", line 36, in <lambda>\n    row.label, MLLibVectors.fromML(row.features)))\n  File \"/root/spark/python/pyspark/mllib/linalg/__init__.py\", line 904, in fromML\n    raise TypeError(\"Unsupported vector type %s\" % type(vec))\nTypeError: Unsupported vector type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/root/spark/python/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/root/spark/python/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/root/spark/python/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/root/spark/python/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/root/spark/python/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"/root/spark/python/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/elephas/ml/adapter.py\", line 36, in <lambda>\n    row.label, MLLibVectors.fromML(row.features)))\n  File \"/root/spark/python/pyspark/mllib/linalg/__init__.py\", line 904, in fromML\n    raise TypeError(\"Unsupported vector type %s\" % type(vec))\nTypeError: Unsupported vector type <class 'str'>\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "my_dl = dl_pipeline.fit(df_with_vectors)\n",
    "t2 = time.time() - t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e415fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_1_ckpt/assets\n"
     ]
    }
   ],
   "source": [
    "pretrained.save(\"model_1_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bec79ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.save_weights(\"pretrained_1_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0628acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/root/spark/model/keras_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d94609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('/root/spark/model/keras_model_1_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59b060df",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('/root/spark/model/new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12c7fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save_weights('/root/spark/model/new_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae9ee61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. EstimatorÎ•º Ï†ÄÏû•ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.ml_model import load_ml_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.save(\"/root/spark/model/v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13663451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_estimator = ElephasEstimator()\n",
    "new_estimator = load_ml_estimator(\"/root/spark/model/v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. TransformerÎ•º Ï†ÄÏû•ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "751b91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.ml_model import ElephasTransformer\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "import h5py\n",
    "from elephas.ml_model import load_ml_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "148155b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElephasTransformer_555fc48a907f\n"
     ]
    }
   ],
   "source": [
    "cs_model = my_dl.stages[0]\n",
    "print(cs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1492e6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_model.save(\"/root/spark/model/estimator1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad3bb01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_ml_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e75ad83f76bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ml_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/root/spark/model/estimator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_ml_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "new_transformer = load_ml_transformer(\"/root/spark/model/estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ModelÏùÑ Ï†ÄÏû•ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68fc79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/root/spark/model/keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac23c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('/root/spark/model/keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3165ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_modelÏùÑ fixÌïòÍ≥† untrainableÌïòÍ≤å Î™áÍ∞úÏùò layerÎ•º trainable ÎßåÎì†Îã§Ïùå define the estimator again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72325668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelineÏóê ÎÑ£Ïñ¥ÏÑú fit again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2cafdbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_f5cdbf85529a"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_estimator = ElephasEstimator()\n",
    "new_estimator.set_keras_model_config(loaded_model.to_json())\n",
    "new_estimator.setFeaturesCol(\"feature\")\n",
    "new_estimator.setLabelCol(\"label_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7abb06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline = Pipeline(stages=[new_estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b9fc740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label_index', 'feature']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_vectors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87c4faf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ElephasEstimator' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-87a4270c2cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElephasEstimator' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "new_transformer = new_estimator.transform(df_with_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fbab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5527336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff35938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. TransformerÎ•º Ï†ÄÏû•ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf881ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = indexerModel._call_java(\"write\")\n",
    "writer.save(\"indexerModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796379b",
   "metadata": {},
   "source": [
    "## Í∏∞Ï°¥ TransformerÎ•º Ïù¥Ïö©Ìï¥ÏÑú ÏòàÏ∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2183f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSequence2(Transformer):\n",
    "    def __init__(self):\n",
    "        super(TextToSequence2, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        # 200Ïùò Í∏∏Ïù¥Î°ú ÏÉÅÏúÑ 2ÎßåÍ∞úÏùò ÌÜ†ÌÅ∞Îßå Î∞òÏòÅ\n",
    "        #vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "        \n",
    "        x_test = TextToSequence.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, labels, x_test.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text','label','feature'])\n",
    "        \n",
    "        return fdf\n",
    "    \n",
    "# get_tokens Ìï®ÏàòÍ≥º Í∞ôÏùÄ Ïö©ÎèÑ\n",
    "stage_5 = TextToSequence2()\n",
    "stage_6 = StringIndexer(inputCol='label', outputCol='label_index')\n",
    "test_pipeline = Pipeline(stages=[stage_5, stage_6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e941a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing2 = test_pipeline.fit(second_df)\n",
    "preprocessed2 = preprocessing2.transform(second_df)\n",
    "\n",
    "df_test = preprocessed2.select(\n",
    "    preprocessed2[\"label_index\"], \n",
    "    list_to_vector_udf(preprocessed2[\"feature\"]).alias(\"feature\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e508e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|label_index|             feature|\n",
      "+-----------+--------------------+\n",
      "|        1.0|[9706.0,10047.0,7...|\n",
      "|        1.0|[1683.0,16.0,1208...|\n",
      "|        1.0|[8767.0,1.0,1234....|\n",
      "|        1.0|[1318.0,1185.0,11...|\n",
      "|        1.0|[7658.0,1.0,1302....|\n",
      "+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85a317d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = my_dl.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37de04ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|prediction                               |\n",
      "+-----------------------------------------+\n",
      "|[0.00904042087495327, 0.990959644317627] |\n",
      "|[0.7892279624938965, 0.21077196300029755]|\n",
      "|[0.8867055773735046, 0.11329440027475357]|\n",
      "+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test.select('prediction').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11d54a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test2 = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50103e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test3 = pred_test2.select(\n",
    "    pred_test2[\"label_index\"], \n",
    "    list_to_vector_udf(pred_test2[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7c79e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|label_index|rawPrediction|\n",
      "+-----------+-------------+\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          0.0|\n",
      "|        1.0|          0.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          0.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          1.0|\n",
      "|        1.0|          0.0|\n",
      "+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test3.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test3.groupBy('rawPrediction').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "945bf2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87435\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"rawPrediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pred_test3)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07734d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "311a0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_(text):\n",
    "    # ÏòÅÏñ¥, Ïà´Ïûê, ÌäπÏàòÎßåÎ¨∏Ïûê Ï†úÏô∏ ÏÇ≠Ï†ú.\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URLÏ†úÍ±∞\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URLÏ†úÍ±∞\n",
    "    text = re.sub(pattern, '', text)\n",
    "    only_english = re.sub('[^ a-zA-Z]', '', text)\n",
    "    pic_str = 'pic.twitter.com/'\n",
    "    only_english = only_english.lower()\n",
    "    \n",
    "    return only_english\n",
    "#     if bool(only_english and only_english.strip()) and len(only_english) >= 10:\n",
    "#         return only_english\n",
    "#     return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c23435ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_nlkt = udf(regex_, StringType())\n",
    "new_df = df1.withColumn(\"text\", udf_nlkt(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54c62882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @nanox_finance...|\n",
      "|*/inamin HAHAHAHA...|\n",
      "|4.  hai yang mau ...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f6d7ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|rt nanoxfinance a...|\n",
      "|inamin hahahahaha...|\n",
      "|  hai yang mau se...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b5b07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSequence3(Transformer):\n",
    "    def __init__(self):\n",
    "        super(TextToSequence3, self).__init__()\n",
    "        \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # Ïö∞Î¶¨Í∞Ä Ìï¥ÏïºÌï† ÏùºÏùÄ dataframeÏùÑ ÏàúÏÑúÌÜµÏùºÌï¥ÏÑú textÏôÄ label Î¶¨Ïä§Ìä∏Î°ú ÎÇòÎàÑÎäîÍ≤É\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        x_test = TextToSequence.vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, x_test.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, 306) #, numSlices=306\n",
    "        fdf = rdd.toDF(['text', 'feature'])\n",
    "        \n",
    "        return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd87fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_t = TextToSequence3()\n",
    "test_pipeline = Pipeline(stages=[stage_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8108aafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|             feature|\n",
      "+--------------------+--------------------+\n",
      "|rt nanoxfinance a...|[64, 1, 1, 141, 1...|\n",
      "|inamin hahahahaha...|[1, 1, 0, 0, 0, 0...|\n",
      "|  hai yang mau se...|[12344, 12604, 1,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessing2 = test_pipeline.fit(new_df)\n",
    "preprocessed2 = preprocessing2.transform(new_df)\n",
    "preprocessed2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2aec25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "df_test = preprocessed2.select(\n",
    "    preprocessed2[\"text\"], \n",
    "    list_to_vector_udf(preprocessed2[\"feature\"]).alias(\"feature\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f78954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = my_dl.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97959860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|prediction                               |\n",
      "+-----------------------------------------+\n",
      "|[0.42767348885536194, 0.5723265409469604]|\n",
      "|[0.05208161845803261, 0.9479184150695801]|\n",
      "|[0.02548149600625038, 0.9745185375213623]|\n",
      "+-----------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test.select('prediction').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98cd2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test2 = pred_test.select(\n",
    "    pred_test[\"text\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a13ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l>=0.9 else (0.0 if l< 0.1 else None) , DoubleType())\n",
    "pred_test3 = pred_test2.select(\n",
    "    pred_test2[\"text\"], \n",
    "    list_to_vector_udf(pred_test2[\"prediction2\"]).alias(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d78ab67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|text                                                                                                                       |label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|rt nanoxfinance airdrop alert  nanoxfinance have started it last community airdropno referral needed  guaranteed for all pa|null |\n",
      "|inamin hahahahahahahahahahahaha                                                                                            |0.0  |\n",
      "|  hai yang mau sewa zoom unlock cheggcek turnitin bisa banget loh sama aku sudah ada  testi ya bisa cek                    |0.0  |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test3.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33ad0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74aa534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pred_test3.filter((col('label') == 0.0) | (col('label') == 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64a35487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|  1.0|111107|\n",
      "|  0.0|421952|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05578a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(\"/root/spark/df1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ebcc4b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType(\n",
    "   [\n",
    "    StructField('text', StringType(), False),\n",
    "    StructField('label', DoubleType(), False)\n",
    "   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fea1f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(\"hdfs:///root/spark/first.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1bf8027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "first.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c66c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test3ÏùÑ Ï†ÄÏû•ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39431766",
   "metadata": {},
   "source": [
    "## Ï†ÄÏû•Îêú ModelÏùÑ Î∂àÎü¨ÏôÄÏÑú ÏòàÏ∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a61228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.ml_model import ElephasTransformer\n",
    "import h5py\n",
    "from elephas.ml_model import load_ml_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "565c395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_transformer = load_ml_transformer(\"/root/spark/model/estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c757c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_ = new_transformer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95c8dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|label_index|             feature|          prediction|\n",
      "+-----------+--------------------+--------------------+\n",
      "|        1.0|[42.0,3.0,540.0,1...|[0.49246680736541...|\n",
      "|        1.0|[177.0,1710.0,336...|[0.49246680736541...|\n",
      "|        1.0|[4390.0,837.0,487...|[0.49246680736541...|\n",
      "|        1.0|[3525.0,1328.0,12...|[0.49246680736541...|\n",
      "|        1.0|[121.0,4969.0,148...|[0.49246680736541...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test_.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "51810abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|label_index|             feature|          prediction|\n",
      "+-----------+--------------------+--------------------+\n",
      "|        1.0|[42.0,3.0,540.0,1...|[0.48319944739341...|\n",
      "|        1.0|[177.0,1710.0,336...|[0.48319944739341...|\n",
      "|        1.0|[4390.0,837.0,487...|[0.48319944739341...|\n",
      "|        1.0|[3525.0,1328.0,12...|[0.48319944739341...|\n",
      "|        1.0|[121.0,4969.0,148...|[0.48319944739341...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e7e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4f347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4c085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af06063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6958e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25d9b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text):    \n",
    "    tmp = list()\n",
    "    tmp.append(str(text))\n",
    "    y_prob = new_model.predict([tmp])\n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "    prediction = y_classes.tolist()[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3caec72b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/root/spark/python/pyspark/serializers.py\", line 597, in dumps\n",
      "    return cloudpickle.dumps(obj, 2)\n",
      "  File \"/root/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/root/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 409, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 736, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/root/spark/python/pyspark/cloudpickle.py\", line 400, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/root/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 852, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 521, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 634, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 521, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 634, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 847, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 476, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 821, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 846, in _batch_setitems\n",
      "    save(k)\n",
      "  File \"/usr/lib/python3.6/pickle.py\", line 496, in save\n",
      "    rv = reduce(self.proto)\n",
      "TypeError: can't pickle weakref objects\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: can't pickle weakref objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m             \u001b[0;31m# Subtle.  Same as in the big comment below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 or themodule is None):\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_function_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/cloudpickle.py\u001b[0m in \u001b[0;36msave_function_tuple\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qualname'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTUPLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    851\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle weakref objects",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-fb6f58c467e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mudf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudf_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# new_df = new_df.drop(\"text\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# and should have a minimal performance impact.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_judf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m     37\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
      "\u001b[0;32m~/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: TypeError: can't pickle weakref objects"
     ]
    }
   ],
   "source": [
    "udf_model = udf(prediction, StringType())\n",
    "new_df = label_df.withColumn(\"prediction\", udf_model(\"text\"))\n",
    "# new_df = new_df.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72ac129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------+--------------------+\n",
      "|                text|label|label_index|          prediction|\n",
      "+--------------------+-----+-----------+--------------------+\n",
      "|how to run spider...|    1|        1.0|[[how to run spid...|\n",
      "|wordpress magic f...|    1|        1.0|[[wordpress magic...|\n",
      "|announcing apache...|    1|        1.0|[[announcing apac...|\n",
      "+--------------------+-----+-----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bba3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399613c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44100d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dff9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = my_dl.transform(second_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_double = udf(lambda l: l[0], DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_double(pred_test[\"prediction\"]).alias(\"prediction2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l:1.0 if l<0.5 else 0.0 , DoubleType())\n",
    "pred_test = pred_test.select(\n",
    "    pred_test[\"label_index\"], \n",
    "    list_to_vector_udf(pred_test[\"prediction2\"]).alias(\"rawPrediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eed5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_index\", predictionCol=\"rawPrediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(pred_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e5be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d8254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a4807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
