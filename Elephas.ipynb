{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8f7016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import zeros\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb381960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D\n",
    "from keras.layers import MaxPooling2D, Dropout,Flatten\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e98d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import pyspark.sql.functions as f\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import sys\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7139c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kafka1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71354425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.deploy.mode', 'cluster'),\n",
       " ('spark.sql.execution.arrow.enabled', 'true'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.cores', '5'),\n",
       " ('spark.executor.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.extraJavaOptions', '-XX:+UseG1GC'),\n",
       " ('spark.driver.host', 'kafka1'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.yarn.archive', 'hdfs:///user/spark/conf/spark-libs.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.app.id', 'application_1641116526368_0094'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.kryoserializer.buffer.max', '2047'),\n",
       " ('spark.executor.memoryOverhead', '1g'),\n",
       " ('spark.driver.memoryOverhead', '1g'),\n",
       " ('spark.driver.port', '36301'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.logDirectory', 'file:///root/spark/eventLog'),\n",
       " ('spark.default.parallelism', '163'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.network.timeout', '3600s'),\n",
       " ('spark.driver.appUIAddress', 'http://kafka1:4040'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1641116526368_0094'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/root/spark/python/lib/py4j-0.10.7-src.zip:/root/spark/python/:/python:/python/lib/py4j-0.10.7-src.zip:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  '0.0.0.0'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://0.0.0.0:8089/proxy/application_1641116526368_0094'),\n",
       " ('spark.eventLog.dir', 'file:///root/spark/eventLog'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa85c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def regex_(text):\n",
    "#     # 영어, 숫자, 특수만문자 제외 삭제.\n",
    "#     pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     pattern = '(http|ftp|https):// (?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'  # URL제거\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     only_english = re.sub('[^ a-zA-Z]', '', text)\n",
    "#     only_english = only_english.lower()\n",
    "\n",
    "#     if bool(only_english and only_english.strip()) and len(only_english) >= 10:\n",
    "#         return only_english\n",
    "\n",
    "#     return text\n",
    "\n",
    "# def get_lemma(word):\n",
    "#     lemma = wn.morphy(word)\n",
    "#     if lemma is None:\n",
    "#         return word\n",
    "#     else:\n",
    "#         return lemma\n",
    "\n",
    "\n",
    "# def get_tokens(sentence):\n",
    "#     tknzr = TweetTokenizer()\n",
    "#     tokens = tknzr.tokenize(sentence)\n",
    "#     tokens = [token for token in tokens if (token not in stop_words and len(token) > 1)]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691f7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로 들어온 csi\n",
    "csi_pos_neg = spark.read.csv(\"hdfs:///user/spark/datafile/csiposneg.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cefe094",
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_pos_neg = csi_pos_neg.withColumn(\"label\",col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead972e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_tokens 함수과 같은 용도\n",
    "stage_1 = RegexTokenizer(inputCol= 'text', outputCol='pos_t', pattern= '\\\\W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702de69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+\n",
      "|_c0|                text|label|               pos_t|\n",
      "+---+--------------------+-----+--------------------+\n",
      "|  0|threatmeter hacke...|    1|[threatmeter, hac...|\n",
      "|  1|first android mal...|    1|[first, android, ...|\n",
      "|  2|adobe fixes six c...|    1|[adobe, fixes, si...|\n",
      "|  3| scienceporn  in ...|    1|[scienceporn, in,...|\n",
      "|  4|riskware hmoyfzb ...|    1|[riskware, hmoyfz...|\n",
      "+---+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r1 = stage_1.transform(csi_pos_neg)\n",
    "r1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38be563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_lemma 함수와 같은 용도\n",
    "stage_2 = StopWordsRemover(inputCol= stage_1.getOutputCol(), outputCol= 'filtered_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a1decdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|_c0|                text|label|               pos_t|      filtered_words|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "|  0|threatmeter hacke...|    1|[threatmeter, hac...|[threatmeter, hac...|\n",
      "|  1|first android mal...|    1|[first, android, ...|[first, android, ...|\n",
      "|  2|adobe fixes six c...|    1|[adobe, fixes, si...|[adobe, fixes, si...|\n",
      "|  3| scienceporn  in ...|    1|[scienceporn, in,...|[scienceporn, vac...|\n",
      "|  4|riskware hmoyfzb ...|    1|[riskware, hmoyfz...|[riskware, hmoyfz...|\n",
      "+---+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r2 = stage_2.transform(r1)\n",
    "r2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e72617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "class TextToSequence(Transformer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextToSequence, self).__init__()\n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        # 우리가 해야할 일은 dataframe을 순서통일해서 text와 label 리스트로 나누는것\n",
    "        samples = df.select(\"text\").rdd.flatMap(lambda x: x).collect()\n",
    "        labels = df.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "        \n",
    "        vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
    "        text_ds = tf.data.Dataset.from_tensor_slices(samples).batch(128)\n",
    "        vectorizer.adapt(text_ds)\n",
    "        \n",
    "        voc = vectorizer.get_vocabulary()\n",
    "        word_index = dict(zip(voc, range(len(voc))))\n",
    "        \n",
    "        x_train = vectorizer(np.array([[s] for s in samples])).numpy()\n",
    "        zip_array = list(zip(samples, labels, x_train.tolist()))\n",
    "        rdd = sc.parallelize(zip_array, numSlices=306)\n",
    "        fdf = rdd.toDF(['text','label','feature'])\n",
    "        \n",
    "        #fdf = spark.createDataFrame(df)\n",
    "        return fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8014e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequence = TextToSequence()\n",
    "df_example = text_sequence.transform(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c33439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                |label|feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|threatmeter hacked emails of san francisco muni rail system hacker reveals clues about identity and tactics kr  fin |1    |[172, 306, 1310, 7, 443, 1982, 12499, 7661, 287, 26, 1584, 14201, 48, 1512, 6, 2998, 10403, 6385, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]|\n",
      "|first android malware targeting pcs uncovered  fbxtynm                                                              |1    |[114, 125, 78, 1432, 2305, 8293, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                             |\n",
      "|adobe fixes six code execution bugs in flash  mokgxr                                                                |1    |[535, 1418, 2131, 139, 300, 1633, 4, 612, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                          |\n",
      "| scienceporn  in a vacuum i guess                                                                                   |1    |[1, 4, 5, 11120, 15, 1613, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                   |\n",
      "|riskware hmoyfzb  fbjaz                                                                                             |1    |[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                           |\n",
      "+--------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_example.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78b3fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "df_with_vectors = df_example.select(\n",
    "    df_example[\"label\"], \n",
    "    list_to_vector_udf(df_example[\"feature\"]).alias(\"feature\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8a31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_with_vectors.select('label', 'feature').limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d87a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_str_index = StringIndexer(inputCol='label', outputCol='label_index')\n",
    "label_df = label_str_index.fit(df_with_vectors).transform(df_with_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c508d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- feature: vector (nullable = true)\n",
      " |-- label_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b4930cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "652\n",
      "199512\n"
     ]
    }
   ],
   "source": [
    "glom_list =  label_df.rdd.glom().collect()\n",
    "print(len(glom_list))\n",
    "summ = 0\n",
    "for i in glom_list:\n",
    "    print(len(i))\n",
    "    summ += len(i)\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6fbbb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classes = label_df.select(\"label\").distinct().count()\n",
    "nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d11d5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set(kafka1:42691, kafka1:39993, kafka2:33153, kafka3:34299)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "sc = spark._jsc.sc() \n",
    "\n",
    "result1 = sc.getExecutorMemoryStatus().keys() # will print all the executors + driver available\n",
    "\n",
    "result2 = len([executor.host() for executor in sc.statusTracker().getExecutorInfos() ]) -1\n",
    "\n",
    "print(result1, end ='\\n')\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb57a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31718422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401aabd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82406222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ',quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "glove_path = \"/root/spark/glove.6B.100d.txt\"\n",
    "pre_glove = glove2dict(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b1c59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426768d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f940511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"repo_glove.pkl\",\"rb\")\n",
    "newglove = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ef30b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400192"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_glove.update(newglove)\n",
    "len(pre_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ded8b469",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-049704a004f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Prepare embedding matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_glove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = pre_glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71027c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca776ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00126e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from elephas import spark_model, ml_model\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"feature\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_categorical_labels(True) # dense 1이면 False로 설정해야함\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(2)\n",
    "estimator.set_epochs(10)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"asynchronous\")\n",
    "estimator.set_loss(\"binary_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c34464",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23824d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "my_dl = dl_pipeline.fit(label_df)\n",
    "t2 = time.time() - t1\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6bcb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 왜 172.28.0.2는 동작안하는거지? cluster 모드라서? master는 동작안하는거야?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eefb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 연결할 코드 실시간 구현시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e565f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"TwitterStreamingAPI\") \\\n",
    "        .config(\"spark.mongodb.input.uri\",\"mongodb://117.17.189.6:27017/tweet.kafka_tweet\")\\\n",
    "        .config(\"spark.mongodb.output.uri\",\"mongodb://117.17.189.6:27017/tweet.kafka_tweet\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "541c75b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "kafka_df = spark.readStream.format(\"kafka\").option(\"partition.assignment.strategy\",\"range\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"test2\").option(\"startingOffsets\", \"latest\").load()\n",
    "kafka_df.printSchema()\n",
    "kafka_df1 = kafka_df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a28ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_detail_schema = StructType() \\\n",
    "        .add(\"id\", StringType()) \\\n",
    "        .add(\"text\", StringType()) \\\n",
    "        .add(\"created_at\", StringType())\\\n",
    "\n",
    "kafka_df2 = kafka_df1 \\\n",
    "        .select(from_json(col(\"value\"), transaction_detail_schema).alias(\"transaction_detail\"), \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "123f9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df3 = kafka_df2.select(\"transaction_detail.*\", \"timestamp\")\n",
    "udf_model = udf(prediction, StringType())\n",
    "\n",
    "new_df = kafka_df3.withColumn(\"prediction\", udf_model(\"text\"))\n",
    "# new_df = new_df.drop(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_embedding():\n",
    "    # exit code 0인지 확인하는 코드 구현 try - except\n",
    "    subprocess.check_call([\"python3\", \"newGlove.py\"]) # should exit with status 0\n",
    "    \n",
    "    f = open(\"repo_glove.pkl\",\"rb\")\n",
    "    newglove = pickle.load(f)\n",
    "    print(len(newglove))\n",
    "    num_tokens = len(voc) + 2\n",
    "    embedding_dim = 100\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = pre_glove.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    "    \n",
    "    int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "    x = layers.MaxPooling1D(5)(x)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling1D(5)(x)\n",
    "    x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    preds = layers.Dense(2, activation=\"softmax\")(x)\n",
    "    model = keras.Model(int_sequences_input, preds)\n",
    "    model.summary()\n",
    "    \n",
    "    # 노드 하나에서만 동작함\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    "    )\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=1, validation_data=(x_val, y_val))\n",
    "    \n",
    "    string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    x = vectorizer(string_input)\n",
    "    preds = model(x)\n",
    "    end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "#     probabilities = end_to_end_model.predict(\n",
    "#         [[\"this message is about computer graphics and 3D modeling\"]]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae3a6793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(text):\n",
    "#     max_length = 100\n",
    "#     pic_str = 'pic.twitter.com/'\n",
    "#     if text.startswith('RT'):\n",
    "#         text_l = text.split()\n",
    "#         text = ' '.join(text_l[2:])\n",
    "#     text = regex_(text)\n",
    "#     if isinstance(text, str) == False:\n",
    "#         return \"error\"\n",
    "#     tweet_l = text.split()\n",
    "#     for t in tweet_l:\n",
    "#         if pic_str in t:\n",
    "#             len_text = len(t)\n",
    "#     idx = text.find(pic_str)\n",
    "#     if idx != -1:\n",
    "#         text = text[:idx] + text[idx + len_text:]\n",
    "\n",
    "#     reg_text = regex_(text)\n",
    "#     if reg_text == False:\n",
    "#             return \"error\"\n",
    "    \n",
    "    tmp = list()\n",
    "    tmp.append(str(text)) # 전처리가 필요할 경우 reg_text\n",
    "    \n",
    "    y_prob = model.predict([tmp])\n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "    prediction = y_classes.tolist()[0]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b7150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mongo_row(df, epoch_id):\n",
    "    mongoURL = \"mongodb://117.17.189.6:27017/tweet.kafka_tweet\"\n",
    "    df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"uri\", mongoURL).save()\n",
    "    df.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f47d9da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = new_df.writeStream.trigger(processingTime='1 seconds').foreachBatch(write_mongo_row).start() \n",
    "query.awaitTermination()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
